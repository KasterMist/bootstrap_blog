[{"authors":[],"categories":[{"title":"Note","url":"/categories/note/"}],"content":"这部分将介绍如何在VsCode中使用Cmake Tools进行Debug调试。\n我们可以通过配置tasks.json和launch.json文件来进行code run和debug配置。不过这种方式对于处理大型project来说较为麻烦，因为需要将Project的CMakeLists中的大量配置放到tasks.json和launch.json里面。\nVsCode给了官方文档可供查阅: https://github.com/microsoft/vscode-cmake-tools/blob/main/docs/debug-launch.md\n但是如果想要使用CMake Tools进行Debug并自定义程序参数和Debug参数的话，需要使用launch.json。不过对于“Debug using a launch.json file”，本人按照tutorial进行了launch.json配置，但无法实现Debug时添加参数的情况。\n后面发现可能是launch.json里面的设置无法对cmake.debugConfig里面进行配置，所以我采取了了在setting.json里面直接修改cmake.debugConfig里面的参数 (比如下面的代码)，之后证明可行。从VsCode的另一部分的文档中https://github.com/microsoft/vscode-cmake-tools/blob/main/docs/cmake-settings.md#command-substitution 可以知道CMake Tools的参数配置可以在setting.json中进行设置与修改。\n\u0026#34;cmake.debugConfig\u0026#34;: { \u0026#34;args\u0026#34;: [\u0026#34;--arg\u0026#34;, \u0026#34;argument info\u0026#34;], // 在这里设置命令行参数 \u0026#34;stopAtEntry\u0026#34;: false } (注，如果想要实现完全的Debug功能的话，如成功设置断点，需要在CMakeLists里面对调试模式条件下加上\u0026quot;-g\u0026quot;和非高优化flag，比如\u0026quot;-O0\u0026quot;，这并不是一个难点，但是对于接手一个项目来说，我们无法判断接手的项目是否有那么完备的配置)。\n","date":"October 12, 2024","img":"https://KasterMist.com/images/logo/vscode.jpeg","lang":"en","langName":"English","largeImg":"","permalink":"/posts/vscode/%E4%BD%BF%E7%94%A8cmake_tools%E8%BF%9B%E8%A1%8Cdebug%E8%B0%83%E8%AF%95/","series":[],"smallImg":"","tags":[{"title":"VsCode","url":"/tags/vscode/"}],"timestamp":1728704440,"title":"在Vscode中使用Cmake Tools进行Debug调试"},{"authors":[],"categories":[{"title":"Tutorial","url":"/categories/tutorial/"}],"content":"这篇博客将根据《深入理解计算机系统》(Computer Systems: A Programmer\u0026rsquo;s Perspective, CS: APP)总结“第二章: 信息的表示和处理”的知识点。\n信息的表示和处理 信息存储 基本概念 大多数计算机使用8位的块，也可以叫做字节(byte)，来作为最小的可寻址的存储器单位。\n机器程序将存储器视为一个很大的字节数组，也被称之为虚拟存储器(virtual memory)。\n存储器的每一个字节都由一个唯一的数字来标识，称为地址(address)。所有可能的地址集合被称为虚拟地址空间(virtual address space)。\n编译器和运行时系统的一个任务就是将这个存储空间划分为更可管理的单元，来存放不同的程序对象(program object)，这种管理完全是在虚拟地址空间里完成的。C语言中的一个指针的值(无论它指向的是整数、结构还是其他信息)都是某个存储块的第一个字节的虚拟地址。\nC编译器会把每个指针和类型信息联系起来，这样就可以根据指针的类型生成不同的机器级代码来访问存储在指针所指向位置处的值。\n进制 一个字节包括8位，在二进制表示法中，它的值域就是$00000000_2 \\sim 11111111_2$，如果看成十进制整数，值域就是$0_{10} \\sim 255_{10}$，16进制可以更好的来描述位模式，它的值域是$00_{16} \\sim FF_{16}$。我们会在一个数字前加上0b表示这个数是二进制，加上0x表示这个数是十六进制。\n对于二进制和十六进制的转化非常简单，因为我们只需要记住十六进制数字0代表四个二进制0就可以。也就是说我们可以简单理解为十六进制的1个位数可以转化为二进制的4个位数。而对于十进制转化成二进制或十六进制也十分简单。比如$x = 2048 = 2^{11}$，我们有$n = 11 = 3 + 4 * 2$，于是可以得到十六进制的表示$0x800$。\n使用二进制和十六进制表示的场景 使用二进制表示的场景:\n位操作和位掩码: 二进制表示法非常适合进行位操作，例如按位与、按位或、按位异或和移位操作。使用二进制表示可以更直观地看到各个位的状态。 嵌入式编程: 在嵌入式编程中，直接操作硬件寄存器往往需要使用二进制表示，因为每个位代表特定的硬件功能或状态。 调试和分析:当需要调试或分析低级数据（例如，网络协议中的位字段，图像处理中的像素数据），使用二进制表示可以更清晰地看到每个位的值。 使用十六进制表示的场景:\n使用大整数: 使用十六进制表示可以更简洁地表示较大的整数值。每个十六进制数字表示四个二进制位，因此相比二进制表示法更紧凑。 内存地址: 内存地址通常用十六进制表示，因为这样更紧凑且易于阅读。调试工具和内存查看器通常使用十六进制来显示内存地址。 颜色值: 在图形编程和网页设计中，颜色值通常使用十六进制表示，因为每个颜色通道（红、绿、蓝）可以用两个十六进制数字表示。 表示字节数据: 在处理字节数据（如文件头、网络包、加密数据）时，十六进制表示更紧凑且易于与规范文档对照。 编码和解码: 在数据编码和解码过程中，十六进制表示常用于表示原始字节数据。 数据大小 每台计算机都有一个字长(word size)，来指明整数和指针数据的标称大小(nominal size)。字长决定的最终逃的系统参数就是虚拟地址空间的最大大小。换言之，对于一个字长为n的机器而言，虚拟地址的范围为$0 \\sim 2^n - 1$\n我们说的32位计算机和64位计算机，指的是32位的处理器和64位的处理器，这里的位是字长。计算机字长(机器字长)取决于数据总线的宽度，通常就是CPU一次能处理的数据的位数(CPU位数)。下面有一个等式关系:\n​\tCPU位数 = CPU中寄存器的位数 = CPU能够一次并行处理的数据宽度(位数) = 数据总线宽度\n虽然64位处理器可以支持非常大的虚拟地址空间，但操作系统和硬件实际分配和使用的虚拟地址空间会受限于当前技术和需求。例如，一些操作系统可能只实现了48位或52位的虚拟地址空间，即使处理器支持更大的范围。也就是说64位处理器的理论上虚拟空间可以达到$2^{64}$，不过目前情况下许多现代操作系统在64位模式下实现了48位的虚拟地址空间，提供$2^{48}$字节（256TB）的虚拟地址空间。\n通过一个例子可以非常直观的理解上面的概念。假如有一个32位处理器，它的最大虚拟地址空间为$2^{32} = 4GB$，那么如果给他分配一个8GB的内存条，由于32位处理器的地址总线限制，它们最多只能直接寻址4GB的物理内存，也就是其无法处理超过4GB的那部分内存。\n过去32位处理器一直都是标准的情况。32位处理器中C语言的long int是4 bytes，而64位处理器中C语言的long int是8 bytes，等等。这种不一致会导致之前在32位处理器编写的程序如果移植到64位处理器中时，就会因为字长的问题导致错误。\n寻址和字节顺序 对于跨越多字节(大于1字节)的程序对象，我们需要建立规则:这个对象的地址是什么，我们在存储器中该如何对这些字节进行排序。在几乎所有的机器上，多字节对象都被存储为连续的字节序列。对象的地址为所使用字节序列中最小的地址。例如一个类型为int的变量x的地址是0x00，那么表达式\u0026amp;x的值为0x00，x的四字节将被存储在存储器的0x100、0x101、0x102和0x103的位置。\n对表示一个对象的字节序列排序，有两种通用规则。例如一个w位的整数，有位表示$[x_{w-1}, x_{w-2}, \u0026hellip;, x_1, x_0]$，其中$x_{w-1}$是最高有效位，而$x_0$是最低有效位。假设w是8的倍数，那么这些位就被分组成为字节。，其中最高有效字节包含了位$[x_{w-1}, x_{w-2}, \u0026hellip;, x_{w-8}]$，而最低有效字节包含位$[x_7, x_6, \u0026hellip;, x_0]$。某些机器选择在存储器中按照从最低有效字节到最高有效字节的顺序存储对象，而一些机器则按照从最高有效字节到最低有效字节的顺序存储。前一种规则被称为小端法(little endian)，后一种规则被称为大端法(big endian)。\n运算 下面的运算是C语言的语法:\n二进制值1和0表示逻辑值True和False.运算符~、\u0026amp;、|、和^一次表示逻辑运算NOT, AND, OR和EXCLUSIVE-OR(异或，相同为True，不相同为False). 与逻辑运算不同，布尔运算是对二进制中的每一个位进行的运算。这些运算能运用到任何的“整形”数据类型上。 逻辑运算符||、\u0026amp;\u0026amp;和!分对应于命题逻辑的OR、AND和NOT运算。逻辑运算认为所有非零的参数都表示为True。此外，在多个逻辑判断中，如果对第一个参数求值就能确定表达式的结果，那么逻辑运算符就不会对第二个参数求值。 C语言还提供了一系列的移位运算。对于一个位表示为$[x_{n-1}, x_{n-2}, \u0026hellip;, x_0]$的运算符$x$，$x \u0026laquo; k$会生成一个值，其位表示为$[x_{n-k-1}, x_{n-k-2}, \u0026hellip;, x_0, 0, \u0026hellip;, 0]$。简而言之，x向左移动k位，丢弃k个最高位，并在右端补了k个0。移位运算从左至右，运算符的优先级为: 1\u0026laquo;5-1应该按照1\u0026laquo;(5-1)而不是(1\u0026laquo;5)-1。\n对于右移运算，机器支持逻辑右移运算和算数右移运算。逻辑右移在左端补k个0，得到的结果是$[0, \u0026hellip;, 0, x_{n-1}, x_{n-2}, \u0026hellip;, x_n]$。算数右移是在左端补k个最高有效位的拷贝，得到的结果是$[x_{n-1}, \u0026hellip;, x_{n-1}, x_{n-1}, x_{n-2}, \u0026hellip;, x_k]$。这种做法看上去比较奇特，但是我们会发现它对于有符号整数数据的运算十分有用。\nC语言的标准并没有明确定义应该使用哪种类型的右移。对于无符号的数据(以限定词unsigned声明的整型对象)，右移必须是逻辑的。而对于有符号数据(默认)，算数的活着逻辑的右移都可以。然而，这意味着任何假设一种或者另一种右移形式的代码都会潜在的遇到可移植性问题。实际上，几乎所有编译器/机器组合都对有符号数据使用算数右移。简而言之我们可以记为: 无符号数据使用逻辑右移，有符号数据使用算数右移。\n补充: 取模运算可能会遇到负数问题(取模主要是用于计算机术语中。取余则更多是数学概念)，泛化取模计算公式如下: $$ x \\mod y = x - \\lfloor x / y \\rfloor \\times y $$ 例如: $$ 5 \\mod 3 = 5 - \\lfloor 5 / 3 \\rfloor \\times 3 = 2 \\ 5 \\mod -3 = 5 - \\lfloor 5 / (-3) \\rfloor \\times (-3) = -1 \\ -5 \\mod 3 = -5 - \\lfloor -5 / 3 \\rfloor \\times 3 = 1 \\ -5 mod -3 = -5 - \\lfloor -5 / (-3) \\rfloor \\times (-3) = -2 $$\n编码表示 无符号二进制表示 如果一个整数数据类型有w位，我们可以将位向量写成$\\vec{x}$来表示整个向量，或者写成$[x_{w-1}, x_{w-2}, \u0026hellip;, x_0]$来表示向量中的每一位。把$\\vec{x}$看做一个写成二进制表示的数，我们就获得了$\\vec{x}$的无符号表示。我们用函数$B2U_w$(代表“无符号的二进制”，长度为w)来表示这种形式\n$$ B2U_w(\\vec{x}) \\doteq \\sum_{i=0}^{w-1}x_i2^i $$ 这个等式中$\\doteq$表示左手边被定义为右手边。函数$B2U_w$将一个长度为$w$的0、1串映射到非负整数。它的最小值用位向量$[00\\dots0]$，它的最大值是用位向量$[11 \\dots 1]$来表示，也就是$\\sum^{w-1}_{i=0} 2^i = 2^w - 1$。所以，函数$B2U_w$能够被定义为一个(双)映射:\n$$ B2U_w: {0, 1}_w \\longleftrightarrow { 0, \\dots 2^w - 1 } $$\n二进制补码(two\u0026rsquo;s-complement) 如果要表示负数值的话，最常见的有符号数的计算机表示方式就是二进制补码形式。对于有符号的数来说，最高位会被定义为负权(negative weight)，用于表示正负号。我们用函数$B2T_w$(表示“二进制到二进制补码”，长度为w)来表示这种解释: $$ B2T_2(\\vec{x}) \\doteq -x_{w-1}2^{w-1} + \\sum_{i=0}^{w-2} x_i2^i $$ 最高有效位也被称为符号位(sign bit)。当被设置为1时，表示值为负，当被设置为0时，表示为正。它能表示的最小值是位向量$[10\\dots0]$，其整数值为$TMin_w \\doteq -2^{w-1}$，最大值是位向量$[01\\dots1]$，其整数值为$TMax_w \\doteq \\sum^{w-2}_{i=0}2^i = 2^{w-1} - 1$。所以函数$B2T_w$也同样是一个(双)映射: $$ B2U_w: { 0, 1 }^w \\longleftrightarrow { -2^{w-1}, \\dots, 2^{w-1} - 1} $$ 更直观的求某个负整数的二进制的方式(求这个负整数去掉负值的对应正整数的补码)是先得到这个负整数去掉负数部分的二进制值，然后对这个值的每一个位取反(0-\u0026gt;1, 1-\u0026gt;0)，然后在得到的结果上加1，就是这个负整数的二进制表示。补码表示法使得加法和减法运算可以统一处理，不需要单独处理减法。即A - B可以表示为A + (-B)。\n二进制反码(one\u0026rsquo;s-complement)与符号数值（Sign-Magnitude) 二进制反码和二进制补码类似，不过最高有效位的权是$-(2^{w-1}-1)$而不是$-2^{w-1}$: $$ B2O_w(\\vec{x}) \\doteq -x_{w-1}(2^{w-1} - 1) + \\sum^{w-2}_{i=0}x_i2^i $$ 简而言之，二进制反码就是对某个负整数求去掉符号位的数的每一个位取反，并不需要在之后的结果上加1。\n符号数值的表示为: 最高有效位是符号位，剩下的位则不变: $$ B2S_w(\\vec{x}) \\doteq (-1)^{x_{w-1}} \\cdot (\\sum^{w-2}_{i=0}x_i2^i) $$\n","date":"July 24, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0-%E4%BF%A1%E6%81%AF%E7%9A%84%E8%A1%A8%E7%A4%BA%E5%92%8C%E5%A4%84%E7%90%86/","series":[],"smallImg":"","tags":[{"title":"计算机系统","url":"/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"}],"timestamp":1721801959,"title":"计算机系统学习-信息的表示和处理"},{"authors":[],"categories":[{"title":"Tutorial","url":"/categories/tutorial/"}],"content":"“《深入理解计算机系统》（Computer Systems: A Programmer\u0026rsquo;s Perspective, CS: APP）这本书的主要读者是哪些想要通过学习计算机系统的内在运作而提高自身技能的程序员”。计算机知识过于庞大，在工作中我总是会遇到一些底层相关的知识点，而掌握这些知识点需要深入学习透计算机系统的知识。所以，我会根据这本书的内容，记录有用的计算机系统知识点，以便于及时复习巩固。后面的博客将根据书的章节来划分，毕竟我自己也不好按照自己的方式进行布局，那样反而会显得又些混乱:)\n","date":"July 24, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0-%E5%89%8D%E8%A8%80/","series":[],"smallImg":"","tags":[{"title":"计算机系统","url":"/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"}],"timestamp":1721792558,"title":"计算机系统学习-前言"},{"authors":[],"categories":[{"title":"Note","url":"/categories/note/"}],"content":"这部分将记录线性代数的相关知识点，简单用于复习以便工作中使用。\n基本概念 矩阵的类型 对角矩阵 一种特殊的方阵，其中非对角线元素全部为零。对角矩阵的转置等于自身。\n正交矩阵 定义\n一个$n \\times n$的矩阵$Q$被称为正交矩阵，如果它的转置等于它的逆,即: $$ Q^TQ = QQ^T = I $$ 其中$Q^T$是$Q$的转置矩阵，$I$是$n \\times n$的单位矩阵。\n换句话说，正交矩阵满足$Q^T = Q^{-1}$\n正交矩阵的性质\n保持长度: 正交矩阵$Q$变换向量时保持向量的长度(或范数)。对于任意向量: $||Qx|| = ||x||$ 保持内积: 正交矩阵$Q$变换向量时保持向量的内积。对于任意向量$x$和$y$: $(Qx)·(Qy) = x·y$ 列向量正交: 正交矩阵的列向量是两两正交的($r_1 · r_2 = 0$)，并且每个列向量的长度为1(单位向量)。换句话说，正交矩阵的列向量组成了一个正交归一基。 行向量正交：正交矩阵的行向量是两两正交的($r_1 · r_2 = 0$)，并且每个行向量的长度为1(单位向量)。 行列式: 正交矩阵的行列式的绝对值为1，即$|det(Q)| = 1$ 矩阵的计算 矩阵的线性运算 矩阵的加减\n矩阵的加减可正常计算，可以使用交换律\n$$A + B = B + A$$\n$$A - B = A + (- B)$$\n数与矩阵相乘\n满足交换律、结合律、分配律\n$$(\\lambda \\mu) A = \\lambda (\\mu A)$$\n$$(\\lambda + \\mu)A = \\lambda A + \\mu A$$\n$$\\mu (A + B) = \\mu A + \\mu B)$$\n矩阵与矩阵相乘 矩阵乘法不满足交换律，但可以使用结合律和分配律\n$$(AB)C = A(BC)$$\n$$\\mu (AB) = (\\mu A)B = A(\\mu B)$$\n$$ A(B + C) = AB + AC$$\n矩阵的转置 $$(A^T)^T = A$$\n$$(A + B)^T = A^T + B^T$$\n$$(\\lambda A)^T = \\lambda A^T$$\n$$(AB)^T = B^TA^T$$\n逆的运算 对于多个矩阵的乘积的逆，逆运算的顺序需要反过来。比如给定三个可逆矩阵$A$，$B$和$C$，有以下公式: $$ (ABC)^{-1} = C^{-1}B^{-1}A^{-1} $$\n向量 对于任意向量$$w$$和$$x$$(无论是列向量还是行向量)，它们的转置乘积满足以下关系: $$ w^Tx = x^Tw $$\n行列式 (Determinant) 行列式是与方阵（即行数和列数相同的矩阵）相关的一个标量值。\n定义\n对于一个$n \\times n$的方阵$A$，行列式记作$det(A)$或$|A|$。行列式的具体计算方法随矩阵的维数变化而不同。\n$1\\times 1$矩阵: 对于 $A = [a]$，其行列式为$det(A) = a$. $2 \\times 2$矩阵: 对于 $A = \\begin{pmatrix} a \u0026amp; b \\\\ c \u0026amp; d \\ \\end{pmatrix}$，其行列式为$det(A) = ad - bc$. $3 \\times 3$矩阵: 对于$A = \\begin{pmatrix} a \u0026amp; b \u0026amp; c \\\\ d \u0026amp; e \u0026amp; f \\\\ g \u0026amp; h \u0026amp; i \\end{pmatrix}$，其行列式为$det(A) = a(ei - fh) - b(di - fg) + c(dh - eg)$. 对于更高维的矩阵，行列式可以通过递归展开，或使用高斯消元法将矩阵转化为上三角矩阵，再计算对角线元素的乘积。\n行列式的性质\n如果行列式为0，矩阵不可逆。 行列式反映了线性变换的体积缩放因子和方向(正或负)。 行列式在行交换时会改变符号，在行或列加倍时也会相应改变。 特征值 (Eigenvalue) 特征值的定义 设$A$是一个$n \\times n$的方阵。如果存在一个非零向量$v$和一个标量$\\lambda$，使得$Av = \\lambda v$，那么标量$\\lambda$称为矩阵$A$的特征值，对应的非零向量$v$称为特征向量。\n在$Av = \\lambda v$中，$Av$表示矩阵$A$作用于向量$v$，**结果是向量$v$仅仅被拉伸或缩短，而没有改变方向。**这个拉伸或缩短的比例因子就是特征值$\\lambda$.\n求特征值 求解特征值的过程通常涉及以下步骤:\n特征多项式: 计算矩阵A得特征多项式。这个多项式通过求解下面的特征方程得到: $$ AI = \\lambda I \\ A - \\lambda I = 0 \\ det(A - \\lambda I) = 0 $$ $I$是$n \\times n$的单位矩阵，$det$表示行列式。\n求根: 特征多项式是关于$\\lambda$的n次多项式，其根就是矩阵$A$的特征值。\n例子:一个简单的$2 \\times 2$矩阵: $$ A = \\begin{pmatrix} 4 \u0026amp; 1 \\\\ 2 \u0026amp; 3 \\end{pmatrix} $$ 下面求解其特征值:\n构建特征方程: $$ det(A - \\lambda I) = \\det \\begin{pmatrix} 4 - \\lambda \u0026amp; 1 \\\\ 2 \u0026amp; 3 - \\lambda \\end{pmatrix} = (4 - \\lambda)(3 - \\lambda) - 2 = \\lambda^2 - 7\\lambda + 10 = 0 \\ $$ 求解这个特征多项式: $$ \\lambda ^ 2 - 7 \\lambda + 10 = 0 $$ ​\t我们可以得到特征值: $$ \\lambda_1 = 5, \\quad \\lambda_2 = 2 $$ ​\t对于$\\lambda = 5$: $$ \\begin{pmatrix} 4 - 5 \u0026amp; 1 \\\\ 2 \u0026amp; 3 - 5 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} -1 \u0026amp; 1 \\\\ 2 \u0026amp; -2 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$ ​\t我们得到$-v_1 + v_2 = 0$，即$v_2 = v_1$。因此，对应于$\\lambda = 5$的特征向量是$v = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. ​\t对于$\\lambda = 2$:\n$$ \\begin{pmatrix} 4 - 2 \u0026 1 \\\\\\\\ 2 \u0026 3 - 2 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\\\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 2 \u0026 1 \\\\\\\\ 2 \u0026 1 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\\\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\\\\\ 0 \\end{pmatrix} $$ ​\t我们得到$2v_1 + v_2 = 0$，即$v_2 = -2v_1$。因此，对应于$\\lambda = 2$的特征向量是$v = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$.\n特征值分解 (Eigenvalue Decomposition EVD) 特征值分解是一种矩阵分解技术，用于将矩阵分解为其特征值和特征向量的组合。\n特征值分解的定义\n对于一个$n\\times n$的方阵$A$，如果$A$可以分解为: $$ A = PDP^{-1} $$ 其中\n$P$是由矩阵$A$的特征向量按列组成的矩阵。 $D$是一个对角矩阵，其对角元素是矩阵$A$的特征值。 那么，这个分解就是矩阵$A$的特征值分解。\n注意，并不是所有方阵如果有特征值和特征向量的话，都可以进行特征值分解。判断一个方阵是否可以进行特征值分解的关键问题在于其特征向量(如果有)是否形成一个基(即线性无关)，以下是几种情况:\n具有唯一特征值的矩阵: 如果一个矩阵的特征值是唯一的，那么该矩阵可能没有足够的线性无关特征向量。比如某些不可对角化的方阵，即使有特征值和特征向量，仍然不可对角化。 重复特征值: 对于具有重复特征值的矩阵，如果对应的特征向量的数量不足，那么矩阵也是不可对角化的。 一言以蔽之，具有足够多的线性无关特征向量的矩阵(方阵)是可对角化的，可以进行特征值分解。\n然而，要进行特征值分解，矩阵$A$必须为方阵。如果$A$不是方阵，即行和列不相同时，还可以对矩阵进行特征值分解吗？此时需要使用奇异值分解(SVD)来解决(后面会提到)。\n矩阵的秩 (rank) 矩阵的秩是指矩阵中线性无关行或列的最大数量\n计算矩阵的秩 行简化（Row Reduction）：通过高斯消元法（Gaussian Elimination）或行最简形式（Row Echelon Form），可以将矩阵转换成上三角矩阵或阶梯形矩阵。非零行的数量就是矩阵的秩。 极大无关子集（Maximal Independent Subset）：寻找矩阵中最大线性无关的行或列集。 行列式（Determinants）：对于方阵（即 $n \\times n$ 矩阵），如果行列式不为零，则矩阵的秩是 $n$。如果行列式为零，可以通过计算子矩阵的行列式来确定秩。 奇异值分解（Singular Value Decomposition, SVD）：通过SVD分解矩阵，非零奇异值的数量就是矩阵的秩。 例子:\n给定一个矩阵A: $$ A = \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 4 \u0026amp; 5 \u0026amp; 6 \\\\ 7 \u0026amp; 8 \u0026amp; 9 \\end{pmatrix} $$ 通过高斯消元法: $$ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 4 \u0026amp; 5 \u0026amp; 6 \\\\ 7 \u0026amp; 8 \u0026amp; 9 \\end{pmatrix} \\xrightarrow{\\text{R2} - 4\\text{R1}} \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 0 \u0026amp; -3 \u0026amp; -6 \\\\ 7 \u0026amp; 8 \u0026amp; 9 \\end{pmatrix} \\xrightarrow{\\text{R3} - 7\\text{R1}} \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 0 \u0026amp; -3 \u0026amp; -6 \\\\ 0 \u0026amp; -6 \u0026amp; -12 \\end{pmatrix} \\xrightarrow{\\text{R3} - 2\\text{R2}} \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 0 \u0026amp; -3 \u0026amp; -6 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\end{pmatrix} $$\n行简化后得到的矩阵有两个非零行，因此矩阵$A$的秩为2。\n秩亏矩阵 (rank-deficient) 秩亏矩阵指的是那些秩小于其最大可能值的矩阵。具体来说：\n对于一个$m \\times n$的矩阵$A$，其最大可能秩是 $min(m,n)$。 如果矩阵$A$的秩 $𝑟 \u0026lt; min⁡(𝑚,𝑛)$，那么 $A$ 就被称为秩亏矩阵。 秩亏矩阵的特性\n行列式为零：如果一个方阵是rank-deficient的，它的行列式为零。 零特征值：如果一个矩阵$A$是rank-deficient的，它的特征值分解中会包含至少一个零特征值。 线性相关性：rank-deficient矩阵的行或列之间存在线性相关性。 奇异值分解（Singular Value Decomposition, SVD） https://www.cnblogs.com/sun-a/p/13543735.html\n奇异值分解是将任意的$m \\times n$矩阵$A$分解为三个矩阵乘积的方法。这三个矩阵分别是一个正交矩阵、一个对角矩阵和另一个正交矩阵。\n奇异值分解的定义\n对于一个$m * n$的矩阵$A$，其奇异值分解表示为: $$ A = U\\Sigma V^T $$ 其中：\n$U$是一个$m \\times m$的正交矩阵，成为左奇异矩阵。 $\\Sigma$是一个$m \\times n$的对角矩阵，成为奇异值矩阵，其对角元素是$A$的奇异值。 $V$是一个$n \\times n$的正交矩阵，成为右奇异矩阵。 满足下面的内容: $$ UU^T = I \\ VV^T = I \\ \\Sigma = diag(\\sigma_1, \\sigma_2, \u0026hellip; \\sigma_n) \\ \\sigma_1 \\geq \\sigma_2 \\geq \\sigma_3 \\geq \u0026hellip; \\geq \\sigma_n \\geq 0 \\ p = min(m, n) $$ $U\\Sigma V^T$称为矩阵$A$的奇异值分解，$\\sigma$称为矩阵$A$的奇异值，$U$的列向量称为左奇异向量，$V$的列向量称为右奇异向量。\n奇异值分解的计算步骤\n计算$A^TA$和$AA^T$: 找到矩阵$A$的转置并计算这两个矩阵的乘积。 求特征值和特征向量: 分别计算$A^TA$和$AA^T$的特征值和特征向量。 构造$U$和$V$: 使用$AA^T$的特征向量构造$U$，使用$A^TA$的特征向量构造$V$。 构造$\\Sigma$: 奇异值矩阵$\\Sigma$的对角元素是$A^TA$或$AA^T$的非负特征值的平方根。 奇异值分解的性质\n设矩阵$A$的奇异值分解为$A = U\\Sigma V^T$，以下关系成立\n$A^TA = (U\\Sigma V^T)^T(U\\Sigma V^T) = V(\\Sigma^T \\Sigma)V^T$ $AA^T = (U\\Sigma V^T)(U\\Sigma V^T)^T = U(\\Sigma \\Sigma ^T)U^T$ 即，矩阵$A^TA$和$AA^T$的特征分解存在，且可以由矩阵$A$的奇异值分解的矩阵表示。$V$的列向量是$A^TA$的特征向量，$U$的列向量是$AA^T$的特征向量，$\\Sigma$的奇异值是$A^TA$和$AA^T$的特征值的平方根。\n在矩阵$A$的奇异值分解中，奇异值、左奇异向量和右奇异向量之间存在下面的关系: $$ AV = U\\Sigma \\ Av_j = \\sigma_j u_j, j = 1, 2, \u0026hellip;, n $$ 类似的，奇异值、右奇异向量和和左奇异向量之间存在下面的关系: $$ A^TU = V\\Sigma^T \\ A^Tu_j = \\sigma_j v_j, j = 1, 2, \u0026hellip;, n \\ A^Tu_j = 0, j = n + 1, n + 2, \u0026hellip;, m \\ $$\n矩阵$A$的奇异值分解中，奇异值$\\sigma_1$，$\\sigma_2$，\u0026hellip; $\\sigma_n$是唯一的，而$U$和$V$不是唯一的。\n矩阵$A$和$\\Sigma$的秩相等，等于正奇异值$\\sigma_i$的个数r(包含重复的奇异值)(即奇异值的数量等于矩阵$A$的秩)。\n","date":"July 8, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/posts/linear_algebra/linear_algrebra_1/","series":[],"smallImg":"","tags":[{"title":"Math","url":"/tags/math/"}],"timestamp":1720430726,"title":"线性代数相关知识点"},{"authors":[],"categories":[{"title":"Note","url":"/categories/note/"}],"content":"这一部分将探讨vim的寄存器的使用。\n复制与粘贴 vim的复制和删除都会将信息存入到寄存器中，所以说vim的删除可以视为剪切。\n使用xp可以调换光标之后的两个字符。\n使用ddp可以调换当前行与它的下一行。\n使用yyp可以对当前行进行复制粘贴操作。\n使用yiw可以对当前单词进行复制。\n使用diw可以对当前单词进行删除(剪切)。\n深入理解vim寄存器 加深对vim寄存器的理解以及使用，可以让我们更好的在vim中进行复制和删除。我们可以通过\u0026quot;{register}即\u0026quot;加上寄存器前缀的方式制定要用的寄存器。如果不指明，vim将缺省使用无名寄存器。\n有名寄存器(\u0026ldquo;a-\u0026ldquo;z) 比如，我们可以使用\u0026quot;ayiw来将当前单词复制到寄存器a中，使用\u0026quot;bdd来将当前正行文本剪切到寄存器b中。之后，我们可以事情\u0026quot;ap、\u0026quot;bp来粘贴来自寄存器a和b的信息。\n上述是普通模式的命令，我们也可以使用Ex命令来实现:\n:delete c:把当前行剪切到寄存器c :put c: 将寄存器c粘贴至当前光标所在行之下 无名寄存器 如果没有指定要用的寄存器，vim将缺省使用无名寄存器，可用\u0026quot;来表示，即\u0026quot;\u0026quot;p等同于p的指令。换句话说，之前介绍的大部分删除剪切快捷键如果没有指明寄存器的话，都是将信息写入到无名寄存器中。所以，正常使用过程中无名寄存器非常容易被覆盖。\n复制专用寄存器(\u0026ldquo;0) 复制专用寄存器当且仅当使用y{motion}的时候才会被赋值，使用x,s,c{motion}均不会覆盖该寄存器，所以复制寄存器是很稳定的，复制专用寄存器的前缀是0，即可以使用\u0026quot;0p来将复制专用寄存器中的信息粘贴。比如我们使用yy复制了一条信息，然后又使用了diw删除了一个单词，此时无名寄存器会存储diw表示的单词，而如果我们使用\u0026quot;0p则仍然会粘贴之前yy复制的信息。\n系统剪贴板(\"+) 如果想从vim复制文本到外部程序，必须要使用系统剪贴板。\n在vim的复制或者删除命令之前加入\u0026quot;+，即可将相应的文本捕获至系统剪贴板。\n","date":"July 7, 2024","img":"https://KasterMist.com/images/logo/vim.png","lang":"en","langName":"English","largeImg":"","permalink":"/posts/linux/vim-5/","series":[],"smallImg":"","tags":[{"title":"Linux","url":"/tags/linux/"},{"title":"Linux Command","url":"/tags/linux-command/"}],"timestamp":1720335591,"title":"Vim用法 (五) 深入理解Vim的寄存器"},{"authors":[],"categories":[{"title":"Note","url":"/categories/note/"}],"content":"使用vim在处理简单的代码文件的时候非常方便。但是如果想要处理一个大型项目的话，则较为麻烦。因为浏览大型项目需要不断跳转代码，所以这篇博客将介绍如何使用vim进行代码跳转。\n使用ctags配置标签文件 使用ctags可以创建代码库的索引。ctags需要自行下载。在当前工作路径下调用ctags -R .(.表示当前路径，可以省略)即可创建在tags路径下创建标签文件。\n更新配置文件 这个配置文件是静态的，如果项目代码修改了话，我们也需要实时修改tag文件。我们可以通过使用快捷键映射来进行tag更新，比如在~/.vimrc中添加nnoremap \u0026lt;f5\u0026gt; :!ctags -R\u0026lt;CR\u0026gt;即可在vim打开的情况下按\u0026lt;f5\u0026gt;更新配置文件。\n我们也可以编辑vim的自动命令(autocommand)功能允许我们在某个事件发生时调用一条命令，可以在~/.vimrc中创建下面这条命令来每次保存文件的时候自动调用ctags:\nautocmd BufWritePost * call system(\u0026#34;ctags -R\u0026#34;) 不过这种方式会让所有vim打开的文件再保存时都会更新(创建)ctags，如果我们不想对某个文件进行ctags配置标签，这种方式就不太可行了。我们可以自定义函数来识别某个文件是否之前已经包含在了ctags标签文件里面，如果在的话，就进行更新ctags。而因为命令是在~/.vimrc中，我们需要获取保存文件的路径来替换ctags -R的路径，不然每次创建ctags就会一直在~/.vimrc所在路径当中。\n下面是编写的~/.vimrc:\n\u0026#34; 设置autocmd来在保存文件时自动更新ctags autocmd BufWritePost * call UpdateCtags() \u0026#34; 定义UpdateCtags函数，让之前设置过ctags的文件更新ctags，没有ctags的文件并不会创建ctags function! UpdateCtags() \u0026#34; 获取当前文件路径 let l:dir = expand(\u0026#39;%:p:h\u0026#39;) if filereadable(l:dir . \u0026#39;/tags\u0026#39;) \u0026#34; 在当前文件所在目录更新tags文件 execute \u0026#39;silent! !ctags -R \u0026#39; . l:dir \u0026#34; 排序tag文件 execute \u0026#39;silent! !sort \u0026#39; . l:dir . \u0026#39;/tags -o \u0026#39; . l:dir . \u0026#39;/tags\u0026#39; echo \u0026#34;Ctags updated and sorted\u0026#34; endif endfunction 关键字跳转 在配置好ctags后，我们可以在当前光标所在关键字下输入\u0026lt;Ctrl-]\u0026gt;，这样就会自动跳转到关键字第一次声明的地方。相同的Ex语句是:tag \u0026lt;keyword\u0026gt;，我们可以通过:tag \u0026lt;keyword\u0026gt;直接输入某个关键字来跳转。使用\u0026lt;Ctrl-t\u0026gt;会充当后退按钮，跳回到keywords的地方。\n新建标签页进行跳转 不过，如果我们想要实现像一般IDE那样多个标签页跳转的话，可以通过:tab tag \u0026lt;keyword\u0026gt;的方式来实现。如果想要在当前光标下不输入keyword就能实现的话，可以调用tab tag \u0026lt;Ctrl-r\u0026gt;\u0026lt;Ctrl-w\u0026gt;，这样tag后面会补全光标下的keyword内容。\n我们也可以使用快捷键来新建标签页进行跳转。不过需要注意的是，如果当前标签页已经打开了对应的文件，vim是检测不到的，会新建一个新的标签页再次打开对应的文件，我们可以通过编写函数来自定义查找当前标签页，如果已经有对应的文件，则直接跳转到对应文件的标签页，没有的话就再创建标签页。\n下面是在~/.vimrc的实现方式：\nfunction! OpenTagInNewTab() let tag_name = expand(\u0026#39;\u0026lt;cword\u0026gt;\u0026#39;) \u0026#34; 获取标签定义位置 let tag_info = taglist(tag_name) if empty(tag_info) echo \u0026#34;Tag not found\u0026#34; return endif let file_name = tag_info[0].filename \u0026#34; 遍历所有标签页，查找包含目标文件的标签页 let found = 0 for i in range(1, tabpagenr(\u0026#39;$\u0026#39;)) execute i . \u0026#39;tabnext\u0026#39; if bufname(\u0026#39;%\u0026#39;) == file_name execute \u0026#39;tag \u0026#39; . tag_name let found = 1 break endif endfor \u0026#34; 如果未找到，打开新标签页 if !found execute \u0026#39;tabedit \u0026#39; . file_name execute \u0026#39;tag \u0026#39; . tag_name endif endfunction nnoremap \u0026lt;Leader\u0026gt; :call OpenTagInNewTab()\u0026lt;CR\u0026gt; 我们设置了快捷键\u0026lt;leader\u0026gt;(\\按键)，使用这个按键即可调用自定义的函数。\n注: ~/.vimrc中的命令都是使用Vimscript编写的。Vimscript是一种特定于 Vim 编辑器的脚本语言，用于编写vim配置文件(.vimrc)和插件。如果想要自定义更多的功能，需要系统性的学习Vimscript语言。\n在文件间跳转 遍历跳转列表 vim可以通过快捷键在文件内跳转行以及文件之间跳转行。我们需要知道vim中的对跳转命令的定义。在vim中，任何改变当前窗口活动文件的命令都可以被称为跳转命令。此外，使用类似[count]G命令直接跳到指定的行号也会被当成一次跳转，而每次向上向下移动一行不算。可以理解为，大范围的动作命令可能会被当成跳转，小范围的动作命令会被当作成移动。\n使用:jumps可以查看跳转列表的内容。\n使用\u0026lt;C-o\u0026gt;和\u0026lt;C-i\u0026gt;可以根据跳转列表的内容在新文件以及就文件之间进行跳转。\n遍历改变列表 vim在编辑期间回维护一张表，里面记载着对每个缓冲区所做的修改。\n使用:changes可以查看改变列表的内容。\n我们可以使用g;和g,来反向或正向遍历改变列表 (改变列表里面的改变从上到下是由旧到新排列，但是index则是从大到小，使用g;则是index递增遍历，即往前遍历改变列表)。\n生成标签便于跳转 可参考vim用法(一)中的生成标签功能\n","date":"June 14, 2024","img":"https://KasterMist.com/images/logo/vim.png","lang":"en","langName":"English","largeImg":"","permalink":"/posts/linux/vim-4/","series":[],"smallImg":"","tags":[{"title":"Linux","url":"/tags/linux/"},{"title":"Linux Command","url":"/tags/linux-command/"}],"timestamp":1718328662,"title":"Vim用法 (四) 代码跳转详解"},{"authors":[],"categories":[{"title":"Note","url":"/categories/note/"}],"content":"这部分将着重介绍一下如何使用vim打开多个文件以及如何管理打开的文件。\n使用缓冲区打开多个文件 比如在包含多个.txt的路径下调用vim *.txt可以打开多个txt文件。此时窗口内的缓冲区对应着第一个文件，其他文件在窗口中是不可见的。\n可以通过:ls来查看缓冲区的文件列表。\n切换显示缓冲区的文件：\n:bprev: 反向遍历缓冲区的文件 (显示当前显示文件的上一个文件) :bnext: 正向遍历缓冲区的文件 (显示当前显示文件的下一个文件) :bfirst: 跳转到缓冲区文件列表的开头 :blast: 跳转到缓冲区文件列表的结尾 删除缓冲区的某个文件: bdelete N1，N1是在:ls后显示的文件的序号，同时也可以直接输入文件名进行删除。\n将工作区切分称窗口 vim启动时只会打开单个窗口。下面介绍几个水平切分和垂直切分窗口的方法：\n\u0026lt;Ctrl-w\u0026gt;s: 水平切分窗口，两个窗口显示相同的文件信息，同:sp \u0026lt;Ctrl-w\u0026gt;v: 垂直切分窗口，两个窗口显示相同的文件信息，同:vsp :edit {filename}: 在当前窗口下打开另一个文件，即切换为另一个文件显示。 :sp {filename}: 水平切分窗口，新的窗口打开的文件是filename. :vsp {filename}: 垂直切分窗口，新的窗口打开的文件是filename. 窗口之间切换:\n\u0026lt;Ctrl-w\u0026gt;w: 在窗口间循环切换 \u0026lt;Ctrl-w\u0026gt;h: 切换到左边的窗口 \u0026lt;Ctrl-w\u0026gt;j: 切换到右边的窗口 \u0026lt;Ctrl-w\u0026gt;k: 切换到上边的窗口 \u0026lt;Ctrl-w\u0026gt;l: 切换到下边的窗口 实际上，\u0026lt;Ctrl-w\u0026gt;\u0026lt;Ctrl-w\u0026gt;完成的功能和\u0026lt;Ctrl-w\u0026gt;w相同，如果想要多次切换活动窗口的话，一种简单的方法就是按住Ctrl键后输入ww (或者wj等其他切换命令)。\n可以使用:qa,:qa!关闭所有的窗口。\n用标签页将窗口分组 vim同样可以像其他IDE一样是用标签页来管理多个文件。vim的标签页更像是Linux中的虚拟桌面，新的标签页并不会打乱之前标签页设置的窗口排版。\n打开和关闭标签页\n打开标签页: :tabe {filename}，是:tabedit {filename}的简写。 关闭当前标签页: :tabc，是:tabclose的简写。 只保留当前标签页，关闭其他标签页: :tabo，是:tabonly的简写。 切换标签页\n切换到编号为{N}的标签页: :tabn {N}，是:tabnext {N}的简写。普通模式下也可以使用{N}gt来跳转。 切换到下一标签页: :tabn，是:tabnext的简写。普通模式下也可以使用gt来跳转。 切换到上一个标签页: :tabp，是tabprevious的简写。普通模式下也可以使用gT来跳转。 重排标签页\n使用:tabmove {N}即可将当前标签页排序到N的位置处。当{N}为0时，当前标签页会被移到开头。当省略了{N}，当前标签页会被移动到结尾。 ","date":"June 13, 2024","img":"https://KasterMist.com/images/logo/vim.png","lang":"en","langName":"English","largeImg":"","permalink":"/posts/linux/vim-3/","series":[],"smallImg":"","tags":[{"title":"Linux","url":"/tags/linux/"},{"title":"Linux Command","url":"/tags/linux-command/"}],"timestamp":1718269031,"title":"Vim用法 (三) -- 管理多个文件"},{"authors":[],"categories":[{"title":"Note","url":"/categories/note/"}],"content":"大部分Vim命令都是在非插入模式中执行，不过插入模式中仍有一些能够提高效率的功能。此外，可视模式中也有一些快捷方式来快速选区区域。这篇博客将主要添加一些不那么常用的功能，根据需求来选择是否使用。\n插入模式中及时更正错误 (不常用) 除了使用退格键，我们还可以使用Ctrl+字符来实现删除特定的信息:\n\u0026lt;Ctrl-h\u0026gt;: 删除前一个字符(同退格键) \u0026lt;Ctrl-w\u0026gt;: 删除前一个单词 \u0026lt;Ctrl-u\u0026gt;: 删至行首 这些命令在bash shell中也可以使用\n普通模式 Esc: 切换到普通模式 \u0026lt;Ctrl-[\u0026gt;: 切换到普通模式 \u0026lt;Ctrl-o\u0026gt;: 切换到插入-普通模式 插入-普通模式是一种特殊的模式，它可以让我们执行一次普通模式命令，然后自动切换回插入模式\n粘贴寄存器中的文本 (有些时候很有用) 如果在visual模式下使用\u0026quot;+y把文本放入了寄存器中，可以在插入模式下使用\u0026lt;Ctrl-r\u0026gt;0来把寄存器中信息粘贴到光标位置。\n插入模式中执行运算 (不常用) 我们可以在插入模式中，使用\u0026lt;Ctrl-r\u0026gt;= (Ctrl键加上r键再加上=键)来输入计算表达式，输入完后按下回车键即可在将结果插入到当前光标下。\n可视模式快捷选取区域 (自定义修改区域时很有用) 我们可以在可视模式中对精确的文本对象进行选区。假如当前光标在某个区域内，比如{}中，我们可以使用快捷键来选区整个{}区域。下面是一些快捷键使用方法 (默认下面的操作是已经在可视模式下进行，即已经按键v之后)。\n我们可以将快捷键中的i理解为inside，即覆盖某个区域内部的信息，将a理解为around，即也包括了区域标识符。标识符比如(与)的意思相同，可以进行替换。\n按键 内容 i} 选中{}内部的文本，不包括{} a\u0026quot; 选中\u0026quot;\u0026ldquo;内部的文本，包括\u0026rdquo;\u0026quot; i\u0026gt; 选中\u0026lt;\u0026gt;内部的文本，不包括\u0026lt;\u0026gt; it 某个xml标签的内部文本，不包含xml标签 at 某个xml标签的内部文本，包含xml标签 ","date":"June 12, 2024","img":"https://KasterMist.com/images/logo/vim.png","lang":"en","langName":"English","largeImg":"","permalink":"/posts/linux/vim-2/","series":[],"smallImg":"","tags":[{"title":"Linux","url":"/tags/linux/"},{"title":"Linux Command","url":"/tags/linux-command/"}],"timestamp":1718180162,"title":"Vim用法 (二) -- 模式便捷使用及拓展"},{"authors":[],"categories":[{"title":"Note","url":"/categories/note/"}],"content":"本人使用macbook一般都会设置护眼模式，即夜览模式。然而，macbook会认为夜览模式就是在晚上才会启用。当设置为永久夜览模式后，如果将外观模式设置为自动，那么macbook会默认永久使用深色外观。然而我想要实现白天为浅色外观，晚上为深色外观。这篇文章将介绍如何使用Cron调度来自动更新MAC的浅色外观和深色外观。\n在之前的Linux常用命令博客中已经提到了Cron是一个很好用的设置定时任务的命令。我们可以创建脚本来切换外观，然后通过Crontab来执行定时任务调用切换外观的脚本。\n创建脚本 首先创建切换为浅色外观和深色外观的脚本。\n深色模式脚本 dark_mode.applescript\ntell application \u0026#34;System Events\u0026#34; tell appearance preferences set dark mode to true end tell end tell 浅色模式脚本light_mode.applescript\ntell application \u0026#34;System Events\u0026#34; tell appearance preferences set dark mode to false end tell end tell 然后创建一个shell脚本toggle_dark_light.sh来调用这两个脚本，同时加入一个参数来选择调用哪一个脚本:\n#!/bin/bash if [ \u0026#34;$1\u0026#34; == \u0026#34;dark\u0026#34; ]; then osascript /path/to/dark_mode.applescript elif [ \u0026#34;$1\u0026#34; == \u0026#34;light\u0026#34; ]; then osascript /path/to/light_mode.applescript else echo \u0026#34;Invalid argument: use \u0026#39;dark\u0026#39; or \u0026#39;light\u0026#39;\u0026#34; fi (将path切换为自己的path)\n配置cron作业 使用 crontab -e 编辑 cron 表，可以添加下面的代码:\n# 每天早上7点切换到浅色模式 0 7 * * * /path/to/toggle_dark_light.sh light # 每天晚上7点切换到深色模式 0 19 * * * /path/to/toggle_dark_light.sh dark (将path切换为自己的path)\n保存后，Macbook将会启用cron作业来根据时间执行不同的任务。\n如果执行后到了指定时间会弹出询问框，可以在crontab脚本中加入yes命令自动响应，如:\n# 每天早上7点切换到浅色模式 0 7 * * * yes | /path/to/toggle_dark_light.sh light # 每天晚上7点切换到深色模式 0 19 * * * yes | /path/to/toggle_dark_light.sh dark ","date":"June 11, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/posts/linux/%E4%BD%BF%E7%94%A8cron%E8%B0%83%E6%95%B4mac%E5%A4%96%E8%A7%82/","series":[],"smallImg":"","tags":[{"title":"MacOS Usage","url":"/tags/macos-usage/"}],"timestamp":1718095315,"title":"使用Cron调整MAC外观"},{"authors":[],"categories":[{"title":"Note","url":"/categories/note/"}],"content":"在学习CUDA过程中会遇到很多问题，而这些问题过于零碎，无法进行系统化的分类，所以这篇文章主要记录平时遇到的CUDA使用的知识点。\nCUDA官方一堆的知识点: https://docs.nvidia.com/cuda/cuda-c-programming-guide/\ncuda基础概念 thread hierarchy 一个thread block的thread都在同一个multiprocessor core上，共享一个有限的内存资源。目前的GPU上一个thread block最多可以有1024个threads.\ngrid当中的thread block由data size决定，thread block数量一般都超过系统中processors的数量\n同block的threads可以在共享内存中共享数据。也可以通过__syncthreads()来同步。\nthread block clusters 更高层次的组织结构，包含多个thread blocks，适合在多个thread block之间协调任务。clusters也可以一维、二维或者三维。CUDA支持一个thread block cluster最多有8个thread block.\n在kernel函数中使用__cluster_dims__(X, Y, Z)即可以使用thread block cluster\nmemory hierarchy 每个thread都有各自的private local memory，一个thread block包含一个里面的thread都可以访问的shared memory区域。所有的thread可以访问一个相同的global memory. 下面是英伟达官网给出的一个层级关系以及结构图:\nGPU硬件中的对应关系 https://zhuanlan.zhihu.com/p/670063380\n在GPU硬件中，运算单元被划分成了SM (stream multiprocessor) \u0026ndash;\u0026gt;SP (streaming processor)的层次，而相对应的在软件上也划分了grid\u0026ndash;\u0026gt;block\u0026ndash;\u0026gt;thread这样的结构。\nSM (Stream Multiprocessor) 流式多处理器 每个SM都有自己的寄存器文件、共享内存和缓存等资源，并且拥有很多Core资源，可以同时执行多个线程，可以说SM是GPU中的可独立计算单元。\n划分SM的原因:\n通过划分SM的主要目的是提高GPU的并行计算能力和资源利用率。 在划分SM后，GPU就可以通过将将计算任务分解成多个小部分的工作分配给不同的SM并行执行，从而加快速度。 划分SM还可以避免不同计算任务之间的资源竞争，提高GPU并行性能。 一个SM由多个CUDA core组成，每个SM根据GPU架构的不同有不同数量的CUDA core.在这种情况下，由哪些线程资源来占有这些稀缺资源执行任务，就离不开Warp Scheduler调度器。\nWarp 线程束 warp是最基本的执行单元，一般一个warp包含了32个并行的thread，这些thread只能执行相同的指令。假如一个SM最大只能存储1024个线程的信息，但一个SM可以拥有超过1024个线程。此时就需要使用warp来对线程进行调度。\n在一个SM中，可以有多个warp同时处于“活动”状态，但在某一时刻，SM只能真正执行某些warp，而其他warp会等待调度。 Warp并发主要体现在调度上。SM在同一个时间片内可以执行多个warp，也就是并发调度，但并不意味着这些warp真正“并行”执行。SM 会根据资源（如寄存器、执行单元等）在warp之间切换，以保持高效的利用率。因此，在一个SM中，多个warp是并发调度的，但具体的并行执行量取决于硬件资源。 一个warp内的线程：在硬件支持下，warp内的32个线程会一起调度，并且在同一个指令上执行不同的数据。尽管硬件层面可能会将这些线程分批处理，但从逻辑上它们是同时执行的。 SM内多个warp：在一个SM内，可以有多个warp被并发调度，但在某一时刻，只有部分warp真正在执行。其他warp可能在等待调度或因资源限制而暂时停顿。 CUDA编程模型与GPU的映射关系 在软件层面，我们会将计算任务分成多个可并行的子块，交给thread block来计算，在thread block内部，我们再将任务进一步划分成多块，由每个thread计算。GPU硬件也是采用了分层次的组织方式，被划分成多个SM，每个SM内部又有多个CUDA Core。CUDA thread和thread block最终是运行在CUDA Core和SM上面。\n一个grid可以包含多个SM，也可以访问global memory和constant memory\n一个block只能存在于一个SM中，而且一个SM包含多个block，每个block内的thread可以访问自己block的shared memory\n一个block有多个warp，每个warp有32个thread\n下图展示了一部分的映射关系:\nc++中与GPU有关的操作 thrust thrust是C++的一个扩展库，其中thrust::device_vector用于在设备（GPU）内存中存储数据。我们可以在host中通过创建thrust::device_vector来创建一个存储在device的vector并赋值。下面是一个例子:\n#include \u0026lt;thrust/device_vector.h\u0026gt; #include \u0026lt;thrust/host_vector.h\u0026gt; #include \u0026lt;iostream\u0026gt; int main() { // 创建并初始化 host_vector thrust::host_vector\u0026lt;int\u0026gt; h_vec(5); h_vec[0] = 1; h_vec[1] = 2; h_vec[2] = 3; h_vec[3] = 4; h_vec[4] = 5; // 将数据从 host_vector 复制到 device_vector thrust::device_vector\u0026lt;int\u0026gt; d_vec = h_vec; // 可以在 CPU 上通过 host_vector 访问 device_vector 的内容 thrust::host_vector\u0026lt;int\u0026gt; h_vec_copy = d_vec; for (int i = 0; i \u0026lt; h_vec_copy.size(); i++) { std::cout \u0026lt;\u0026lt; h_vec_copy[i] \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } std::cout \u0026lt;\u0026lt; std::endl; return 0; } 此外，我们也可以直接对device_vector进行赋值，比如thrust::device_vector\u0026lt;int\u0026gt; d_vec = {1, 1, 1};，但是在host端我们仍然无法直接访问device_vector的值。\n使用thrust的拓展可以简化CUDA编程，尤其是内存的管理和数据传输。\nthrust::copy用于在GPU和CPU之间复制数据。下面的例子是具体用法:\nint main() { // 创建主机向量并初始化 thrust::host_vector\u0026lt;int\u0026gt; h_vec(5); h_vec[0] = 10; h_vec[1] = 20; h_vec[2] = 30; h_vec[3] = 40; h_vec[4] = 50; // 创建设备向量并分配内存 thrust::device_vector\u0026lt;int\u0026gt; d_vec(5); // 从主机向量复制到设备向量 thrust::copy(h_vec.begin(), h_vec.end(), d_vec.begin()); // 从设备向量复制回主机向量 thrust::host_vector\u0026lt;int\u0026gt; h_vec_copy(5); thrust::copy(d_vec.begin(), d_vec.end(), h_vec_copy.begin()); // 打印结果 for(int i = 0; i \u0026lt; h_vec_copy.size(); i++) { std::cout \u0026lt;\u0026lt; h_vec_copy[i] \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } std::cout \u0026lt;\u0026lt; std::endl; return 0; } ","date":"June 6, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/posts/cuda/cuda%E9%9B%B6%E7%A2%8E%E7%9F%A5%E8%AF%86%E7%82%B9/","series":[],"smallImg":"","tags":[{"title":"CUDA","url":"/tags/cuda/"}],"timestamp":1717638140,"title":"CUDA零碎知识点"},{"authors":[],"categories":[{"title":"Tutorial","url":"/categories/tutorial/"}],"content":"本章将介绍一下远程追踪功能以及git push, git fetch, git pull等命令的参数设置。\n远程追踪 通过之前的演示可以发现，main和o/main(origin/main)似乎是关联的，实际上main和o/main的关联关系就是由分支的“remote tracking”属性决定的。main被设定为跟踪o/main —— 这意味着为main分支指定了推送的目的地以及拉取后合并的目标。\n当克隆仓库的时候，这个属性已经设置好了。当进行克隆时, git会为远程仓库中的每个分支在本地仓库中创建一个远程分支（比如o/main）。然后再创建一个跟踪远程仓库中活动分支的本地分支，默认情况下这个本地分支会被命名为main。\n我们也可以自己定一这个属性，有两种方法：\ngit checkout -b totallyNotMain o/main: 就可以创建一个名为totallyNotMain的分支，它跟踪远程分支o/main。 git branch -u o/main totallyNotMain: 这种方式也可以实现，不过前提是totallyNotMain分支需要存在。如果已经在这个分支上，也可以省略totallyNotMain名称，即git branch -u o/main。 git命令参数设置 git push参数 git push \u0026lt;remote\u0026gt; \u0026lt;place\u0026gt;: 比如git push origin main，意思是切到本地仓库中的main分支，获取所有的提交，再到远程仓库origin中找到main分支，将远程仓库中没有的提交记录都添加上去。\n通过\u0026lt;place\u0026gt;参数来告诉git提交记录来自于main，要推送到远程仓库中的main。它实际上就是要同步的两个仓库的位置。通过指定参数告诉了git所有它需要的信息, 它会忽略目前所切换分支的属性。\n下面一个例子可以比较直观的展现git push \u0026lt;remote\u0026gt; \u0026lt;place\u0026gt;的用途。使用git checkout C0; git push origin main会产生下面的变化。虽然我们将HEAD移到了C0处，但由于设置了git push的对应的分支信息，远程仓库的main分支会得到更新。\n变化前 变化后 如果我们使用git checkout C0, git push，即并不对git指定参数的话，并不会成功进行提交，因为我们切换的HEAD并没有跟踪任何的分支。\n变化前 变化后 如果想要在git push中同时为source和destination指定\u0026lt;place\u0026gt;的话，可以使用\u0026quot;:\u0026ldquo;将二者连接起来。\ngit push origin \u0026lt;source\u0026gt;:\u0026lt;destination\u0026gt;: source可以是任何git识别的位置，比如git push origin foo^:main，git将foo^解析为一个位置，上传所有未被包含到远程仓库里main分支中的提交记录。如果要推送的destination的分支并不存在的话，git会在远程仓库中根据提供的名称创建这个分支。\ngit fetch参数 git fetch的参数与git push非常类似，只不过是将顺序反转。比如git fetch origin \u0026lt;source\u0026gt;:\u0026lt;destination\u0026gt;中\u0026lt;source\u0026gt;是远程仓库的分支位置，\u0026lt;destination\u0026gt;是本地仓库的分支位置。\ngit push与git fetch空source的情况 我们可以在git push和git fetch的时候不指定任何的source，比如:\ngit push origin :side: 这个命令会删除远程仓库的side分支(本地仓库的o/side也会被删除) git fetch origin :bugFix: 这个命令会fetch空到本地，也就是会在本地创建一个名字叫bugFix的分支 git pull参数 因为git pull可以视为git fetch和git merge的缩写，所以可以理解为用同样的参数执行git fetch然后再用git merge合并抓取到的提交记录。\n比如:\ngit pull origin foo等效于git fetch origin foo; git merge o/foo git pull origin bar:bugFix等效于git fetch origin bar:bugFix; git merge bugFix ","date":"May 22, 2024","img":"https://KasterMist.com/images/logo/git.jpg","lang":"en","langName":"English","largeImg":"","permalink":"/posts/git/git_tutorial_5/","series":[],"smallImg":"","tags":[{"title":"Git","url":"/tags/git/"}],"timestamp":1716357239,"title":"Git 学习 (五)"},{"authors":[],"categories":[{"title":"Tutorial","url":"/categories/tutorial/"}],"content":"本章将详细介绍如何使用git push和git pull，以及远程仓库的一些知识。\n远程仓库的用法 git clone 从技术上来讲，git clone命令在真实的环境下的作用是在本地创建一个远程仓库的拷贝。常用用法有: git clone [url] ,git clone [url] --branch [branch]等。\n克隆远程分支时，我们的本地仓库里面会出现一个名为\u0026quot;origin/main\u0026quot;的分支(主要的远程仓库默认命名为\u0026quot;origin\u0026quot;)，这种类型的分支叫作远程分支。切换到远程分支时，自动进入分离HEAD状态(也就是\u0026quot;origin/main\u0026quot;并不会进行更新)，因为git不能直接在这些远程分支上进行更改，我们必须在别的地方完成你的工作,(更新了远程分支之后)再用远程分享成果。\ngit fetch git远程仓库相当的操作实际可以归纳为两点:\n向远程仓库传输数据 从远程仓库获取数据 git fetch用来从远程仓库获取数据。当我们从远程仓库获取数据时, 远程分支也会更新以反映最新的远程仓库。\n下面是一个使用git fetch的实例。图中有一个远程仓库，它有两个本地仓库没有的提交。使用git fetch后，C2和C3被下载到了仓库，同时远程分支o/main也被更新，反映到了这一变化。\n变化前 变化后 git fetch完成了仅有的但是很重要的两步:\n从远程仓库下载本地仓库中缺失的提交记录 更新远程分支指针(o/main) git fetch通常通过互联网(使用http://或git://协议)与远程仓库通信。\n而git fetch并不会改变你本地仓库的状态。它不会更新你的main分支，也不会修改你磁盘上的文件。我们可以将git fetch的理解为单纯的下载操作。\ngit pull 当远程分支中有新的提交时，我们可以像合并本地分支那样来合并远程分支。实际上，由于先抓取更新再合并到本地分支这个流程很常用，我们可以使用git pull来完成这两个操作。git pull也可以理解为是git fetch和git merge的缩写。下图产生的变化可以通过git pull或git fetch和git merge生成。\n变化前 变化后 git push git push负责将本地的变更上传到指定的远程仓库，并在远程仓库上合并你的新提交记录。一旦git push完成, 其他人就可以从这个远程仓库下载新的提交成果(注:git push不带任何参数时的行为与git的一个名为push.default的配置有关。它的默认值取决于你正使用的git的版本)。\n下图展示了使用git push后的变化。远程仓库接收了C2，远程仓库的main分支指向了C2，本地文件的远程分支(o/main)也同样进行了更新，即所有的分支都完成了同步。\n变化前 变化后 处理偏移的提交 假如我们克隆了一个仓库，此时的提交是C1，当我们研发了新的功能准备提交时(记为C3)，远程仓库的提交已经变成C2了，此时我们使用git push的时候就不能直接将C3直接提交到远程仓库中，因为远程仓库最新的提交是C2，而我们的提交C3是基于之前版本的C1实现的。我们需要先合并远程最新的代码，然后才能分享。比如下面的图，我们使用git push时，最新提交的C3基于远程分支中的C1，而远程仓库中该分支已经更新到C2了，所以git会拒绝推送请求。\n解决方法 使用git rebase 最直接的方法是通过git rebase调整工作。\n下图是采用了git fetch; git rebase o/main; git push的变化: 使用git fetch会更新本地仓库中的远程分支，然后用 rebase 将我们的工作移动到最新的提交记录下，最后再用git push推送到远程仓库。\n使用git merge 使用git merge也可以进行解决。\n下图是采用了git fetch; git merge o/main; git push的变化: 使用git fetch更新了本地仓库中的远程分支，然后合并了新变更到我们的本地分支(为了包含远程仓库的变更)，最后用git push把工作推送到远程仓库。\n使用git pull 上面提到过git pull是git fetch和git merge的简写，git pull --rebase是git fetch和git rebase的简写。也就是说，使用git pull或者git pull --rebase就跟上面的结果是一样的。\n远程服务器被拒绝 如果出现远程服务器被拒绝的情况，很可能是main被锁定了，需要一些Pull Request流程来合并修改。如果直接提交(commit)到本地main，然后试图推送(push)修改，你将会收到这样类似的信息: ![远程服务器拒绝] main -\u0026gt; main (TF402455: 不允许推送(push)这个分支; 你必须使用pull request来更新这个分支.)\n出现这种情况的原因可能是远程服务器拒绝直接推送(push)提交到main，因为策略配置要求pull requests来提交更新。\n应该按照流程，新建一个分支，推送(push)这个分支并申请pull request。如果忘记并直接提交给了main，就会出现远程服务器被拒绝的情况。\n解决方法就是新建一个分支推送到远程服务器，然后reset你的main分支和远程服务器保持一致, 否则下次使用git pull并且他人的提交和你冲突的时候就会有问题。\n","date":"May 20, 2024","img":"https://KasterMist.com/images/logo/git.jpg","lang":"en","langName":"English","largeImg":"","permalink":"/posts/git/git_tutorial_4/","series":[],"smallImg":"","tags":[{"title":"Git","url":"/tags/git/"}],"timestamp":1716195897,"title":"Git 学习 (四)"},{"authors":[],"categories":[{"title":"Tutorial","url":"/categories/tutorial/"}],"content":"本章将着重介绍如何更清晰的整理提交记录，git tag和git describe的用法，以及一些复杂的样例来熟练掌握git常用指令。\n整理提交记录 git cherry-pick git cherry-pick \u0026lt;提交号\u0026gt;...可以将一些提交复制到当前所在位置(HEAD)下面。\n举个例子，假如我们想要将side分支上的工作复制到main分支，除了使用git rebase，使用git cherry-pick也是一种方法。下图是输入git cherry-pick C2 C4的变化:\n变化前 变化后 可以看出，在main分支下输入git cherry-pick C2 C4后，main分支就会获取到C2和C4的提交记录。\n但是，使用这种方式需要知道提交记录的哈希值，如果不清楚提交记录的哈希值的话，可以使用交互式的rebase。\ngit rebase -i -i是\u0026ndash;interactive的简写，意思是交互式。在git rebase后添加了这个选项，git会打开一个UI界面并列出将要被复制到目标分支的备选提交记录，它还会显示每个提交记录的哈希值和提交说明，提交说明有助于你理解这个提交进行了哪些更改。实际使用时，显示的UI窗口一般会在文本编辑器(如vim)中打开一个文件。\n在UI界面中我们可以:\n调整提交记录的顺序 删除不想要的提交(通过切换pick的状态来完成，关闭就意味着你不想要这个提交记录) 合并提交(把多个提交记录合并成一个) git tag 分支很容易被人为移动，并且当有新的提交时，它也会移动。分支很容易被改变，大部分分支还只是临时的，并且还一直在变。而使用tag可以永远指向某个提交记录的标识。比如使用git tag \u0026lt;tag name\u0026gt;即可在当前分支创建一个tag。使用git tag \u0026lt;tag name\u0026gt; \u0026lt;ref\u0026gt;可以在对应引用的记录下创建一个tag。\ngit describe git describe可以用来描述最近的tag。\n其语法为git describe \u0026lt;ref\u0026gt;。ref可以是任何能被git识别成提交记录的引用，如果你没有指定的话，git会使用你目前所在的位置(HEAD)。\n输出结果为: \u0026lt;tag\u0026gt;_\u0026lt;numCommits\u0026gt;_g\u0026lt;hash\u0026gt;。tag表示的是离ref最近的标签，numCommits是表示这个ref与tag相差有多少个提交记录，hash表示的是你所给定的ref所表示的提交记录哈希值的前几位。当ref提交记录上有某个tag时，则只输出tag名称。\n复杂的实践操作 多次rebase 下图给出了初始树的状态和变化后的树的状态，要求只使用git rebase\n变化前 变化后 步骤最少的命令如下:\ngit rebase main bugFix git rebase bugFix side git rebase side another git rebase another main 下图展示了变化输入命令的变化情况：\n两个parent节点 操作符^和~一样，后面都可以跟数字。^后面跟数字是指定合并提交记录的某个parent提交。这在某个位置上有多个parent提交的情况下十分有用。下面的例子展示了使用git checkout main^和git checkout main^2的情况:\n变化前 git checkout main^ git checkout main^2 此外，这种操作符也支持链式操作，比如git checkout main~^2~2。\n","date":"May 15, 2024","img":"https://KasterMist.com/images/logo/git.jpg","lang":"en","langName":"English","largeImg":"","permalink":"/posts/git/git_tutorial_3/","series":[],"smallImg":"","tags":[{"title":"Git","url":"/tags/git/"}],"timestamp":1715752136,"title":"Git 学习 (三)"},{"authors":[],"categories":[{"title":"Tutorial","url":"/categories/tutorial/"}],"content":"本章节将介绍git的一些高级操作，这些操作主要用来如何在提交树上进行移动，方便更灵活的更改分支以及提交节点。\n高级操作 HEAD 我们有必要先学习在你项目的提交树上前后移动的几种方法。\nHEAD是一个对当前所在分支的符号引用，也就是指向正在其基础上进行工作的提交记录。HEAD总是指向当前分支最近的一次提交记录。\nHEAD通常情况下是指向分支名的(比如bugFix)。在提交时，改变了bugFix的状态，这一变化通过HEAD变得可见。\n下面一个例子展现了使用git checkout C1; git checkout main; git commit; git checkout C2的变化:\n分离HEAD就是让其指向了某个具体的提交记录而不是分支名。在命令执行之前的状态为: HEAD -\u0026gt; main -\u0026gt; C1，使用git checkout C1后，状态变为: HEAD -\u0026gt; C1，变化如下图所示:\n变化前 变化后 相对引用 我们可以使用git log来访问提交记录的哈希值。通过指定提交记录哈希值来移动提交记录是可行的，但是由于哈希值在git中非常长，使用起来非常不方便。不过，git对哈希的处理很智能。你只需要提供能够唯一标识提交记录的前几个字符即可。因此我可以仅输入一部分字符来匹配。\n此外，git引入了相对引用，便于移动提交记录。使用相对引用的话，可以在容易记忆的地方开始计算。\n把操作符^加在引用名称的后面，表示让git寻找指定提交记录的parent提交。“main^”相当于“main的parent节点”，“main^^”相当于main的第二个parent节点。\n下面展示的是使用git checkout main^发生的变化:\n变化前 变化后 我们也可以将HEAD作为相对引用参照，下面的动图展示了git checkout C3; git checkout HEAD^; git checkout HEAD^; git checkout HEAD^;发生的变化:\n使用~\u0026lt;num\u0026gt;向上移动多个提交记录，如git checkout bugFix~3就会在bugFix分支所在的记录一次性后退3步。\n相对引用也可以用在强制修改分支位置的情况: 我们可以直接使用-f选项让分支指向另一个提交，比如git branch -f main HEAD~3，这种方式可以将main分支强制指向HEAD的第3级的parent提交。下图展示了变化情况:\n变化前 变化后 (git checkout a b^就是在b分支的上一级创建一个名字叫a的分支)\n撤销变更 有两种方法来撤销变更: git reset和git revert\ngit reset git reset通过把分支记录回退几个提交记录来实现撤销改动。你可以将这想象成“改写历史”.git reset向上移动分支，原来指向的提交记录就跟从来没有提交过一样。\n下面是输入git reset HEAD～1的变化情况:\n变化前 变化后 git把main分支移回到C1，我们的本地代码库根本就不知道有C2这个提交了(在reset后，C2所做的变更还在，但是处于未加入暂存区状态)。\n在本地使用git reset很方便，但是git reset无法对远程分支生效。\ngit revert 使用git revert可以撤销更改并分享给别人，下图展示了输入git revert HEAD后的变化，图中可以看出来我们要撤销的提交记录后面多了一个新提交，这是因为新提交记录C2引入了更改，这些更改刚好是用来撤销C2这个提交的。也就是说C2的状态与C1是相同的。revert之后就可以把更改推送到远程仓库和别人分享了。\n变化前 变化后 ","date":"May 12, 2024","img":"https://KasterMist.com/images/logo/git.jpg","lang":"en","langName":"English","largeImg":"","permalink":"/posts/git/git_tutorial_2/","series":[],"smallImg":"","tags":[{"title":"Git","url":"/tags/git/"}],"timestamp":1715503436,"title":"Git 学习 (二)"},{"authors":[],"categories":[{"title":"Tutorial","url":"/categories/tutorial/"}],"content":"本文章根据\u0026quot;Learn Git Branching\u0026quot;网站进行记录知识点。该网站将git流程可视化，可以很好的学习git相关知识。如果有兴趣，可以访问该网站: https://learngitbranching.js.org/?locale=zh_CN\n基础指令 git commit git commit作为常用的git命令之一，其功能可以视为一个提交记录。每次进行提交时，它并不会盲目地复制整个目录。条件允许的情况下，它会将当前版本与仓库中的上一个版本进行对比，并把所有的差异打包到一起作为一个提交记录。同时，Git还保存了提交的历史记录，我们使用git commit的信息都会有保留。\n我们可以使用git commit -m \u0026quot;message\u0026quot;来对提交记录进行一个描述。\ngit branch git branch可以创建一个新的分支，我们可以在新的分支里面进行更新信息而不会影响原有的分支。当更新新的分支时，我们需要在新的分支下面才能更新。\n创建新的分支: git branch \u0026lt;name\u0026gt; 使用git checkout \u0026lt;name\u0026gt;可以切换到新的分支上。 创建一个分支同时切换到该分支: git checkout -b \u0026lt;name\u0026gt; git merge git merge用于将分支进行合并。我们可以创建一个新的分支，在其上面开发，开发完后再合并回主线。在 Git 中合并两个分支时会产生一个特殊的提交记录，它有两个 parent 节点。翻译成自然语言相当于：“我要把这两个 parent 节点本身及它们所有的祖先都包含进来。”\n如下图所示，现在有两个分支，每个分支都有一个独自的提交。这样的话没有一个分支包含了所有的提交，我们可以使用git merge来进行合并。如果我们当前是在main分支下，我们可以使用git merge bugFix来把bugFix合并到main里面。\n变化前 变化后 现在从新的 main 开始沿着箭头向上看，在到达起点的路上会经过所有的提交记录。这意味着 main 包含了对代码库的所有修改。\n如果再把 main 分支合并到 bugFix 分支，就会让两个分支都包含所有的修改，使用git checkout bugFix和git merge main即可实现，如下图所示:\n变化前 变化后 git rebase git rebase是另一种合并方法，Rebase 实际上就是取出一系列的提交记录，“复制”它们，然后在另外一个地方逐个的放下去。Rebase 的优势就是可以创造更线性的提交历史。\n如下图所示，现在有两个分支，我们现在在 bugFix 分支上。如果我们想让 bugFix 分支里的工作直接移到 main 分支上，移动以后会使得两个分支的功能看起来像是按顺序开发，但实际上它们是并行开发的。我们可以使用git rebase main来进行实现。此时 bugFix 分支上的工作在 main 的最顶端，同时我们也得到了一个更线性的提交序列。\n值得注意的是，提交记录C3仍然存在，C3\u0026rsquo;是我们rebase到 main 分支的C3副本。\n变化前 变化后 剩下的操作就是更新 main 分支，将 main 分支和 bugFix 分支同步。使用git checkout main切换到 main 分支，然后git rabase bugFix即可，由于 bugFix 继承 main ，所以git只是简单的把 main 分支的引用向前移动了一下。变化如下图所示:\n变化前 变化后 此外，如果git rebase后面跟两个分支名称的话，例如git rebase main bugFix，意思就是将bugFix分支移到main分支下面。\n","date":"May 11, 2024","img":"https://KasterMist.com/images/logo/git.jpg","lang":"en","langName":"English","largeImg":"","permalink":"/posts/git/git_tutorial_1/","series":[],"smallImg":"","tags":[{"title":"Git","url":"/tags/git/"}],"timestamp":1715418127,"title":"Git 学习 (一)"},{"authors":[],"categories":[{"title":"Note","url":"/categories/note/"}],"content":"本章将介绍vscode的C/C++插件的具体使用方法。\nvscode的C/C++插件是一个强大的工具，它提供了代码的IntelliSense (智能识别)、running、debugging等功能。不过，每次创建一个新的项目的时候，有时候需要重新配置相关功能的文件，下面将介绍如何配置这些功能的配置文件。\n一般来说，这些配置文件会在C/C++扩展安装好后配置完成。可以在vscode的settings选项中选择C/C++相关的设置进行更改。此外，也可以在当前工作路径下创建.vscode路径，然后在路径里面添加新的配置来覆盖默认的配置信息。\nIntelliSense 在工作路径下，可以使用⇧⌘P(macos) 或者ctrl shift p来打开命令选项，输入C/C++: Edit Configurations (UI)，即可在.vscode路径下创建配置智能识别的配置文件c_cpp_properties.json。有时候找不到该指令，则需要手动创建。\nc_cpp_properties.json提供了必要的设置来配置 IntelliSense，这包括告诉 IntelliSense 哪些目录包含了项目的头文件（通过 includePath 设置）、使用的编译器路径（compilerPath）、预处理器定义（defines）、C/C++ 标准版本（如 cStandard 和 cppStandard）、以及 IntelliSense 模式（intelliSenseMode）等。\nDebug 使用⇧⌘P(macos) 或者ctrl shift p来打开命令选项，输入C/C++ Add Debug Configuration，即可在.vscode路径下创建配置文件launch.json。launch.json 用于配置调试器，debug程序 。\nTask 使用⇧⌘P(macos) 或者ctrl shift p来打开命令选项，输入Tasks: Configure Task，即可在.vscode路径下创建配置文件tasks.json。tasks.json文件用于配置和管理构建任务，如编译代码(添加需要的compiler和compiler flag等)，运行脚本，打包程序等。\n如果想要在debug的时候中想要添加一些环境变量，可以在tasks.json的\u0026quot;\u0026ldquo;args\u0026quot;参数中添加，比如\u0026quot;-I/${workspaceFolder}/include\u0026quot;来添加include路径。\n键盘快捷方式 如果在命令选项中找找不到相关命令，可以在界面左下角的Manage中打开Keyboard Shortcuts(键盘快捷方式)，然后在里面输入对应的命令，之后设置快捷键以方便之后的调用。\nCmake Vscode的Cmake工具拓展能够很方便的使用，可参考链接: https://code.visualstudio.com/docs/cpp/cmake-linux\nCmake 手动选择kit: https://code.visualstudio.com/docs/cpp/cmake-linux\n","date":"May 8, 2024","img":"https://KasterMist.com/images/logo/vscode.jpeg","lang":"en","langName":"English","largeImg":"","permalink":"/posts/vscode/1/","series":[],"smallImg":"","tags":[{"title":"VsCode","url":"/tags/vscode/"}],"timestamp":1715152664,"title":"Vscode 插件: C/C++"},{"authors":[],"categories":[{"title":"Draft","url":"/categories/draft/"}],"content":"本文章主要记录使用博客期间遇到的有关hugo bootstrap skeleton主题的一些问题以及解决方法。\n图片设置 md文件的图片引用默认路径以及md文件的front matter的“image []”信息的默认路径 是在项目root路径的static路径里面。为了方便，一些网站需要的图片可以都放到static路径下面。\n","date":"May 2, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/posts/hugo_theme_usage_notes/hugo_bootstrap_skeleton/","series":[],"smallImg":"","tags":[{"title":"Hugo Bootstrap Skeleton","url":"/tags/hugo-bootstrap-skeleton/"}],"timestamp":1714636844,"title":"Hugo Bootstrap Skeleton Notes"},{"authors":[],"categories":[{"title":"Note","url":"/categories/note/"}],"content":"本章节将介绍vim的常用语法。\n在命令行中可以输入vimtutor来进行vim学习。\nvim学习参考视频链接: https://www.bilibili.com/video/BV1PL411M7bg/?spm_id_from=333.788\u0026vd_source=302ea1a1118a80c10a5b35e58bd9c8bf\n创建或编辑一个文件: vim filename (如果filename不存在，则创建该文件)\n常用vim语句 vim的三种模式 普通模式 (Normal mode) / 命令模式 (Command mode): 默认模式，用于导航、删除、复制等操作。 插入模式 (Insert mode): 用于输入文本。 视图模式 (Visual Mode): 用于文本搜索、筛选。 在普通模式下按下i即可进入插入模式，按下v即可进入视图模式。在插入模式或者视图模式下按下esc即可退出该模式并进入普通模式。\n模式转换的快捷键使用\n普通模式\u0026ndash;\u0026gt;插入模式\n在下方插入一行: o 在上方插入一行: O(大写的o) 在当前光标后插入: a 在当前光标前插入: i 在行尾插入: A 在行首插入: I 特殊模式 replace mode: 输入R即可进入replace模式，此时输入的字符会替换当前字符(即先删除当前字符再输入新的字符) 光标移动 基础部分\n上下左右: 左: h 下: j 上: k 右: l 移动到第一行: gg 移动到做后一行/指定行: G 行数+大写G跳到指定行 进阶部分\n移动到下一个单词的开头: w 移动到下一个单词的结尾: e 移动到上一个单词的开头: b 移动到上一个单词的结尾: ge 注意，上面这四个使用方式有大写的方法，如果为大写，则表示的是字串 (可以理解为两个空格之间的信息)，小写表示的是单词。 移动行首: 0 (数字零) 移动到第一个非空字符: ^ 移动到行尾: $ 移动到匹配的括号处: % 移动到变量定义处: gd 移动到前一个没有匹配的左大括号处: [{ 移动到下一个没有匹配的右大括号处: ]} 修改大小写(如果是大写，则修改为小写。如果是小写，则修改为大写): ~ 保存与退出 保存: :w 退出: :q 保存+退出: :wq 强制退出(不保存): :q! 在vim里面运行命令行: :!后带命令，可以将:后的!理解为允许vim执行外部命令。 在visual mode中选取多行后，可以通过:w filename将选取的行保存到filename当中。 提取、合并文件 在当前位置插入另外文件的内容: :r filename :r也可以读取外部命令的输出，比如可以通过:r!ls来将ls的输出放置到当前光标下面 复制粘贴, 替换修改 复制: yy 粘贴: p 使用dd删除的行使用p也可以进行粘贴 将当前光标下的字符替换为想要的字符: r后加上一个字符 字符c的功能: 更改字符 使用方式: c [number] motion，motion则是之前常用的动作参数。使用后会自动转换到插入模式。 从光标处删除两个单词并自动转换到插入模式进行修改: c2w 从光标处删除到行尾柄并自动转换到插入模式进行修改:c$ 该变文本直到一个单词的末尾并自动转换到插入模式进行修改: ce 注: 使用复制或者删除的时候复制或删除的内容会存到寄存器中，在当前vim环境下面可以进行粘贴，但是无法在vim外进行粘贴。如果想要将内容复制到剪贴板中，可以在visual mode下选中信息后输入\u0026quot;+y来将选中的信息复制到剪贴板中。注意需要通过vim --version | grep clipboard查看vim版本是否支持剪贴板。如果输出包含了+clipboard，则说明vim版本支持剪贴板。\n修改数字:\n直接跳转到第一个数字并加1: Ctrl-a 直接跳转到第一个数字并减1: Ctrl-x 可以在前面加上数字来表示执行多次: 10Ctrl-a执行10次加1 vim把以0开头的数字解释为八进制值，而不是十进制。如果想要修改默认为十进制，则可以在vimrc中添加set nrformats= 定位 输入Ctrl-G即可得知当前光标所在行位置以及文件信息 光标跳转到最后一行: G 光标跳转到第一行: gg 跳转到指定行: number G 在normal模式下跳转到首个匹配的字符: f + char。例如 fX，表示在当前行内向后查找字符X，光标会移动到第一个匹配到的字符位置。使用t + char则跳转到首个匹配的字符的前面一个的位置。 而如果选择向前匹配，则可以使用F + char和T + char。 ;键表示为执行上一步查找的命令(f, F, t, T)。 ,键则与;表示相反的命令。如果;表示前进，那么,表示回退。 这种方式的使用非常便捷，而且易于修改比如一个长句子包含了\u0026quot;hello, goodbye.\u0026quot;，我们可以使用f,跳转到,处，然后使用dt.将包括,以及后面的信息删除，除了最后的. 撤销 撤销: u 撤销一整行的修改: U 重写: Ctrl-R 删除 删除整行: dd 另一种删除方法: c+指令，用法与d+指令相同，区别在于c在d的基础上又转化为了插入模式，相当于删除后开始写入信息，不需要d+指令后输入i进入插入模式。比如cw就是删除当前光标到单词结尾的信息并进入插入模式。 删除当前字符: x 删除当前字符并进入插入模式: s 删除到行尾: D 在visual mode中选取文本内容后可以通过输入d删除选中的文本内容。 删除该单词(只要光标在该单词范围内): daw 可以理解为delete a word的缩写 组合快捷键 删除两个单词: d2w 删除单词，执行两次: 2dw 删除两个单词，执行两次: 2d2w 在视图模式下选中后5行删除: d5j 搜索替换 在当前光标下搜索下一个匹配的信息: / + 匹配的信息 在当前光标下搜索上一个匹配的信息: ? + 匹配的信息 搜索之后跳转到下一个匹配的信息: n 搜索之后跳转到上一个匹配的信息: N 快速搜索当前光标的单词: 向后 * 向前 #，之后也可以使用n和N来改变方向 将range范围内的from替换为to： :[range]s/from/to/[flags] 在要查找的内容后面加上“\\c”（不区分大小写）或“\\C”（区分大小写），比如/+匹配的信息后面加上\\c或\\C或者在:[range]s/from/to/[flags]的from后加上\\c或\\C 还有一种搜索方式就是让光标某个单词中按下*，会自动跳转下一个匹配的单词。如果想要高亮匹配的单词，可以使用:set hls，关闭高亮可以使用:nohls (另一种打开和关闭高亮的方式是:set hlsearch和:set nohlsearch)。搜索一次过后也可以使用N和n。 range列表\nRange Description Example 21 line 21 :21s/old/new/g 1 first line :1s/old/new/g $ last line :$s/old/new/g % all lines, same as 1,$ :%s/old/new/g 21,25 lines 21 to 25 :21,25s/old/new/g 21,$ lines 21 to end :21,$s/old/new/g .,$ current line to end :.,$s/old/new/g .+1,$ line after current line to end :.+1,$s/old/new/g .,.+5 six lines (current to current +5 inclusive) :.,.+5s/old/new/g .,.5 same (.5 is intepreted as .+5) :.,.5s/old/new/g 有些特殊符号需要在前面加上\\才能识别。\n需要注意的是，如果同一行有多个能匹配到的位置，替换的话只会替换第一个匹配的信息。添加flag: g可以实现每一行中所有匹配的替换(比如上面range列表中的最后的/g)。\nflag list\nflag 作用 \u0026amp; 复用上次替换命令的flags g 替换每行的所有匹配值(默认没有g的情况下只会替换每行的第一个匹配值) c 替换前需确认 e 替换失败时不报错 i 大小写不敏感 I 大小写敏感 此外，使用sed可以直接将某个文件里面的某个信息替换为另一个信息: sed -i \u0026quot;[range]s/from/to/[flags]\u0026quot; filename就是将filename文件中的from替换为to。-i表示在文件内更改。否则更改结果只会在终端中打印出来。\n分窗口 生成水平的窗口: :sp 生成垂直窗口: :vsp 移动到另一个窗口操作: Ctrl-W + [hjkl] 滚动窗口 Ctrl+E - 向下滚动窗口一行，不移动光标。 Ctrl+Y - 向上滚动窗口一行，不移动光标。 Ctrl+D - 向下滚动半个屏幕。 Ctrl+U - 向上滚动半个屏幕。 Ctrl+F - 向下滚动一个整屏幕。 Ctrl+B - 向上滚动一个整屏幕。 zz - 将当前行移至窗口中央，光标位置不变。 zt - 将当前行移至窗口顶部，光标位置不变。 zb - 将当前行移至窗口底部，光标位置不变。 生成标签 (便于跳转) 生成的标签可以是小写字母a-z或者大写字母a-z，也可以是数字0-9。小写字母的标记，仅用于当前缓冲区；而大写字母的标记和数字0-9的标记，则可以跨越不同的缓冲区。小写字母的标签可以被delmarks!删除，大写字母和0-9不行。大写字母和0-9只能通过delmarks character来进行删除\n生成一个标签a: ma 跳转到标签a所在位置: `a 跳转到标签a所在的行首: 'a 查找所有的标签: :marks 删除标签a: :delmarks a 删除a-z的标签: :delmarks a-z 删除A-Z的标签: :delmarks A-Z 删除所有标签(不包括大写的标签): :delmarks! 注释代码 可以使用visual block模式来注释多行代码\nvisual block: Ctrl V 在visual block模式下通过[hjkl]选中多行后，使用I来进行插入，例如输入//然后Esc即可实现多行注释。 使用注释插件\nhttps://github.com/tpope/vim-commentary\nmkdir -p ~/.vim/pack/tpope/start cd ~/.vim/pack/tpope/start git clone https://tpope.io/vim/commentary.git vim -u NONE -c \u0026#34;helptags commentary/doc\u0026#34; -c q Use gcc to comment out a line (takes a count), gc to comment out the target of a motion.\n代码补全 在vim中自带了基础的自动补全功能。但该功能的局限之处在于，只能补全之前已经出现过的单词。当写好了单词一部分后，输入Ctrl-N，自动补全功能会出现提供匹配列表、完成补全、匹配失败等三种不同的情况。\n代码跳转 可以下载ctags来跳转到某对象的定义位置。\n在代码所在路径下输入ctags -R .可以创建代码关联的文件tag。\n默认情况下在一个代码文件里面使用关联只能在当前路径下寻找关联，在～/.vimrc里面添加set tags=./tags;,tags可以寻找tag文件路径下所有的位置是否有关联。\nvim打开文件后，在对应的声明的地方按Ctrl-]就可以自动跳转到对象的定义的文件的对应位置。\n直接查找某个对象(比如class_name)的定义的文件以及对应位置: vim -t class_name\n查找历史Ex命令 输入q:即可打开一个命令历史窗口，我们可以在此窗口中查看之前执行过的Ex命令(以冒号开头的命令)的历史记录，并且可以编辑和重新执行这些命令。编辑命令遵循vim语法。编辑完成后，按下Enter键即可执行编辑好的命令。\nvim相关的插件 插件网站: https://vimawesome.com/\nvim-plug插件管理工具\ngithub链接: https://github.com/junegunn/vim-plug?tab=readme-ov-file\n安装教程: https://github.com/junegunn/vim-plug/wiki/tutorial\nvim-plug是一个基于Rust编写的vim插件管理工具，可以轻松下载需要的vim有关的插件。\n安装方式 (Unix):\ncurl -fLo ~/.vim/autoload/plug.vim --create-dirs \\ https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim 之后在打开~/.vimrc文件添加下面的信息：\n\u0026#34; Plugins will be downloaded under the specified directory. call plug#begin(has(\u0026#39;nvim\u0026#39;) ? stdpath(\u0026#39;data\u0026#39;) . \u0026#39;/plugged\u0026#39; : \u0026#39;~/.vim/plugged\u0026#39;) \u0026#34; Declare the list of plugins. Plug \u0026#39;tpope/vim-sensible\u0026#39; Plug \u0026#39;junegunn/seoul256.vim\u0026#39; \u0026#34; List ends here. Plugins become visible to Vim after this call. call plug#end() 之后重启vim就可以使用Plugin插件。在vim打开文件内输入:Pluginstall即可下载~/.vimrc中声明的插件。输入:PluginUpdate可以更新~/.vimrc中新添加的插件。输入:PlugClean可以清楚~/.vimrc中被删除的插件。\nfzf.vim https://github.com/junegunn/fzf.vim\n该插件包含了fzf的很多功能，并且移植到了vim中。可以使用诸如:Ag等功能。详情可以查看上面的源码链接。\nNERDTree NERDTree可以在vim打开文件后在左边栏显示当前路径下的文件。\nvim打开文件后输入:NERDTree即可在左边栏显示当前路径下的文件信息。左边栏可以选择目录中不同文件，按ENTER即可显示选中的文件信息。 光标左右界面跳转: Ctrl-WW 在对应的vim界面命令行模式下输入退出命令，q!或wq，即可退出对应的界面。 在~/.vimrc中添加autocmd VimEnter * NERDTree即可在vim打开文件后自动开启NERDTree插件。 EasyComplete https://zhuanlan.zhihu.com/p/366496399\n超轻量级的vim代码补全工具，如果想要更全面的补全功能，可以尝试coc\nvim-colors-solarized https://vimawesome.com/plugin/vim-colors-solarized-ours\nvim 文本编辑器的精确配色方案\nsurround.vim surround.vim是一个可以快速给某个字符外面添加\u0026quot;surroundings\u0026quot;的工具，如果想要将某个单词用大括号或其他字符括起来，使用这个工具可以快速达成。\n使用语句 https://vimawesome.com/plugin/surround-vim\n按键 功能 变化 cs\u0026quot;' 将单词外的\u0026quot;\u0026ldquo;替换为'' \u0026ldquo;Hello world!\u0026rdquo; \u0026ndash;\u0026gt; \u0026lsquo;Hello world!\u0026rsquo; cs'\u0026lt;q\u0026gt; 将单词外的\u0026rsquo;\u0026lsquo;替换为xml的标签\u0026lt;p\u0026gt; \u0026lsquo;Hello world!\u0026rsquo; \u0026ndash;\u0026gt; \u0026lt;q\u0026gt;Hello world!\u0026lt;/q\u0026gt; cst\u0026quot; 将单词外的xml标签替换为\u0026rdquo;\u0026quot; \u0026lt;q\u0026gt;Hello world!\u0026lt;/q\u0026gt; \u0026ndash;\u0026gt; \u0026ldquo;Hello world!\u0026rdquo; ds\u0026quot; 将单词外的\u0026quot;\u0026ldquo;删去 \u0026ldquo;Hello world!\u0026rdquo; \u0026ndash;\u0026gt; Hello world! 光标Hello里面 ysiw] 将单词外加上[] Hello world \u0026ndash;\u0026gt; [Hello] world! 光标在Hello里面 vaw S] 先切换到视图模式，全选单词，然后使用S]在单词外添加[] Hello world \u0026ndash;\u0026gt; [Hello] world! 光标Hello里面 ysiw[, 或者 光标在Hello里面 vaw S[ 使用括号左边来让单词与括号之间隔一个空格 Hello world \u0026ndash;\u0026gt; [ Hello ] world! ","date":"April 13, 2024","img":"https://KasterMist.com/images/logo/vim.png","lang":"en","langName":"English","largeImg":"","permalink":"/posts/linux/vim/","series":[],"smallImg":"","tags":[{"title":"Linux","url":"/tags/linux/"},{"title":"Linux Command","url":"/tags/linux-command/"}],"timestamp":1712966400,"title":"Vim用法 (一)"},{"authors":[],"categories":[{"title":"Note","url":"/categories/note/"}],"content":"基础命令 查询文件和子目录 ls 查询文件和子目录的最简单的命令是ls。它可以列出当前目录的文件和子目录。常用的指令有:\nls /path/to/directory 列出指定目录中的文件和子目录 ls -a列出隐藏文件和子目录 ls -l以详细格式列出文件和子目录，包含读写权限、创建时间等信息 查询当前工作目录的绝对路径 pwd 查询当前工作目录的绝对路径是pwd\n改变当前工作目录 cd 使用cd可更改当前工作目录，可以在当前目录下使用cd subpath进入子目录，也可以使用cd absolutePath进入绝对路径目录下。\n使用cd ..可移动到上一级目录。\n创建目录 mkdir mkdir 命令用于创建一个新的目录。\nmkdir directory在当前目录下创建一个directory的子目录 mkdir -p /path/to/new/directory如果要递归创建目录的话，需要加上-pflag 删除目录 rmdir rmdir只能用于删除空的目录，如果是非空目录，则需要时用rm\nrmdir -v /path/subpath会删除path里面的subpath子目录，同时显示详细的输出。 rmdir -p /path/subpath会首先删除path里面的subpath子目录，之后会尝试删除path目录。 创建空文件 touch touch filename创建一个名叫filename的空文件。 touch -c filename如果文件已经存在，则不创建该文件。这样可以避免意外覆盖现有文件。 复制文件和目录 cp 语法: cp [option] source destination\ncp old_path/old_file.txt new_path/new_file.txt将old_path的old_file.txt以new_file.txt的名字复制到new_path中。如果不更改名字的话，destination可以只写new_path cp -r old_path/old_sub_path new_path/new_subpath递归复制整个目录 移动或者重命名文件和目录 mv 语法: mv [options] source destination\nmv old_path/old_file.txt new_path/new_file.txt将old_path的old_file.txt以new_file.txt的名字移动到new_path中。 mv -i old_path/old_file.txt new_path/new_file.txt在覆盖目标位置的任何现有文件前提示。这样可以防止意外覆盖数据。 mv old_file.txt new_file.txt可以通过mv在同一路径下重命名old_file.txt为new_file.txt。 移除文件和目录 rm rm在使用时需要格外小心，因为恢复使用rm删除的文件和目录会非常困难。\nrm -r name递归删除目录，包括目录里面的所有内容。\nrm -f name强制删除并抑制所有提示。\nrm -i name在删除每个文件或目录前提示确认，以防意外删除。\n比如使用rm -rf path可以强制删除path路径和path里面的所有内容。\n在目录层次结构中搜索文件 find 语法: find [path] [critical]\n示例\nfind . -name example.txt: 查找当前目录及其子目录中所有名为 example.txt 的文件 find /home -type f: 查找 /home 目录中所有的普通文件 find / -type f -size +1M: 查找文件大小大于 1MB 的文件 find / -name name: 在/路径下查找名字为name的文件 使用条件匹配搜索文本 grep 语法: grep [options] pattern [files]\n示例\ngrep \u0026quot;a\u0026quot; example.txt搜索example.txt中\u0026quot;a\u0026quot;这个单词。 grep -c \u0026quot;a\u0026quot; example.txt搜索example.txt中\u0026quot;a\u0026quot;这个单词出现的次数。 比较文件 diff 语法: diff [options] file1 file2\n用于比较两个文件的差异\ndiff original.txt updated.txt比较 original.txt和updated.txt两个文件的差异，会输出产生差异的不同行。\n字数统计 wc wc -l example.txt: 只打印行计数\nwc -w example.txt: 只打印字数\nwc -c example.txt: 只打印字节数\n历史命令 history 使用history可以查看之前输入过的命令语句。每个命令前面都有一个编号。你可以使用 ! 加上命令编号来执行历史记录中的命令，比如!123来执行第123号的命令。还可以使用 !! 来执行最后一个命令，或者使用 !string 来执行最近包含字符串 string 的命令。\n使用 history -c 命令来清除命令历史记录。\n文件权限命令 更改文件模式或访问权限 chmod 文件的权限包括：只读(r)，写入(w)，执行(x)。\n模式：指定要修改的权限模式。权限模式可以使用数字表示，也可以使用符号表示。\n数字表示：使用三位数字（0-7）表示权限。每一位数字代表一个权限位，分别对应于读（4）、写（2）和执行（1）权限。例如，755 表示所有者具有读、写和执行权限，组和其他用户具有读和执行权限。 符号表示：使用符号来表示权限。符号表示包括以下几个部分： 操作符：可以是 +（添加权限）、-（删除权限）或 =（设置权限）。 权限范围：可以是 u（所有者）、g（组）、o（其他用户）或 a（所有用户）。 权限类型：可以是 r（读取权限）、w（写入权限）或 x（执行权限）。 例如，u+x 表示为所有者添加执行权限，go-w 表示删除组和其他用户的写入权限。\n示例:\nchmod 644 example.txt: 将文件 example.txt 的权限设置为所有者可读写、组和其他用户只读。 chmod -R 777 documents: 将目录 documents 及其子目录中所有文件的权限设置为所有者可读写执行，组和其他用户可读写执行 (-R用于递归地修改目录及其子目录中的文件权限)。 chmod +x script.sh: 为 script.sh 添加执行权限。 管理命令 查看当前进程信息 ps 用于显示当前正在运行的进程信息。它可以显示当前用户的进程、所有用户的进程或者系统的所有进程。\n显示linux进程 top 动态显示系统的进程信息和资源使用情况。它会实时更新显示当前正在运行的进程列表，并且会以交互式的方式展示系统的 CPU 使用情况、内存使用情况等\n交互式进程浏览 htop htop 命令是 top 命令的改进版本，提供了更加友好和直观的界面，并且支持更多的交互操作。它可以显示更多的进程信息，并且可以通过键盘快捷键进行排序、过滤、查找等操作。\n向进程发送终结信号 kill kill PID: 通过输入PID(进程ID)或程序的二进制名称来终结进程。\nkill -9 name: 通过输入进程名称来终结进程，需要添加-9选项。\n示例: 查找一个进程并终结\nps aux | grep example_process: 使用 ps 命令查找名为 example_process 的进程，述命令会显示包含 example_process 关键词的进程信息，并输出其 PID。 kill PID: 在获取到PID后，即可通过kill PID的方式来终结进程。 报告虚拟内存统计数据 vmstat 打印有关内存、交换、I/O 和 CPU 活动的详细报告。其中包括已用/可用内存、交换入/出、磁盘块读/写和 CPU 进程/闲置时间等指标。\nvmstat -n 5: 每隔5秒更新一次信息。 vmstat -a: 显示活动和非活动内存。 vmstat -s: 显示事件计数器和内存统计信息。 vmstat -S: 以 KB 而不是块为单位输出。 报告CPU和I/O统计数据 iostat 监控并显示 CPU 利用率和磁盘 I/O 指标。其中包括 CPU 负载、IOPS、读/写吞吐量等。\niostat -c: 显示CPU使用率信息。 iostat -t: 为每份报告打印时间戳。 iostat -x: 显示服务时间和等待计数等扩展统计信息。 iostat -d: 显示每个磁盘/分区的详细统计信息，而不是合计总数。 iostat -p: 显示特定磁盘设备的统计信息。 显示可用和已用内存量 free free -b: 以字节为单位显示输出。 free -k: 以KB为单位显示输出结果。 free -m: 以MB为单位显示输出，而不是以字节为单位。 free -h: 以GB、MB等人类可读格式打印统计数据，而不是字节。 自动化 cron cron 是一个用于设置定时任务的命令。\ncron 的功能\n定时任务调度：根据预定的时间表执行任务。 后台运行：在后台持续运行，无需用户干预。 灵活调度：支持多种时间格式，如分钟、小时、天、月和星期几。 crontab 是管理 cron 作业的工具，提供了编辑、查看和删除 cron 作业的方法。crontab 文件包含了 cron 作业的配置。\ncrontab 的功能\n编辑定时任务：使用 crontab -e 命令打开并编辑 crontab 文件。 查看定时任务：使用 crontab -l 命令列出当前用户的 crontab 文件内容。 删除定时任务：使用 crontab -r 命令删除当前用户的 crontab 文件。 crontab 文件语法\ncrontab 文件中的每一行代表一个定时任务，格式如下：\n* * * * * command_to_execute - - - - - | | | | | | | | | +----- 一周中的第几天 (0 - 7) (0 和 7 都表示星期天) | | | +------- 一个月中的第几天 (1 - 31) | | +--------- 月份 (1 - 12) | +----------- 小时 (0 - 23) +------------- 分钟 (0 - 59) 保存crontab文件后即可自动开始任务调度。\n有用的Unix插件 z z用于快速导航目录。相当于cd的进阶版本。它根据目录访问频率和最近访问时间来排序，使用户可以快速跳转到常用目录。\n可以输入z \u0026lt;partial_path即可通过关键词跳转到常用的目录。\nfzf 参考资料:https://zhuanlan.zhihu.com/p/41859976\ngithub源码: https://github.com/junegunn/fzf\nfzf是一种非常好用的下拉查找工具，通常需要与其他的命令组合。下面是一些常用的功能:\n单独使用fzf命令会展示当前目录下所有文件列表，可以用键盘上下键或者鼠标点出来选择。 使用vim组合fzf来查找并打开当前目录下的文件: vim $(fzf) 切换当前工作目录: cd $(find * -type d | fzf)，其实现逻辑如下: 使用find命令找出所有的子目录 把子目录列表pipe到fzf上进行选择 再把结果以子命令的形式传给cd 可以将cmd | fzf理解为将列出的结果以fzf下拉查找工具的方式来实现，比如ls | fzf就是会通过下拉查找的方式查看当前路径下(不包括子路径)的文件和文件夹。$(fzf)意味着通过fzf选取的信息将输入到变量当中，比如vim $(fzf)就是通过fzf选取文件后再用vim打开。 切换git分支: git checkout $(git branch -r | fzf) 使用fzf插件补全shell命令 fzf自带一种插件可以通过输入**来自动生成下拉框窗口来补全信息。比如使用cd **然后tab可以使用下拉菜单选择路径。可以在https://github.com/junegunn/fzf搜索“Fuzzy completion for bash and zsh”找到有关的信息。\n如果用的是homebrew，则先通过brew install fzf，然后执行$(brew --prefix)/opt/fzf/install。之后在~/.zshrc的plugins=(...)中添加fzf。之后执行source ~/.zshrc，即可使用。\n我们也可以对FZF_DEFAULT_OPTS进行设置，来自定义fzf的界面，比如FZF_DEFAULT_OPTS=\u0026quot;--height 40% --layout=reverse --preview '(highlight -O ansi {} || cat {}) 2\u0026gt; /dev/null | head -500'\u0026quot;\nag 一个类似于 grep 的代码搜索工具，比grep有更高的性能。ag 在搜索时会自动忽略 .gitignore 中的文件和目录，从而提高搜索效率。它支持正则表达式，高亮显示匹配结果，并且可以直接在你的编辑器中使用。\n在当前目录以及子目录搜索文本: ag \u0026quot;search pattern\u0026quot; 在特定文件类型中搜索: ag \u0026quot;search pattern\u0026quot; --cpp只在HTML文件中搜索 忽略特定文件或目录: ag \u0026quot;search pattern\u0026quot; --ignore dir/*忽略特定目录下的搜索 ag \u0026quot;search pattern\u0026quot; --ignore *.log 忽略所有.log文件 搜索特定目录: ag \u0026quot;search pattern\u0026quot; /path/to/directory 仅显示文件名: ag \u0026quot;search pattern\u0026quot; -l 与grep一样，可以配合其他语句执行，比如cat \u0026quot;filename\u0026quot; | ag \u0026quot;search pattern\u0026quot; 引号可加可不加 ","date":"April 4, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/posts/linux/linux-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","series":[],"smallImg":"","tags":[{"title":"Linux","url":"/tags/linux/"},{"title":"Linux Command","url":"/tags/linux-command/"}],"timestamp":1712188800,"title":"Linux 常用命令"},{"authors":[],"categories":[{"title":"Tutorial","url":"/categories/tutorial/"}],"content":"前面几章主要是对参考书目的内容进行一个概括。本章将根据参考书目的内容对所学到的所有函数进行一个整理和总结，以便复习和参考。英伟达的官方网站包含了所有的CUDA函数，可参考https://developer.download.nvidia.cn/compute/DevZone/docs/html/C/doc/html/index.html\n基本语法 这部分将介绍一下CUDA的基本语法，即参数的创建、传递、释放。\ncudaMalloc cudaMalloc函数的语法结构如下表示：\ncudaError_t cudaMalloc(void** devPtr, size_t size) 与C的malloc相似，cudaMalloc在设备上分配“size”字节大小的线性内存，并在*devPtr中返回一个指向所分配内存的指针。所分配的内存对于任何类型的变量都是适当对齐的。如果分配成功，则返回cudaSuccess，如果分配失败，则会返回cudaErrorMemoryAllocation。\n注意，void** devPtr表示需要传递指针的地址。\ncudaMemcpy cudaMemcpy函数的语法如下所示：\ncudaError_t cudaMemcpy(void* dst, const void* src, size_t, count, enum cudaMemcpyKind kind) 将“count”字节从src指向的内存区域复制到dst指向的内存区。\ndst是目标位置destination，src是源位置source。\nkind用于规定复制的方向，总共有以下几种：\ncudaMemcpyHostToHost cudaMemcpyHostToDevice cudaMemcpyDeviceToHost cudaMemcpyDeviceToDevice 返回的值有：\ncudaSuccess cudaErrorInvalidValue cudaErrorInvalidDevicePointer cudaErrorInvalidMemcpyDirection 注：此函数还可能返回以前异步启动的错误代码。\ncudaFree cudaFree函数的语法如下所示\ncudaError_t cudaFree(void* devPtr) 释放devPtr指向的内存。与cudaMalloc()或者cudaMallocPitch()一一对应，对之前分配的内存进行释放。\n返回的值有：\ncudaSuccess cudaErrorInvalidDevicePointer cudaErrorInitializationError 注：此函数还可能返回以前异步启动的错误代码。\n核函数 核函数前面需要加上__global__，此方法会将函数标记为设备代码“Device Code”。函数格式可以为：\n__global__ kernel(arguments) 进行函数调用的格式可以为：\nkernel\u0026lt;\u0026lt;\u0026lt;blocksPerGrid, threadsPerBlock\u0026gt;\u0026gt;\u0026gt;(arguments) 下面将详细介绍threads，blocks相关概念。也可以参考以下链接：https://zhuanlan.zhihu.com/p/675603584\nCUDA里面用Grid和Block作为线程组织的组织单位，一个Grid可包含了N个Block，一个Block包含N个thread。\n我们一般用得到的参数是gridDim, blockDim, blockIdx和threadIdx。\ngridDim: dim3类型，表示blocks在grid里面数量的维度。 blockDim：dim3类型，表示threads在block里面数量的维度。 blockIdx：dim3类型，表示blocks在grid里面的索引。 threadIdx：dim3类型，表示threads在block里面的索引。 上图给了一个参考的thread和block的结构。在上面的结构中，一个grid有2*3个block块，每个block块各有15*15个thread。x轴是从左往右，y轴是从上到下(示例图片并没有显示z轴，即z轴最大值为1)。\n如何获取某个thread的坐标，可以分为以下几步：\n计算thread在block中的位置 $$ threadInBlock = threadIdx.x + threadIdx.y * blockDim.x + threadIdx.z * blockDim.x * blockDim.y $$\n计算该block在grid中的位置 $$ blockInGrid = blockIdx.x + blockIdx.y * gridDim.x + blockIdx.z * gridDim.x * gridDim.y $$\n计算每个block的线程，计算得出某个thread的位置索引 $$ oneBlockSize = blockDim.x * blockDim.y * blockDim.z $$\n$$ idx = threadInBlock + oneBlockSize * blockInGrid $$\n查询设备 cudaGetDevice cudaError_t cudaGetDevice(int* device) 将当前主机线程调用的设备索引返回给device的地址。\ncudaGetDeviceCount cudaError_t cudaGetDeviceCount(int* count) 返回可执行的设备数量到count的地址。如果没有这样的设备，则返回cudaErrorNoDevice。如果无法加载驱动程序来确定是否存在任何此类设备，则返回cudaErrorInsufficientDriver。\ncudaGetDeviceProperties cudaError_t cudaGetDeviceProperties(struct cudaDeviceProp* prop, int device) 将device的属性的信息传递给prop。cudaDeviceProp结构可以参考下面的链接：\nhttps://developer.download.nvidia.cn/compute/DevZone/docs/html/C/doc/html/group__CUDART__DEVICE_g5aa4f47938af8276f08074d09b7d520c.html#g5aa4f47938af8276f08074d09b7d520c\n内存 共享内存(Shared Memory) 共享内存是在GPU的每个线程块（block）中共享的内存空间，用于线程之间的通信和数据共享。 共享内存的访问速度比全局内存更快，因为它位于芯片上，与处理器更近。 共享内存的使用需要程序员显式地将数据从全局内存复制到共享内存中，并在使用完毕后将数据写回全局内存。 共享内存在内核中的声明是在内核函数的参数列表之外使用 __shared__ 关键字。 全局内存(Global Memory) 全局内存是GPU中所有线程都可以访问的主要内存池，在设备内存中分配。 全局内存的访问速度相对较慢，因为它位于芯片之外，需要通过总线等方式与GPU核心通信。 全局内存通常用于存储大规模的数据，如数组、结构体等。 全局内存可以通过 cudaMalloc 分配内存，并使用 cudaMemcpy 在主机内存和设备内存之间进行数据传输。 常量内存(Constant Memory) 常量内存是GPU上的一种只读内存，用于存储在GPU核心中被所有线程共享的常量数据。\n常量内存通常用于存储对所有线程都是常量的数据，比如常量数组、常量参数等。\n常量内存的优势在于其高速缓存和对齐的特性，可以加速访问频繁的常量数据。\n常量内存的访问速度比全局内存更快，但相对来说容量较小。\n常量内存通常在内核启动之前被初始化，并且其内容在内核执行期间不会改变。可以使用 CUDA 的 __constant__ 修饰符定义常量内存，使用 cudaMemcpyToSymbol 将数据从主机内存拷贝到常量内存中。\n以下是GPT生成的一个常量内存的使用例子：\n#include \u0026lt;cuda_runtime.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; // 定义常量数组大小 #define ARRAY_SIZE 10 // 声明常量内存 __constant__ int constantArray[ARRAY_SIZE]; // CUDA内核函数 __global__ void kernel(int *result) { // 获取线程索引 int idx = threadIdx.x; // 使用常量内存中的数据进行计算 result[idx] = constantArray[idx] * idx; } int main() { int hostArray[ARRAY_SIZE]; int *devResult; // 初始化常量数组 for (int i = 0; i \u0026lt; ARRAY_SIZE; ++i) { hostArray[i] = i + 1; } // 分配设备端内存 cudaMalloc((void**)\u0026amp;devResult, ARRAY_SIZE * sizeof(int)); // 将常量数组拷贝到设备端常量内存中 cudaMemcpyToSymbol(constantArray, hostArray, ARRAY_SIZE * sizeof(int)); // 调用内核函数 kernel\u0026lt;\u0026lt;\u0026lt;1, ARRAY_SIZE\u0026gt;\u0026gt;\u0026gt;(devResult); // 同步CUDA流，确保内核执行完成 cudaDeviceSynchronize(); // 将结果拷贝回主机端 int result[ARRAY_SIZE]; cudaMemcpy(result, devResult, ARRAY_SIZE * sizeof(int), cudaMemcpyDeviceToHost); // 打印结果 printf(\u0026#34;Result:\\n\u0026#34;); for (int i = 0; i \u0026lt; ARRAY_SIZE; ++i) { printf(\u0026#34;%d \u0026#34;, result[i]); } printf(\u0026#34;\\n\u0026#34;); // 释放设备端内存 cudaFree(devResult); return 0; } 操作 原子操作 atomicAdd(addr, val)\n读取地址addr处的值，将y增加到这个值，以及将结果保存回地址addr。\n原子锁 int *mutex; void lock(void){ if(*mutex == 0){ *mutex = 1; //将1保存到锁 } } 当mutex为0时，将数值1保存到锁，后续访问的时候如果mutex为1则无法进行后续任务。然而，如果在线程读取到0并且还没有修改这个值之前，另一个线程将1写入到互斥体，则两个线程都会执行后面的操作。要实现正确的操作，整个运算都要以原子方式来进行。\n下面是lock的实现\nstruct Lock{ int *mutex; Lock(void){ int state = 0; HANDLE_ERROR(cudaMalloc((void**)\u0026amp; mutex, sizeof(int))); HANDLE_ERROR(cudaMemcpy(mutex, \u0026amp;state, sizeof(int), cudaMemcpyHostToDevice)); } ~Lock(void){ cudaFree(mutex); } __device__ void lock(void){ while(atomicCAS(mutex, 0, 1) != 0); } __device__ void unlock(void){ atomicExch(mutex, 0); } } atomicCas()是一个原子操作。调用atomicCAS()将返回位于mutex地址上的值。因此while循环会不断运行，直到atomicCAS发现mutex的值为0。当发现为0时，比较操作成功，线程将把1写入到mutex。\n通过atomicExch(mutex, 0)来重制mutex的值，将其与第二个参数进行交换，并返回它读到的值。\n事件 线程同步 \\_\\_syncthreads __syncthreads()函数的功能是确保同一个线程块的线程执行完该语句之前的所有语句。使用__syncthreads()需要注意的点是要确保所有线程都能够执行该语句，否则其他线程就会永远等待那些执行不了该语句的线程，从而停止下一步的执行。\n注：CUDA并没有提供一个可以同步所有线程（包括不同线程块）的函数。这是因为CUDA的并行模型设计使得在不同线程块之间进行同步更加困难和昂贵。通常情况下，CUDA编程模型假设各个线程块是独立执行的，并且不会直接相互影响。\n","date":"March 11, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E5%8D%81-%E4%B9%A6%E7%B1%8D%E4%B8%AD%E7%9A%84%E5%87%BD%E6%95%B0%E6%95%B4%E5%90%88/","series":[],"smallImg":"","tags":[{"title":"CUDA","url":"/tags/cuda/"}],"timestamp":1710115200,"title":"CUDA学习(十)--书籍中的函数整合"},{"authors":[],"categories":[{"title":"Tutorial","url":"/categories/tutorial/"}],"content":"这部分是基于原子操作章节进行的高级操作介绍，即实现锁定数据结构。\n原子操作只能确保每个线程在完成读取-修改-写入的序列之前，将不会有其他的线程读取或者写入目标内存。然而，原子操作并不能确保这些线程将按照何种顺序执行。例如当有三个线程都执行加法运算时，加法运行的执行顺序可以为(A + B) + C，也可以为A + (B + C)。这对于整数来说是可以接受的，因为中间结果可能被截断，因此(A + B) + C通常并不等于A + (B + c)。因此，浮点数值上的原子数学运算是否有用就值得怀疑🤔。因此在早期的硬件中，浮点数学运算并不是优先支持的功能。\n然而，如果可以容忍在计算结果中存在某种程度的不确定性，那么仍然可以完全在GPU上实现归约运算。我们首先需要通过某种方式来绕开原子浮点数学运算。在这个解决方案中仍将使用原子操作，但不是用于算数本身。\n原子锁 基本思想是，分配一小块内存作为互斥体，互斥体这个资源可以是一个数据结构，一个缓冲区，或者是一个需要以原子方式修改的内存位置。当某个线程从这个互斥体中读到0时，表示没有其他线程使用这块内存。因此，该线程就可以锁定这块内存，并执行想要的修改，而不会收到其他线程的干扰。要锁定这个内存位置，线程将1写入互斥体，这将防止其他竞争的线程锁定这个内存。然后，其他竞争线程必须等待直到互斥体的所有线程将0写入到互斥体后才能尝试修改被锁定的内存。实现锁定过程的代码可以像下面这样:\nvoid lock(void){ if(*mutex == 0){ *mutex = 1; //将1保存到锁 } } 不过这段代码中存在一个严重的问题。如果在线程读取到0并且还没有修改这个值之前，另一个线程将1写入到互斥体，那么会发生什么情况？也就是说，这两个线程都将检查mutex上的值，并判断其是否为0。然后，它们都将1写入到这个位置，并且都执行后面的语句。这会产生严重的后果。\n我们想要完成的操作是：将mutex的值与0相比较，如果mutex等于0，则将1写入到这个位置。要正确实现这个操作，整个运算都需要以原子方式执行，这样就可以确保当线程当线程检查和更新mutex值时，不会有其他的线程进行干扰。在CUDA中，这个操作可以通过函数atomicCAS()来实现，这是一个原子的比较-交换操作(Compare-and-Swap)。函数atomicCAS()的参数包括一个指向目标内存的指针，一个与内存中的值进行比较的值，以及一个当比较相等时保存到目标内存上的值。通过这个操作，我们可以实现一个GPU锁定函数，如下：\n__device__ void lock(void){ while(atomicCAS(mutex, 0, 1) != 0); } 调用atomicCAS()将返回位于mutex地址上的值。**因此while循环会不断运行，直到atomicCAS发现mutex的值为0。当发现为0时，比较操作成功，线程将把1写入到mutex。本质上来看，这个线程将在while循环中不断重复，直到它成功地锁定这个数据结构。**我们将使用这个锁定机制来实现GPU散列表。下面是一种实现方式：\nstruct Lock{ int *mutex; Lock(void){ int state = 0; HANDLE_ERROR(cudaMalloc((void**)\u0026amp; mutex, sizeof(int))); HANDLE_ERROR(cudaMemcpy(mutex, \u0026amp;state, sizeof(int), cudaMemcpyHostToDevice)); } ~Lock(void){ cudaFree(mutex); } __device__ void lock(void){ while(atomicCAS(mutex, 0, 1) != 0); } __device__ void unlock(void){ atomicExch(mutex, 0); } } 代码中通过atomicExch(mutex, 0)来重制mutex的值，将其与第二个参数进行交换，并返回它读到的值。然而，为什么不用跟简单的方法，例如*mutex = 0;呢，因为原子事务和常规的内存操作将采用不同的执行路径。如果同时使用原子操作和标准的全局内存操作，那么将使得unlock()与后面的lock()调用看上去似乎没有被同步。虽然这种混合使用的方式仍可以实现正确的功能，但是为了增加应用程序的可读性，对于所有对互斥体的访问都应使用相同的方式。因此，在使用原子语句来锁定资源后，同样应使用原子语句来解锁资源。\n我们想要在最早的点积运算示例中加上原子锁。Lock结构位于lock.h中，在修改后的点积示例中将包含这个头文件。\n#include \u0026#34;../common/book.h\u0026#34; #include \u0026#34;lock.h\u0026#34; #define imin(a, b) (a \u0026lt; b ? a : b) const int N = 33 * 1024 * 1024; const int threadsPerBlock = 256; const int blocksPerGrid = imin(32, (N + threadsPerBlock - 1) / threadsPerBlock); 核函数也有所不同，在修改后的点积函数中，将Lock类型的变量，以及输入矢量和输出缓冲区传递给核函数。Lock将被用于在最后的累加步骤中控制对输出缓冲区的访问。另一处修改是float *c。之前float *c是一个包含N个浮点数的缓冲区，其中每个线程块都将其计算得到的临时结果保存到相应的元素中。这个缓冲区将被复制到CPU以便计算最终的点积值。然而，现在的参数c将不再指向一个缓冲区，而是指向一个浮点数值，这个值表示a和b中矢量的点积。\n__global__ void dot(Lock lock, float *a, float *b, float *c){ __shared__ float cache[threadsPerBlock]; int tid = threadIds.s + blockIdx.x * blockDim.x; int cacheIndex = threadIdx.x; float temp = 0; while(tid \u0026lt; N){ temp += a[tid] * b[tid]; tid += blockDim.x * gridDim.x; } //设置cache中相应位置上的值 cache[cacheIndex] = temp; //对线程块中的线程进行同步 __syncthreads(); //对于归约运算来说，以下代码要求threadPerBlock必须是2的幂 int i = blockDim.x / 2; while(i != 0){ if(cacheIndex \u0026lt; i){ cache[cacheIndex] += cache[cacheIndex + i]; } __syncthreads(); i /= 2; } 现在到执行到这里时，每个线程块中的256个线程都已经把各自计算的乘积相加起来，并且保存到cache[0]中。现在，每个线程块都需要将其临时结果相加到c执行的内存位置上。为了安全地执行这个操作，我们将使用锁来控制对该内存位置的访问，因此每个线程在更新*c的值之前，要先获取这个锁。在线程块的临时结果与c处的值相加后，将解锁互斥体，这样其他的线程可以继续累加它们的值。在将临时值与最终结果相加后，这个线程块将不再需要任何计算，因此从核函数中返回。\nif(cacheIndex == 0){ lock.lock(); *c += cache[0]; lock.unlock(); } } 下面是main函数：\nint main(){ float *a, *b, c = 0; float *dev_a, *dev_b, *dev_c; //在CPU上分配内存 a = (float*)malloc(N * sizeof(float)); b = (float*)malloc(N * sizeof(float)); //在GPU上分配内存 HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_a, N * sizeof(float))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_b, N * sizeof(float))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_c, N * sizeof(float))); //用数据填充主机内存 for(int i = 0; i \u0026lt; N; i++){ a[i] = i; b[i] = i * 2; } //将数组“a“和”b“复制到GPU HANDLE_ERROR(cudaMemcpy(dev_a, a, N * sizeof(float), cudaMemcpyHostToDevice)); HANDLE_ERROR(cudaMemcpy(dev_b, b, N * sizeof(float), cudaMemcpyHostToDevice)); //将“0”复制到dev_c HANDLE_ERROR(cudaMemcpy(dev_c, \u0026amp;c, N * sizeof(float), cudaMemcpyHostToDevice)); //声明Lock，调用核函数，并将结果复制回CPU。 Lock lock; dot\u0026lt;\u0026lt;\u0026lt;blocksPerGrid, threadsPerBlock\u0026gt;\u0026gt;\u0026gt;(lock, dev_a, dev_b, dev_c); //将数组“c”从GPU复制到CPU HANDLE_ERROR(cudaMemcpy(\u0026amp;c, dev_a, sizeof(float), cudaMemcpyDeviceToHost)); #define sum_squares(x) (x * (x + 1) * (2 * x + 1) / 6) printf(\u0026#34;Does GPU value %.6g = %.6g?\\n\u0026#34;, c, 2 * sum_squares((float)(N - 1))); //释放GPU上的内存 cudaFree(dev_a); cudaFree(dev_b); cudaFree(dev_c); //释放CPU上的内存 free(a); free(b); } 散列表 (Hash Table) 接下来我们将介绍hash table的CPU以及GPU实现。\n我们先介绍一下什么是hash table。hash table是一种保存键-值二元组的数据结构，一个字典就可以被视为一个hash table。我们在使用hash table时，需要将查找某个键对应的值的时间降到最低。我们希望这是一个常量时间，即不管hash table多大，搜索某个键对应的值所需时间应该是不变的。\nhash table根据值相应的键，把值放入到“桶(bucket)”中。这种将键映射到值的方法通常被称为散列函数(Hash Function)。对于理想的hash function，每个键都会被映射到一个不同的桶。然而，当多个键被映射到同一个桶中时，我们需要将桶中的所有值保存到一个链表里，每当同一个桶添加新的值时，就将新的值添加到链表的末尾。\nCPU散列表 我们将分配一个长度为N的数组，并且数组中的每个元素都表示一个键-值二元链表。下面是实现的数据结构：\n#include \u0026#34;../common/book.h\u0026#34; struct Entry{ unsigned int key; void* value; Entry* next; }; struct Table{ size_t count; Entry** entries; Entry* pool; Entry* firstFree; } Entry结构中包含了键和值。在应用程序中，键是无符号整数。与键相关的值可以是任意数据类型，因此我们将value声明为一个void*变量。程序重点是介绍如何创建hash table数据结构，因此在value域中并不保存任何内容，仅保证完整性。结构Entry最后的一个成员是指向下一个Entry节点的指针。在遇到冲突时，在同一个桶中将包含多个Entry节点，因此我们决定将这些对象保存为一个链表。所以每个对象都指向桶中的下一个节点，从而形成一个节点链表。最后一个Entry节点的next指针为NULL。\n本质上，table结构本身是一个“桶”数组，这个桶数组是一个长度为count的数组，其中entries中的每个桶都是指向某Entry的指针。如果每添加一个Entry节点时都分配新的内存，那么将对程序性能产生负面的影响。为了避免这种情况，hash table将在成员pool中维持一个可用Entry节点的数组。firstFree指向下一个可用的Entry节点，因此当需要将一个节点添加到hash table时，只需使用由firstFree指向的Entry然后递增这个指针，就能避免新分配内存，而且只需要free()一次就能释放所有这些节点。\n下面是其他的支持代码：\nvoid initialize_table(Table \u0026amp;table, int entries, int elements) { table.count = entries; table.entries = (Entry**)calloc( entries, sizeof(Entry*) ); table.pool = (Entry*)malloc( elements * sizeof( Entry ) ); table.firstFree = table.pool; } 在hash table初始化的过程中，主要的操作包括为桶数组entries分配内存，我们也为节点池分配了内存，并将指针firstFree初始化为指向节点池数组中的第一个节点。程序末尾释放内存时将释放桶数组和空闲节点池:\nvoid free_table(Table \u0026amp;table){ free(table.entries); free(table.pool); } 在本示例中，我们采取了无符号整数作为键，并且需要将这些键映射到桶数组的索引。也就是说，将节点e保存在table.entries[e.key]中。然而，我们需要确保键的取值范围将小于桶数组的长度。下面是解决方法：\nsize_t hash(unsigned int key, size_t count){ return key % count; } 这里的实现方式是将键对数组长度取模，实际情况会更复杂，这里仅仅作为示例程序来展示。我们将随机生成键，如果我们假设随机数值生成器生成的值大致是平均的，那么这个hash function应该将这些键均匀地映射到hash table的所有桶中。真正的实际情况中我们可能需要创建更为复杂的hash function。\n将键值二元数组添加到hash table中包括三个基本的步骤:\n将键放入hash function中计算出新节点所属的桶。 从节点池中取出一个预先分配的Entry节点，初始化该节点的key和value等变量。 将这个节点插入到计算得到的桶的链表首部。 下面是实现的代码:\nvoid add_to_table(Table \u0026amp;table, unsigned int key, void* value){ // step 1 size_t hashValue = hash(key, table.count); // step 2 Entry* location = table.firstFree++; location-\u0026gt;key = key; location-\u0026gt;value = value; // step 3 location-\u0026gt;next = table.entries[hashValue]; table.entries[hashValue] = location; } 步骤3可能会有点难理解。链表的第一个节点是储存在了table.entries[hashValue]中，我们需要在链表的头节点中插入一个新的节点(如果在链表的末尾插入新的节点的话则需要对链表进行遍历直到末尾，增加了复杂度): 首先将新节点的next指针设置为指向链表的第一个节点，然后再将新节点保存到桶数组中(桶数组保存的是链表的第一个节点)，这样就完成了。\n为了判断这段代码能否工作，我们实现了一个函数对hash table执行完好性检查。检查过程中首先遍历这张表并查看每个节点。将节点的键放入到hash function进行计算，并确认这个节点被保存到正确的桶中。在检查了每个节点后，还要验证hash table中的节点数量确实等于添加到hash table的节点数量。如果这些数值并不相等，那么要么是无意中将一个节点添加到多个桶，要么没有正确的插入节点。\n#define SIZE (100 * 1024 * 1024) #define ELEMENTS (size / sizeof(unsigned int)) void verify_table(const Table \u0026amp;table){ int count = 0; for(size_t i = 0; i \u0026lt; table; i++){ Entry* current = table.entries[i]; while(current != NULL){ count++; if(hash(current-\u0026gt;value, table.count) != i){ printf(\u0026#34;%d hashed to %ld, but was located at %ld\\n\u0026#34;, current-\u0026gt;value, hash(current-\u0026gt;value, table.count), i); current = current-\u0026gt;next; } } if(count != ELEMENTS){ printf(\u0026#34;%d elements found in hash table. Should be %ld\\n\u0026#34;, count, ELEMENTS); } else{ printf(\u0026#34;All %d elements found in hash table.\\n\u0026#34;, count); } } } 由于大部分的功能实现都放到了函数中，因此main()函数就相对比较简单:\n#define HASH_ENTRIES 1024 int main(){ unsigned* buffer = (unsigned int*)big_random_block(SIZE); clock_t start, stop; start = clock(); Table table; initialize_table(table, HASH_ENTRIES, ELEMENTS); for(int i = 0; i \u0026lt; ELEMENTS; i++){ add_to_table(table, buffer[i], (void*)NULL); } stop = clock(); float elapsedTime = (float)(stop - start) / (float)CLOCK_PER_SEC * 1000.0f; printf(\u0026#34;Time to hash: %3.1f ms\\n\u0026#34;, elapsedTime); verify_table(table); free_table(table); free(buffer); } 我们首先分配了一大块内存来保存随机数值。这些随机生成的无符号整数将被作为插入到hash table中的键。在生成了这些数值后，接下来将读取系统时间以便统计程序的性能。我们对hash table进行初始化，然后通过for循环将每个随机键插入到hash table。在添加了所有的键后，再次读取系统时间，通过之前读取的系统时间与这次系统时间就可以计算出在初始化和添加键上花费的时间。最后，我们通过完整性检查函数来验证hash table，并且释放了分配的内存。\n多线程环境下的hash table 多线程环境下的hash table可能会遇到race condition。那么如何在GPU上构建一个hash table呢？在点积示例中，每次只有一个线程可以安全地将它的值与最终结果相加。如果每个桶都有一个原子锁，那么我们可以确保每次只有一个线程对指定的桶进行修改。\nGPU hash table 在有了某种方法来确保对hash table实现安全的多线程访问，我们就可以实现GPU的hash table的应用程序。我们需要使用Lock，还需要把hash function声明为一个__device__函数。\n#include \u0026#34;../common/book.h\u0026#34; #include \u0026#34;lock.h\u0026#34; struct Entry{ unsigned int key; void* value; Entry* next; } struct Table{ size_t count; Entry** entries; Entry* pool; } __device__ __host__ size_t hash(unsigned int value, size_t count){ return value % count; } 当__host__与__device__关键字一起使用时，将告诉NVIDIA编译器同时生成函数在设备和主机上的版本。设备版本的函数将在设备上运行，并且只能从设备代码中调用。主机版本的函数将在主机上运行，并且只能从主机代码中调用。__host__与__device__关键字一起使用可以让这个函数既可以在设备上使用又可以在主机上使用。\ninitialize_table()和free_table()与CPU版本差别不大，只是数组的初始化以及释放的代码修改成的GPU版本:\nvoid initialize_table(Table \u0026amp;table, int entries, int elements){ table.count = entries; HANDLE_ERROR(cudaMalloc((void**)\u0026amp;table.entries, entries * sizeof(Entry*))); HANDLE_ERROR(cudaMemset(table.entries, 0, entries * sizeof(Entry*))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;table.pool, elements * sizeof(Entry))); } void free_table(Table \u0026amp;table){ cudaFree(table.pool); cudaFree(table.entries); } verify_table()的CPU版本和GPU版本相同，仅仅需要在开头增加一个函数将hash table从GPU复制到CPU。下面是将hash table从GPU复制到CPU的代码:\nvoid copy_table_to_host(const Table \u0026amp;table, Table \u0026amp;hostTable){ hostTable.count = table.count; hostTable.entries = (Entry**)calloc(table.count, sizeof(Entry*)); hostTable.pool = (Entry*)malloc(ELEMENTS * sizeof(Entry)); HANDLE_ERROR(cudaMemcpy(hostTable.entries, table.entries, table.count * sizeof(Entry*), cudaMemcpyDeviceToHost)); HANDLE_ERROR(cudaMemcpy(hostTable.pool, table.pool, ELEMENTS * sizeof(Entry), cudaMemcpyDeviceToHost)); 在复制的数据中，有一部分数据是指针。我们不能简单地将这些指针复制到主机，因为这些指针指向的地址是在GPU上，它们在主机上并不是有效的指针。然而，这些指针的相对偏移量仍然是有效的。每个指向Entry节点的GPU指针都指向数组table.pool[]中的某个位置，但为了在主机上使用hash table，我们需要它们指向数组hostTable.pool[]中相同的Entry。\n给定一个GPU的指针X，需要将这个指针相对于table.pool的偏移与hostTable.pool相加，从而获得一个有效的主机指针，新指针应该按照以下公式计算: $$ (X - table.pool) + hostTable.pool $$ 对于每个被复制的Entry指针，都要执行这个更新操作：包括hostTable.entries中的Entry指针，以及hash table的节点池中每个Entry的next指针:\nfor(int i = 0; i \u0026lt; table.count; i++){ if(hostTable.entries[i] != NULL){ hostTable.entries[i] = (Entry*)((size_t)hostTable.entries[i] - (size_t)table.pool + (size_t)hostTable.pool); } } for(int i = 0; i \u0026lt; ELEMENTS; i++){ if(hostTable.pool[i].next != NULL){ hostTable.pool[i].next = (Entry*)((size_t)hostTable.pool[i].next - (size_t)table.pool + (size_t)hostTable.pool); } } } 在介绍完了数据结构、hash function、初始化过程、内存释放过程以及验证代码后，还剩下的重要部分就是CUDA C原子语句的使用。核函数add_to_table()的参数包括一个键数组、一个值数组、hash table本身以及一个lock数组。这些数组将被用于锁定hash table中的每个桶。由于输入的数据是两个数组，并且在线程中需要对这两个数组进行索引，因此还需要将索引线性化:\n__global_ void add_to_table(unsigned int* keys, void** values, Table table, Lock* lock){ int tid = threadIdx.x + blockIdx.x * blockDim.x; int stride = blockDim.x * gridDim.x; 线程会像点积示例那样遍历输入数组。对于数组key[]中的每个键，线程都将通过hash function计算出这个键值二元数组属于哪个桶。在计算出目标桶之后，线程会锁定这个桶，添加它的键值二元组，然后解锁这个桶。\nwhile(tid \u0026lt; ELEMENTS){ unsigned int key = keys[tid]; size_t hashValue = hash(key, table.count); for(int i = 0; i \u0026lt; 32; i++){ if((tid % 32) == i){ Entry* location = \u0026amp;(table.pool[tid]); location-\u0026gt;key = key; location-\u0026gt;value = values[tid]; lock[hashVaue].lock(); location-\u0026gt;next = table.entries[hashValue]; table.entries[hashValue] = location; lock[hashValue].unlock(); } } tid += stride; } } for循环和后面的if语句看上去是不必要的。然而，代码中的线程束是一个包含32线程的集合，并且这些线程以步调一致的方式执行。每次在线程束中只有一个线程可以获取这个锁。如果让线程束中的所有32给线程都同时竞争这个锁，那么将会发生严重的问题。这种情况下，最好的方式就是在软件中执行一部分工作，遍历线程束中的线程，并给每个线程一次机会来获取数据结构的锁，执行它的工作，然后释放锁。\nmain函数的执行流程跟CPU版本的相似:\nint main(){ unsigned int* buffer = (unsigned int*)big_random_block(SIZE); cudaEvent_t start, stop; HANDLE_ERROR(cudaEventCreate(\u0026amp;start)); HANDLE_ERROR(cudaEventCreate(\u0026amp;stop)); HANDLE_ERROR(cudaEventRecord(start, 0)); unsigned int* dev_keys; void** dev_values; HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_keys, SIZE)); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_values, SIZE)); HANDLE_ERROR(cudaMemcpy(dev_keys, buffer, SIZE, cudaMemcpyHostToDevice)); Table table; initialize_table(table, HADH_ENTRIES, ELEMENTS); // 声明一个锁数组，数组中的每个锁对应于桶数组中的每个桶。并将它们复制到GPU上。 Lock lock[HASH_ENTRIES]; Lock* dev_lock; HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_lock, HASH_ENTRIES * sizeof(Lock))); HANDLE_ERROR(cudaMemcpy(dev_lock, lock, HASH_ENTRIES * sizeof(Lock), cudaMemcpyHostToDevice)); // 将键添加到hash table，停止性能计数器，验证hash table的正确性，执行释放工作 add_to_table\u0026lt;\u0026lt;\u0026lt;60, 256\u0026gt;\u0026gt;\u0026gt;(dev_keys, dev_values, tavle, dev_lock); HANDLE_ERROR(cudaEventRecord(stop, 0)); HANDLE_ERROR(cudaEventSynchronize(stop)); float elapsedTime; HANDLE_ERROR(cudaEventElapsedTime(\u0026amp;elapsedTime, start, stop)); printf(\u0026#34;Time to hash: 3%3.1f ms\\n\u0026#34;, elapsedTime); verify_table(table); HANDLE_ERROR(cudaEventDestroy(start)); HANDLE_ERROR(cudaEventDestroy(stop)); free_table(table); cudaFree(Dev_lock); cudaFree(dev_keys); cudaFree(dev_values); free(buffer); } ","date":"March 8, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E4%B9%9D/","series":[],"smallImg":"","tags":[{"title":"CUDA","url":"/tags/cuda/"}],"timestamp":1709856000,"title":"CUDA学习(九)"},{"authors":[],"categories":[{"title":"Tutorial","url":"/categories/tutorial/"}],"content":"本章将介绍如何分配和使用零拷贝内存(Zero-Copy Memory)，如何在同一个应用程序中使用多个GPU，以及如何分配和使用可移动的固定内存(Portable Pinned Memory)。\n零拷贝主机内存 上一章介绍了固定内存（页锁定内存），这种新型的主机内存能够确保不会交换出物理内存。我们通过cudaHostAlloc()来分配这种内存，并且传递参数cudaHostAllocDefault来获得默认的固定内存。本章会介绍在分配固定内存时可以使用其他参数值。除了cudaHostAllocDefault外，还可以传递的标志之一是cudaHostAllocMapped。通过cudaHostAllocMapped分配的主机内存也是固定的，它与通过cudaHostAllocDefault分配的固定内存有着相同的属性，特别是当它不能从物理内存中交换出去或者重新定位时。但这种内存除了可以用于主机与GPU之间的内存复制外，还可以打破主机内存规则之一：可以在CUDA C核函数中直接访问这种类型的主机内存。由于这种内存不需要复制到GPU，因此也被称为零拷贝内存。\n通过零拷贝内存实现点积运算 通常，GPU只能访问GPU内存，而CPU也只能访问主机内存。但在某些环境中，打破这种规则或许能带来更好的效果。下面仍然给出一个矢量点积运算来进行介绍。这个版本不将输入矢量显式复制到GPU，而是使用零拷贝内存从GPU中直接访问数据。我们将编写两个函数，其中一个函数是对标准主机内存的测试，另一个函数将在GPU上执行归约运算，并使用零拷贝内存作为输入缓冲区和输出缓冲区。首先是点积运算的主机内存版本:\nfloat malloc_test(int size){ //首先创建计时事件，然后分配输入缓冲区和输出缓冲区，并用数据填充输入缓冲区。 cudaEvent_t start, stop; float *a, *b, c, *partial_c; float *dev_a, *dev_b, *dev_partial_c; float elapsedTime; HANDLE_ERROR(cudaEventCreate(\u0026amp;start)); HANDLE_ERROR(cudaEventCreate(\u0026amp;stop)); //在CPU上分配内存 a = (float*)malloc(size * sizeof(float)); b = (float*)malloc(size * sizeof(float)); partial_c = (float*)malloc(blocksPerGrid * sizeof(float)); //在GPU上分配内存 HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_a, size * sizeof(float))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_b, size * sizeof(float))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_partial_c, blocksPerGrid * sizeof(float))); //用数据填充主机内存 for(int i = 0; i \u0026lt; size; i++){ a[i] = i; b[i] = i * 2; } //启动计时器，将输入数据复制到GPU，执行点积核函数，并将中间计算结果复制回主机。 HANDLE_ERROR(cudaEventRecord(start, 0)); //将数组“a“和”b”复制到GPU HANDLE_ERROR(cudaMemcpy(dev_a, a, size * sizeof(float), cudaMemcpyHostToDevice)); HANDLE_ERROR(cudaMemcpy(dev_b, b, size * sizeof(float), cudaMemcpyHostToDevice)); dot\u0026lt;\u0026lt;\u0026lt;blocksPerGrid, threadsPerBlock\u0026gt;\u0026gt;\u0026gt;(size, dev_a, dev_b, dev_partial_c); //将数组“c”从GPU复制到CPU HANDLE_ERROR(cudaMemcpy(partial_c, dev_partial_c, blocksPerGrid * sizeof(float), cudaMemcpyDeviceToHost)); //停止计时器 HANDLE_ERROR(cudaEventRecord(stop, 0)); HANDLE_ERROR(cudaEventSynchronize(stop)); HANDLE_ERROR(cudaEventElapsedTime(\u0026amp;elapsedTime, start, stop)); //将中间计算结果相加起来，并释放输入缓冲区和输出缓冲区 //结束CPU上的计算 c = 0; for(int i = 0; i \u0026lt; blocksPerGrid; i++){ c += partial_c[i]; } HANDLE_ERROR(cudaFree(dev_a)); HANDLE_ERROR(cudaFree(dev_b)); HANDLE_ERROR(cudaFree(dev_partial_c)); //释放CPU上的内存 free(a); free(b); free(partial_c); //释放事件 HANDLE_ERROR(cudaEventDestroy(start)); HANDLE_ERROR(cudaEventDestroy(stop)); printf(\u0026#34;Value calculated: %f\\n\u0026#34;, c); return elapsedTime; } 使用零拷贝内存的版本是非常类似的多，只是在内存分配上有所不同：\nfloat cuda_host_alloc_test(int size){ cudaEvent_t start, stop; float *a, *b, c, *partial_c; float *dev_a, *dev_b, *dev_partial_c; float elapsedTime; HANDLE_ERROR(cudaEventCreate(\u0026amp;start)); HANDLE_ERROR(cudaEventCreate(\u0026amp;stop)); //在CPU上分配内存 HANDLE_ERROR(cudaHostAlloc((void**)\u0026amp;a, size * sizeof(float), cudaHostAllocWriteCombined | cudaHostAllocMapped)); HANDLE_ERROR(cudaHostAlloc((void**)\u0026amp;b, size * sizeof(float), cudaHostAllocWriteCombined | cudaHostAllocMapped)); HANDLE_ERROR(cudaHostAlloc((void**)\u0026amp;partial_c, blocksPerGrid * sizeof(float), cudaHostAllocMapped)); //用数据填充主机内存 for(int i = 0; i \u0026lt; size; i++){ a[i] = i; b[i] = i * 2; } 使用cudaHostAlloc()时，通过参数flags来指定内存的其他行为。cudaHostAllocMapped这个标志告诉运行时将从GPU中访问这块内存。这个标志意味着分配零拷贝内存。对于这两个输入缓冲区，我们还制定了标志cudaHostAllocWriteCombined。这个标志运行时应该将内存分配为“合并式写入（Write-Combined）”内存。这个标志并不会改变应用程序的功能，但却可以显著地提升GPU读取内存时的性能。然而，当CPU也要读取这块内存时，“合并式写入”会显得低效，因此在决定是否使用这个标志之前，必须首先考虑应用程序的可能访问模式。\n在使用标志cudaHostAllocMapped来分配主机内存后，就可以从GPU中访问这块内存。然而，GPU的虚拟内存空间与CPU是不同的，因此在**GPU上访问它们与在CPU上访问它们有着不同的地址。调用cudaHostAlloc()将返回这块内存在CPU上的指针，因此需要调用cudaHostGetDevicePointer()来获得这块内存在GPU上的有效指针。**这些指针将被传递给核函数，并在随后由GPU对这块内存执行读取和写入等操作。即使dev_a、dev_b和dev_partial_c都位于主机上，但对于核函数来说，它们看起来就像GPU内存一样，这正是由于调用了cudaHostGetDevicePointer()。由于部分计算结果已经位于主机上，因此就不再需要通过cudaMemcpy()将它们从设备上复制回来。\nHANDLE_ERROR(cudaHostGetDevicePointer(\u0026amp;dev_a, a, 0)); HANDLE_ERROR(cudaHostGetDevicePointer(\u0026amp;dev_b, b, 0)); HANDLE_ERROR(cudaHostGetDevicePointer(\u0026amp;dev_partial_c, partial_c, 0)); //启动计时器以及核函数 HANDLE_ERROR(cudaEventRecord(start, 0)); dot\u0026lt;\u0026lt;\u0026lt;blocksPerGrid, threadsPerBlock\u0026gt;\u0026gt;\u0026gt;(size, dev_a, dev_b, dev_partial_c); //不再需要通过cudaMemcpy()将它们从设备上复制回来 HANDLE_ERROR(cudaThreadSynchronize()); HANDLE_ERROR(cudaEventRecord(stop, 0)); HANDLE_ERROR(cudaEventSynchronize(stop)); HANDLE_ERROR(cudaEventElapsedTime(\u0026amp;elapsedTime, start, stop)); //结束GPU上的操作 c = 0; for(int i = 0; i \u0026lt; blocksPerGrid; i++){ c += partial_c[i]; } //在使用cudaHostAlloc()的点积运算代码中，唯一剩下的事情就是执行释放操作 HANDLE_ERROR(cudaFreeHost(a)); HANDLE_ERROR(cudaFreeHost(b)); HANDLE_ERROR(cudaFreeHost(partial_c)); //释放事件 HANDLE_ERROR(cudaEventDestroy(start)); HANDLE_ERROR(cudaEventDestroy(stop)); printf(\u0026#34;Value calculated: %f\\n\u0026#34;, c); return elapsedTime; } 无论cudaHostAlloc()中使用什么标志，总是按照相同的方式来释放内存，即只需调用cudaFreeHost()。剩下的工作就是观察main()如何将这些代码片段组合在一起。\nint main(){ cudaDeviceProp prop; int which Device; HANDLE_ERROR(cudaGetDevice(\u0026amp;whichDevice)); HANDLE_ERROR(cudaGetDeviceProperties(\u0026amp;prop, whichDevice)); if(prop.canMapHostMemory != 1){ printf(\u0026#34;Device, cannot map memory. \\n\u0026#34;); } //如果设备支持零拷贝内存，那么接下来就是将运行时置入能分配零拷贝内存的状态 //通过调用cudaSetDeviceFlags()来实现这个操作，并且传递标志值cudaDeviceMapHost来表示我们希望设备映射主机内存 HANDLE_ERROR(cudaSetDeviceFlags(cudaDeviceMapHost)); //运行两个测试，分别显示二者的执行时间，并推出应用程序： float elapsedTime = malloc_test(N); printf(\u0026#34;Time using cudaMalloc: %3.1f ms\\n\u0026#34;, elapsedTime); elapsedTime = cuda_host_alloc_test(N); printf(\u0026#34;Time using cudaHostAlloc: %3.1f ms\\n\u0026#34;, elapsedTime); } 下面是给出的核函数\n#define imin(a, b) (a \u0026lt; b ? a : b) const int N = 33 * 1024 * 1024; const int threadsPerBlock = 256; const int blocksPerGrid = imin(32, (N + threadsPerBlock - 1) / threadsPerBlock); __global__ void dot(int size, float *a, float *b, float *c){ __shared__ float cache[threadsPerBlock]; int tid = threadIdx.x + blockIdx.x * blockDim.x; int cacheIndex = threadIdx.x; float temp = 0; while(tid \u0026lt; size){ temp += a[tid] * b[tid]; tid += blockDim.x * gridDim.x; } //设置cache中的值 cache[cacheIndex] = temp; //同步这个线程块中的线程 __syncthreads(); //对于归约运算， threadsPerBlock必须为2的幂 int i = blockDim.x / 2; while(i != 0){ if(cacheIndex \u0026lt; i){ cache[cacheIndex] += cache[cacheIndex + i]; } __syncthreads(); i /= 2; } if(cacheIndex == 0){ c[blockIdx.x] = cache[0]; } } 使用多个GPU 我们将把点积应用程序修改为使用多个GPU。为了降低编码难度，我们将在上一个结构中把计算点积所需的全部数据都相加起来。\nstruct DataStruct{ int deviceID; int size; float *a; float *b; float returnValue; } 这个结构包含了在计算点积时使用的设备标识，以及输入缓冲区的大小和指向两个输入缓冲区的指针a和b。最后，它还包含了一个成员用于保存a和b的点积运算结果。\n要使用N个GPU，我们首先需要准确地知道N值是多少。因此，在应用程序的开头调用cudaDeviceCount()，从而判断在系统中安装了多少个支持CUDA的处理器。\nint main(){ int deviceCount; HANDLE_ERROR(cudaGetDeviceCount(\u0026amp;deviceCount)); if(deviceCount \u0026lt; 2){ printf(\u0026#34;We need at least two compute 1.0 or greater devices, but only found %d\\n\u0026#34;, deviceCount); } //为输入缓冲区分配标准的主机内存，并按照之前的方式填充 float *a = (float*)malloc(sizeof(float) * N); HANDLE_NULL(a); float *b = (float*)malloc(sizeof(float) * N); HANDLE_NULL(b); //用数据填充主机内存 for(int i = 0; i \u0026lt; N; i++){ a[i] = i; b[i] = i * 2; } 通过CUDA运行API来使用多个GPU时，要意识到每个GPU都需要由一个不同的CPU线程来控制。由于之前只是用了单个GPU，因此不需要担心这个问题。我们将多线程代码的大部分复杂性都移入到辅助代码文件book.h中。在精简了代码后，我们需要做的就是填充一个结构来执行计算。虽然在系统中可以有任意数量的GPU，但为了简单，在这里只使用两个：\nDataStruct data[2]; data[0].deviceID = 0; data[0].size = N / 2; data[0].a = a; data[0].b = b; data[1].deviceID = 1; data[1].size = N / 2; data[1].a = a + N / 2; data[1].b = b + N / 2; 我们将其中一个DataStruct变量传递给辅助函数start_thread()。此外，还将一个函数指针传给了start_thread()，新创建的线程将调用这个函数，这个示例中的线程函数为routine()。函数start_thread()将创建一个新的线程，这个线程将调用routine()，并将DataStruct变量作为参数传递进去。在应用程序的默认线程中也将调用routine()(因此只多创建了一个线程)。\nCUTThread thread = start_thread(routine, \u0026amp;(data[0])); routine(\u0026amp;(data[1])); //通过调用end_thread()，主应用程序线程将等待另一个线程执行完成。 end_thread(thread); //由于这两个线程都在main()的这个位置上执行完成，因此可以安全地释放内存并显示结果。 free(a); free(b); //我们要将每个线程的计算结果相加起来。 printf(\u0026#34;Value calculated: %f\\n\u0026#34;, data[0].returnValue + data[1].returnValue); } 在声明routine()时指定该函数带有一个void*参数，并返回void*，这样在start_thread()部分代码保持不变的情况下可以任意实现线程函数。\nvoid* routine(void *pvoidData){ DataStruct *data = (DataStruct*)pvoidData; HANDLE_ERROR(cudaSetDevice(data-\u0026gt;deviceID)); 除了调用cudaSetDevice()来指定希望使用的CUDA设备外，routine()的实现非常类似于之前提到的malloc_test()。我们为输入数据和临时计算结果分别分配了内存，随后调用cudaMemcpy()将每个输入数组复制到GPU。\nint size = data-\u0026gt;size; float *a, *b, c, *partial_c; float *dev_a, *dev_b, *dev_partial_c; //在CPU上分配内存 a = data-\u0026gt;a; b = data-\u0026gt;b; partial_c = (float*)malloc(blocksPerGrid * sizeof(float)); //在GPU上分配内存 HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_a, size * sizeof(float))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_b, size * sizeof(float))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_partial_c, blocksPerGrid * sizeof(float))); //将数组“a”和“b”复制到GPU上 HANDLE_ERROR(cudaMemcpy(dev_a, a, size * sizeof(float), cudaMemcpyHostToDevice)); HANDLE_ERROR(cudaMemcpy(dev_b, b, size * sizeof(float), cudaMemcpyHostToDevice)); //启动点积核函数，复制回计算结果，并且结束CPU上的操作 dot\u0026lt;\u0026lt;\u0026lt;blocksPerGrid, threadsPerBlock\u0026gt;\u0026gt;\u0026gt;(size, dev_a, dev_b, dev_partial_c); //将数组“c”从GPU复制回CPU HANDLE_ERROR(cudaMemcpy(partial_c, dev_partial_c, blocksPerGrid * sizeof(float), cudaMemcpyDeviceToHost)); //结束CPU上的操作 c = 0; for(int i = 0; i \u0026lt; blocksPerGrid; i++){ c += partial_c[i]; } HANDLE_ERROR(cudaFree(dev_a)); HANDLE_ERROR(cudaFree(dev_b)); HANDLE_ERROR(cudaFree(dev_partial_c)); //释放CPU侧的内存 free(partial_c); data-\u0026gt;returnValue = c; } 可移动的固定内存 我们可以将固定内存分配为可移动的，这意味着可以在主机线程之间移动这块内存，并且每个线程都将其视为固定内存。要达到这个目的，需要使用cudaHostAlloc()来分配内存，并且在调用时使用一个新的标志：cudaHostAllocPortable。这个标志可以与其他标志一起使用，例如cudaHostAllocWriteCombined和cudaHostAllocMapped。这意味着在分配主机内存时，可将其作为可移动、零拷贝以及合并式写入等的任意组合。\n为了说明可移动固定内存的作用，我们将进一步修改使用多GPU的点积运算应用程序。\nint main(){ int deviceCount; HANDLE_ERROR(cudaGetDeviceCount(\u0026amp;deviceCount)); if(deviceCount \u0026lt; 2){ printf(\u0026#34;We need at least two compute 1.0 or greater devices, but only found %d\\n\u0026#34;, deviceCount); } cudaDeviceProp prop; for(int i = 0; i \u0026lt; 2; i++){ HANDLE_ERROR(cudaGetDeviceProperties(\u0026amp;prop, i)); if(prop.canMapHostMemory != 1){ printf(\u0026#34;Devide %d cannot map memory.\\n\u0026#34;, i); } } float *a, *b; HANDLE_ERROR(cudaSetDevice(0)); HANDLE_ERROR(cudaSetDeviceFlags(cudaDeviceMapHost)); HANDLE_ERROR(cudaHostAlloc((void**)\u0026amp;a, N * sizeof(float), cudaHostAllocWriteCombined | cudaHostAllocPortable | cudaHostAllocMapped)); HANDLE_ERROR(cudaHostAlloc((void**)\u0026amp;b, N * sizeof(float), cudaHostAllocWriteCombined | cudaHostAllocPortable | cudaHostAllocMapped)); 在使用cudaHostAlloc()分配页锁定内存时，首先要通过调用cudaSetDevice()来初始化设备。我们将新介绍的标志cudaHostAllocPortable传递给这两个内存分配操作。由于这些内存是在调用了cudaSetDevice(0)之后才分配的，因此，如果没有将这些内存指定为可移动的内存，那么只有第0个CUDA设备会把这些内存视为固定内存。\n继续之前的应用程序，为输入矢量生成数据，并采用之前的示例的方式来准备DataStruct结构。\n//用数据填充主机内存 for(int i = 0; i \u0026lt; N; i++){ a[i] = i; b[i] = i * 2; } //为使用多线程做好准备 DataStruct data[2]; data[0].deviceID = 0; data[0].offset = 0; data[0].size = N / 2; data[0].a = a; data[0].b = b; data[1].deviceID = 1; data[1].offset = N / 2; data[1].size = N / 2; data[1].a = a; data[1].b = b; //创建第二个线程，并调用routine()开始在每个设备上执行计算 CUTThread thread = start_thread(routine, \u0026amp;(data[1])); routine(\u0026amp;(data[0])); end_thread(thread); //由于主机内存时由CUDA运行时分配的，因此需要用cudaFreeHost()而不是free()来释放它 HANDLE_ERROR(cudaFreeHost(a)); HANDLE_ERROR(cudaFreeHost(b)); printf(\u0026#34;Value calculated: %f\\n\u0026#34;, data[0].returnValue + data[1].returnValue); } 为了在多GPU应用程序中支持可移动的固定内存和零拷贝内存，我们需要对routine()的代码进行两处修改。\nvoid* routine(void *pvoidData){ DataStruct *data = (DataStruct*)pvoidData; if(data-\u0026gt;deviceID != 0){ HANDLE_ERROR(cudaSetDevice(data-\u0026gt;deviceID)); HANDLE_ERROR(cudaSetDeviceFlags(cudaDeviceMapHost)); } 在多GPU版本的代码中，我们需要在routine()中调用cudaSetDevice()，从而确保每个线程控制一个不同的GPU。另一方面，在这个示例中，我们已经在主线程中调用了一次cudaSetDevice()。这么做的原因时为了在main()中分配固定内存。因此，我们只希望在还没有调用cudaSetDevice()的设备上调用cudaSetDevice()和cudaSetDeviceFlags()。也就是，如果devideID不是0，那么将调用这两个函数。虽然在第0个设备上再次调用这些函数会使代码更简洁，但是这种做法是错误的。一旦在某个线程上设置了这些设备，那么将不能再次调用cudaSetDevice()，即便传递的是相同的设备标识符。\n除了使用可移动的固定内存外，我们还使用了零拷贝内存，一边从GPU中直接访问这些内存。因此，我们使用cudaHostGetDevicePointer()来获得主机内存的有效设备指针，这与前面零拷贝示例中采用的方法一样。然而，你可能会注意到使用了标准的GPU内存来保存临时计算结果。这块内存同样是通过cudaMalloc()来分配的。\nint size = data-\u0026gt;size; float *a, *b, c, *partial_c; float *dev_a, *dev_b, *dev_partial_c; //在CPU上分配内存 a = data-\u0026gt;a; b = data-\u0026gt;b; partial_c = (float*)malloc(blocksPerGrid * sizeof(float)); HANDLE_ERROR(cudaHostGetDevicePointer(\u0026amp;dev_a, a, 0)); HANDLE_ERROR(cudaHostGetDevicePointer(\u0026amp;dev_b, b, 0)); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_partial_c, blocksPerGrid * sizeof(float))); //计算GPU读取数据的偏移量“a”和“b” dev_a += data-\u0026gt;offset; dev_b += data-\u0026gt;offset; dot\u0026lt;\u0026lt;\u0026lt;blocksPerGrid, threadsPerBlock\u0026gt;\u0026gt;\u0026gt;(size, dev_a, dev_b, dev_partial_c); //将数组“c“从GPU复制回CPU HANDLE_ERROR(cudaMemcpy(partial_c, dev_partial_c, blocksPerGrid * sizeof(float), cudaMemcpyDeviceToHost)); //结束在CPU上的操作 c = 0; for(int i = 0; i \u0026lt; blocksPerGrid; i++){ c += partial_c[i]; } HANDLE_ERROR(cudaFree(dev_partial_c)); //释放CPU上的内存 free(partial_c); data-\u0026gt;returnValue = c; } ","date":"February 29, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E5%85%AB/","series":[],"smallImg":"","tags":[{"title":"CUDA","url":"/tags/cuda/"}],"timestamp":1709164800,"title":"CUDA学习(八)"},{"authors":[],"categories":[{"title":"Tutorial","url":"/categories/tutorial/"}],"content":"在并行环境中，任务可以是任意操作。例如，应用程序可以执行两个任务：其中一个线程重绘程序的GUI，而另一个线程通过网络下载更新包。这些任务并行执行，彼此之间没有任何共同的地方。虽然GPU上的任务并行性并不像CPU上的任务并行性那样灵活，但仍然可以进一步提高程序在GPU上的运行速度。本章将介绍CUDA流，以及如何通过流在GPU上同时执行多个任务。\n页锁定(Page-Locked)主机内存 CUDA提供了自己独有的机制来分配主机内存：cudaHostAlloc()。malloc()分配的内存与cudaHostAlloc()分配的内存之间存在一个重要差异。C库函数malloc()将分配标准的、可分页的(Pagable)主机内存，而cudaHostAlloc()将分配页锁定的主机内存。页锁定内存也称为固定内存(Pinned Memory)或者不可分页内存，它有一个重要属性：操作系统将不会对这块内存分页并交还到磁盘上，从而确保了该内存始终驻留在物理内存中。因此，操作系统能够安全地使某个应用程序访问该内存的物理地址，因为这块内存将不会被破坏或者重新定位。\n由于GPU知道内存的物理地址，因此可以通过“直接内存访问（Direct Memory Access，DMA）”技术来在GPU和主机之间复制数据。由于DMA在执行复制时无需CPU的介入，这也就意味着，CPU很可能在DMA的执行过程中将目标内存交换到磁盘上，或者通过更新操作系统的分页来重新定位目标内存的物理地址。CPU可能会移动可分页的数据，这就可能对DMA操作造成延迟。因此，在DMA复制过程中使用固定内存时非常重要的。\n事实上，当使用可分页内存进行复制时，CUDA驱动程序仍然会通过DAM把数据传输给GPU。因此，复制操作将执行两遍，第一遍从可分页内存复制到一块“临时的”页锁定内存，然后再从这个页锁定内存复制到GPU上。因此，**每当从可分页内存中执行复制操作时，复制速度将受限于PCIE（高速串行计算机扩展总线标准）传输速度和系统前端总线速度相对较低的一方。当在GPU和主机间复制数据时，这种差异会使页锁定主机内存的性能比标准可分页内存的性能要高达约2倍。**计时PCIE的速度与前端总线的速度相等，由于可分页内存需要更多一次由CPU参与的复制操作，因此会带来额外的开销。\n然而，使用cudaHostAlloc()分配固定内存时，将失去虚拟内存的所有功能。特别是在应用程序中使用每个页锁定内存时都需要分配物理内存，因为这些内存不能交换到磁盘上。这意味着与使用标准的malloc()调用相比，系统将更快地耗尽内存。因此，应用程序在物理内存较少的机器上会运行失败，而且意味着应用程序将影响在系统上运行的其他应用程序的性能。建议仅对cudaMemcpy()调用中的源内存或者目标内存才使用页锁定内存，并且在不再需要使用它们时立即释放，而不是等到应用程序关闭时才释放。\n下面给的例子来说明如何分配固定内存，以及它相对于标准可分页内存的性能优势。这个例子主要是测试cudaMemcpy()在可分配内存和页锁定内存上的性能。我们要做的就是分配一个GPU缓冲区以及一个大小相等的主机缓冲区，然后两个缓冲区之间执行一些复制操作（从主机到设备、从设备到主机）。为了获得精确的时间统计，我们为复制操作的起始时刻和结束时刻分别设置了CUDA事件。\nfloat cuda_malloc_test(int size, bool up){ cudaEvent_t start, stop; int *a, *dev_a; float elapsedTime; HANDLE_ERROR(cudaEventCreate(\u0026amp;start)); HANDLE_ERROR(cudaEventCreate(\u0026amp;stop)); a = (int*)malloc(size * sizeof(*a)); HANDLE_NULL(a); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_a, size * sizeof(\u0026amp;dev_a))); HANDLE_ERROR(cuddaEventRecord(start, 0)); //为size个整数分别分配主机缓冲区和GPU缓冲区，然后执行100次复制操作，并由参数up来指定复制方向，在完成复制操作后停止计时器 for(int i = 0; i \u0026lt; 100; i++){ if(up){ HANDLE_ERROR(cudaMemcpy(dev_a, a, size * sizeof(*dev_a), cudaMemcpyHostToDevice)); } else{ HANDLE_ERROR(cudaMemcpy(a, dev_a, size * sizeof(*dev_a), cudaMemcpyDeviceToHost)); } } HANDLE_ERROR(cudaEventRecord(stop, 0)); HANDLE_ERROR(cudaEventSynchronize(stop)); HANDLE_ERROR(cudaEventElapsedTime(\u0026amp;elapsedTime, start, stop)); free(a); HANDLE_ERROR(cudaFree(dev_a)); HANDLE_ERROR(cudaEventDestroy(start)); HANDLE_ERROR(cudaEventDestroy(stop)); return elapsedTime; } 函数cuda_malloc_test()通过标准的C函数malloc()来分配可分页主机内存，在分配固定内存时则使用了cudaHostAlloc()。\nfloat cuda_host_alloc_test(int size, bool up){ cudaEvent_t start, stop; int *a, *dev_a; float elapsedTime; HANDLE_ERROR(cudaEventCreate(\u0026amp;start)); HANDLE_ERROR(cudaEventCreate(\u0026amp;stop)); HANDLE_ERROR(cudaHostAlloc((void**)\u0026amp;a, size * sizeof(*a), cudaHostAllocDefault)); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_a, size * sizeof(*dev_a))); HANDLE_ERROR(cudaEventRecord(start, 0)); for(int i = 0; i \u0026lt; 100; i++){ if(up){ HANDLE_ERROR(cudaMemcpy(dev_a, a, size * sizeof(*dev_a), cudaMemcpyHostToDevice)); } else{ HANDLE_ERROR(cudaMemcpy(a, dev_a, size * sizeof(*dev_a), cudaMemcpyDeviceToHost)); } } HANDLE_ERROR(cudaEventRecord(stop, 0)); HANDLE_ERROR(cudaEventSynchronize(stop)); HANDLE_ERROR(cudaEventElapsedTime(\u0026amp;elapsedTime, start, stop)); HANDLE_ERROR(cudaFreeHost(a)); HANDLE_ERROR(cudaFree(dev_a)); HANDLE_ERROR(cudaEventDestroy(start)); HANDLE_ERROR(cudaEventDestroy(stop)); return elapsedTime; } cudaHostAlloc()分配的内存与malloc()分配的内存在使用方式是相同的，与malloc()不同之处在于最后一个参数cudaHostAllocDefault。最后一个参数的取值范围是一组标志，我们可以通过这些标志来修改cudaHostAlloc()的行为，并分配不同形式的固定主机内存。就目前而言，只需使用默认值。最后需要使用cudaFreeHost()来释放内存。\nmain()函数的代码如下：\n#include \u0026#34;../common/book.h\u0026#34; #define SIZE (10 * 1024 * 1024) int main(){ float elapsedTime; float MB = (float)100 * SIZE * sizeof(int) / 1024 / 1024; elapsedTime = cuda_malloc_test(SIZE, true); printf(\u0026#34;Time using cudaMalloc: %3.1fms\\n\u0026#34;, elapsedTime); printf(\u0026#34;\\tMB/s during copy up: %3.1f\\n\u0026#34;, MB / (elapsedTime / 1000)); //要执行相反方向的性能，可以执行相同的调用，只需要将第二个参数指定为false elapsedTime = cuda_malloc_test(SIZI, false); printf(\u0026#34;Time using cudaMalloc: %3.1f ms \\n\u0026#34;, elapsedTime); printf(\u0026#34;\\tMB/s during copy down: %3.1f\\n\u0026#34;, MB / (elapsedTime / 1000)); //测试cudaHostAlloc()的性能 elapsedTime = cuda_host_malloc_test(SIZE, true); printf(\u0026#34;Time using cudaHostMalloc: %3.1fms\\n\u0026#34;, elapsedTime); printf(\u0026#34;\\tMB/s during copy up: %3.1f\\n\u0026#34;, MB / (elapsedTime / 1000)); elapsedTime = cuda_host_malloc_test(SIZI, false); printf(\u0026#34;Time using cudaHostMalloc: %3.1f ms \\n\u0026#34;, elapsedTime); printf(\u0026#34;\\tMB/s during copy down: %3.1f\\n\u0026#34;, MB / (elapsedTime / 1000)); } CUDA流 之前介绍过cudaEventRecord()，并没有详细解释这个函数的第二个参数，这个第二个参数是用于指定插入事件的流(Stream)。CUDA流表示一个GPU操作队列，并且该队列中的操作将以指定的顺序执行。我们可以在流中添加一些操作，例如核函数启动、内存复制，以及时间的启动和结束等。你可以将每个流视为GPU上的一个任务，并且这些任务可以并行执行。我们将首先介绍如何使用流，然后介绍如何使用流来加速应用程序。\n使用单个CUDA流 仅当使用多个流时才能显现出流的真正威力。不过我们先用一个流来说明用法。下面的示例中，我们将计算a中三个值和b中三个值的平均值。\n#include \u0026#34;../common/book.h\u0026#34; #define N (1024 * 1024) #define FULL_DATA_SIZE (N * 20) __global__ void kernel(int *a, int *b, int *c){ int idx = threadIdx.x + blockIdx.x * blockDim.x; if(idx \u0026lt; N){ int idx1 = (idx + 1) % 256; int idx2 = (idx + 2) % 256; float as = (a[idx] + a[idx1] + a[idx2]) / 3.0f; float bs = (b[idx] + b[idx1] + b[idx2]) / 3.0f; c[idx] = (as + bs) / 2; } } int main(){ //选择一个一个支持设备重叠功能的设备。支持设备重叠功能的GPU能够在执行一个CUDA C核函数的同时， //还能在设备与主机之间执行复制操作。\tcudaDeviceProp prop; int whichDevice; HANDLE_ERROR(cudaGetDevice(\u0026amp;whichDevice)); HANDLE_ERROR(cudaGetDeviceProperties(\u0026amp;prop, whichDevice)); if(!prop.deviceOverlap){ printf(\u0026#34;Device will not handle overlaps, so no speed up from streams\\n\u0026#34;); } cudaEvent_t start, stop; float elapsedTime; //启动计时器 HANDLE_ERROR(cudaEventCreate(\u0026amp;start)); HANDLE_ERROR(cudaEventCreate(\u0026amp;stop)); HANDLE_ERROR(cudaEventRecord(start, 0)); //初始化流 cudaStream_t stream; HANDLE_ERROR(cudaStreamCreate(\u0026amp;stream)); //数据分配操作 int *host_a, *host_b, *host_c; int *dev_a, *dev_b, *dev_c; //在GPU上分配内存 HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_a, N * sizeof(int))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_b, N * sizeof(int))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_c, N * sizeof(int))); //分配由流使用的页锁定内存 HANDLE_ERROR(cudaHostAlloc((void**)\u0026amp;host_a, FULL_DATA_SIZE * sizeof(int), cudaHostAllocDefault)); HANDLE_ERROR(cudaHostAlloc((void**)\u0026amp;host_b, FULL_DATA_SIZE * sizeof(int), cudaHostAllocDefault)); HANDLE_ERROR(cudaHostAlloc((void**)\u0026amp;host_c, FULL_DATA_SIZE * sizeof(int), cudaHostAllocDefault)); for(int i = 0; i \u0026lt; FULL_DATA_SIZE; i++){ host_a[i] = rand(); host_b[i] = rand(); } 程序将使用主机上的固定内存。我们还会使用一种新的cudaMemcpy()函数，并且在这个新函数中需要页锁定主机内存。在分配完输入内存后，调用C的库函数rand()并用随机证书填充主机内存。\n执行计算的方式是将两个输入缓冲区复制到GPU，启动核函数，然后将输出缓冲区复制回主机。不过，本示例做出了小的调整。首先，我们不将输入缓冲区整体都复制到GPU，而是将输入缓冲区划分为更小的块，并在每个块上执行一个包含三个步骤的过程。我们将一部分输入缓冲区复制到GPU，在这部分缓冲区上运行核函数，然后将输出缓冲区的这部分结果复制回主机。下面给出了一个需要这种方法的情形：GPU的内存远少于主机内存，由于整个缓冲区无法一次性填充到GPU，因此需要分块进行计算。执行“分块”计算的代码如下所示：\n//在整体数据上循环，每个数据块的大小为N for(int i = 0; i \u0026lt; FULL_DATA_SIZE; i += N){ //将锁定内存以异步的方式复制到设备上 HANDLE_ERROR(cudaMemcpyAsync(dev_a, host_a + i, N * sizeof(int), cudaMemcpyHostToDevice, stream)); HANDLE_ERROR(cudaMemcpyAsync(dev_b, host_b + i, N * sizeof(int), cudaMemcpyHostToDevice, stream)); kernel\u0026lt;\u0026lt;\u0026lt;N / 256, 256, 0, stream\u0026gt;\u0026gt;\u0026gt;(dev_a, dev_b, dev_c); //将数据从设备复制到锁定内存 HANDLE_ERROR(cudaMemcpyAsync(host_c + i, dev_c, N * sizeof(int), cudaMemcpyDeviceToHost. stream)); } 上述代码使用cudaMemcpyAsync()在GPU与主机之间复制数据。cudaMemcpy()的行为类似于C库函数memcpy()，尤其是这个函数将以同步的方式进行。这意味着当函数返回时，复制操作就已经完成，并且在输出缓冲区中包含了复制进去的内容。异步函数的行为与同步函数相反，在调用cudaMemcpyAsync()时，只是放置一个请求，表示在流中执行一次内存复制操作，这个流时通过参数stream来指定的。当函数返回时，我们无法确保复制操作是否已经启动，更无法保证它是否已经结束。**我们能够得到的保证是，复制操作肯定会将肯定会当下一个被放入流中的操作之前执行。**任何传递给cudaMemcpyAsync()的主机内存指针都必须已经通过cudaHostAlloc()分配好内存。也就是，你只能以异步方式对页锁定内存进行复制操作。\n在核函数调用的尖括号中还可以带有一个流参数。此时核函数调用将是异步的。从技术上来说，当循环迭代完一次时，有可能不会启动任何内存复制或核函数执行。我们能够确保的是，第一次放入流中的复制操作将在第二次复制操作之前执行。此外，第二个复制操作将在核函数启动之前完成。而核函数将在第三次复制操作开始之前完成。\n当for循环结束时，在队列中应该包含了许多等待GPU执行的工作。如果想要确保GPU执行完了计算和内存复制等操作，那么就需要将GPU与主机同步。也就是说，主机在继续执行之前，要首先等待GPU执行完成。可以调用cudaStreamSynchronize()并制定想要等待的流。当程序执行到stream与主机同步之后的代码时，所有的计算和复制操作都已经完成，因此停止计时器，收集性能数据，并释放输入缓冲区和输出缓冲区。\n//将计算结果从页锁定内存复制到主机内存 HANDLE_ERROR(cudaStreamSynchronize(stream)); HANDLE_ERROR(cudaEventRecord(stop, 0)); HANDLE_ERROR(cudaEventSynchronize(stop)); HANDLE_ERROR(cudaEventElapsedTime(\u0026amp;elapsedTime, start, stop)); printf(\u0026#34;Time taken: 3.1%f ms\\n\u0026#34;, elapsedTime); //释放流和内存 HANDLE_ERROR(cudaFreeHost(host_a)); HANDLE_ERROR(cudaFreeHost(host_b)); HANDLE_ERROR(cudaFreeHost(host_c)); HANDLE_ERROR(cudaFree(dev_a)); HANDLE_ERROR(cudaFree(dev_b)); HANDLE_ERROR(cudaFree(dev_c)); //销毁对GPU操作进行排队的流 HANDLE_ERROR(cudaStreamDestroy(stream)); } 使用多个CUDA流 我们将上面的示例改为使用两个不同的流。我们将实现：在第0个流执行核函数的同时，第1个流将输入缓冲区复制到GPU。然后在第0个流将计算结果复制回主机的同时，第1个流将执行核函数。接着，第1个流将计算结果复制回主机，同时第0个流开始在下一块数据上执行核函数。假设内存复制操作和核函数执行的事件大致相同，那么应用程序的执行时间线将如图下所示（后续图片中函数调用cudaMemcpyAsync()被简写为复制）：\n核函数的代码保持不变。与使用单个流的版本一样，我们将判断设备是否支持计算与内存复制操作的重叠。如果设备支持重叠，那么就像前面一样创建CUDA事件并对应用程序计时。创建两个流的方式与之前代码中创建单个流的方式是一样的。\n#include \u0026#34;../common/book.h\u0026#34; #define N (1024 * 1024) #define FULL_DATA_SIZE (N * 20) __global__ void kernel(int *a, int *b, int *c){ int idx = threadIdx.x + blockIdx.x * blockDim.x; if(idx \u0026lt; N){ int idx1 = (idx + 1) % 256; int idx2 = (idx + 2) % 256; float as = (a[idx] + a[idx1] + a[idx2]) / 3.0f; float bs = (b[idx] + b[idx1] + b[idx2]) / 3.0f; c[idx] = (as + bs) / 2; } } int main(){ cudaDeviceProp prop; int whichDevice; HANDLE_ERROR(cudaGetDevice(\u0026amp;whichDevice)); HANDLE_ERROR(cudaGetDeviceProperties(\u0026amp;prop, whichDevice)); if(!prop.deviceOverlap){ printf(\u0026#34;Device will not handle overlaps, so no speed up from streams\\n\u0026#34;); } cudaEvent_t start, stop; float elapsedTime; //启动计时器 HANDLE_ERROR(cudaEventCreate(\u0026amp;start)); HANDLE_ERROR(cudaEventCreate(\u0026amp;stop)); HANDLE_ERROR(cudaEventRecord(start, 0)); //初始化流 cudaStream_t stream0, stream1; HANDLE_ERROR(cudaStreamCreate(\u0026amp;stream0)); HANDLE_ERROR(cudaStreamCreate(\u0026amp;stream1)); //数据分配操作 int *host_a, *host_b, *host_c; int *dev_a0, *dev_b0, *dev_c0; //为第0个流分配的GPU内存 int *dev_a1, *dev_b1, *dev_c1; //为第1个流分配的GPU内存 //在GPU上分配内存 HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_a0, N * sizeof(int))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_b0, N * sizeof(int))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_c0, N * sizeof(int))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_a1, N * sizeof(int))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_b1, N * sizeof(int))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_c1, N * sizeof(int))); //分配由流使用的页锁定内存 HANDLE_ERROR(cudaHostAlloc((void**)\u0026amp;host_a, FULL_DATA_SIZE * sizeof(int), cudaHostAllocDefault)); HANDLE_ERROR(cudaHostAlloc((void**)\u0026amp;host_b, FULL_DATA_SIZE * sizeof(int), cudaHostAllocDefault)); HANDLE_ERROR(cudaHostAlloc((void**)\u0026amp;host_c, FULL_DATA_SIZE * sizeof(int), cudaHostAllocDefault)); for(int i = 0; i \u0026lt; FULL_DATA_SIZE; i++){ host_a[i] = rand(); host_b[i] = rand(); } 之后，程序在输入数据块上循环。然而，由于现在使用了两个流，因此在for()循环的迭代中需要处理的数据量也是原来的两倍。在stream()中，我们首先将a和b的异步复制操作放入GPU的队列，然后将一个核函数执行放入队列，接下来再将一个复制回c的操作放入队列：\n//在整体数据上循环，每个数据块的大小为N for(int i = 0; i \u0026lt; FULL_DATA_SIZE; i += N * 2){ //将锁定内存以异步方式复制到设备上 HANDLE_ERROR(cudaMemcpyAsync(dev_a0, host_a + i, N * sizeof(int), cudaMemcpyHostToDevice, stream0)); HANDLE_ERROR(cudaMemcpyAsync(dev_b0, host_b + i, N * sizeof(int), cudaMemcpyHostToDevice, stream0)); kernel\u0026lt;\u0026lt;\u0026lt;N / 256, 256, 0, stream0\u0026gt;\u0026gt;\u0026gt;(dev_a0, dev_b0, dev_c0); //将数据从设备复制回锁定内存 HANDLE_ERROR(cudaMemcpyAsync(host_c + i, dev_c0, N * sizeof(int), cudaMemcpyDeviceToHost, stream0)); //在将这些操作放入stream0队列后，再把下一个数据块上的相同操作放入stream1的队列中 //将锁定内存以异步方式复制到设备上 HANDLE_ERROR(cudaMemcpyAsync(dev_a1, host_a + i + N, N * sizeof(int), cudaMemcpyHostToDevice, stream1)); HANDLE_ERROR(cudaMemcpyAsync(dev_b1, host_b + i + N, N * sizeof(int), cudaMemcpyHostToDevice, stream1)); kernel\u0026lt;\u0026lt;\u0026lt;N / 256, 256, 0, stream1\u0026gt;\u0026gt;\u0026gt;(dev_a1, dev_b1, dev_c1); //将数据从设备复制回锁定内存 HANDLE_ERROR(cudaMemcpyAsync(host_c + i + N, dev_c1, N * sizeof(int), cudaMemcpyDeviceToHost, stream0)); } 这样，在for循环的迭代过程中，将交替地把每个数据块放入这两个流的队列，直到所有待处理的输入数据都被放入队列。在结束了for循环后，在停止应用程序的计时器之前，首先将GPU与GPU进行同步，由于使用了两个流，因此要对二者都进行同步。\nHANDLE_ERROR(cudaStreamSynchronize(stream0)); HANDLE_ERROR(cudaStreamSynchronize(stream1)); HANDLE_ERROR(cudaEventRecord(stop, 0)); HANDLE_ERROR(cudaEventSynchronize(stop)); HANDLE_ERROR(cudaEventElapsedTime(\u0026amp;elapsedTime, start, stop)); printf(\u0026#34;Time taken: 3.1%f ms\\n\u0026#34;, elapsedTime); //释放流和内存 HANDLE_ERROR(cudaFreeHost(host_a)); HANDLE_ERROR(cudaFreeHost(host_b)); HANDLE_ERROR(cudaFreeHost(host_c)); //销毁两个流，释放两倍的GPU内存 HANDLE_ERROR(cudaFree(dev_a0)); HANDLE_ERROR(cudaFree(dev_b0)); HANDLE_ERROR(cudaFree(dev_c0)); HANDLE_ERROR(cudaFree(dev_a1)); HANDLE_ERROR(cudaFree(dev_b1)); HANDLE_ERROR(cudaFree(dev_c1)); //销毁对GPU操作进行排队的流 HANDLE_ERROR(cudaStreamDestroy(stream0)); HANDLE_ERROR(cudaStreamDestroy(stream1)); } GPU的工作调度机制 我们可以将流视为有序的操作序列，其中既包含内存复制操作，又包含核函数调用。下图将展示任务调度情形。\n从图中得到，第0个流对A的内存复制需要在对B的内存复制之前完成，而对B的复制又要在核函数A启动之前完成。然而，一旦这些操作放入到硬件的内存复制引擎和核函数执行引擎的队列中时，这些依赖性将丢失，因此CUDA驱动程序需要确保硬件的执行单元不破坏流内部的依赖性。\n从之前的代码中可以得知，应用程序基本上是对a调用一次cudaMemcpyAsync()，对b调用一次cudaMemcpyAsync()，然后再是执行核函数以及调用cudaMemcpyAsync()将c复制回主机。应用程序首先将第0个流的所有操作放入队列，然后是第1个流的所有操作。CUDA驱动程序负责按照这些操作的顺序把他们调度到硬件上执行，这就维持了流内部的依赖性。下图体现了这些依赖性，其中从复制操作到核函数的箭头表示，复制操作要等核函数执行完成之后才能开始。\n假定理解了GPU的工作调度远离后，我们可以得到关于这些操作在硬件上执行的时间线，如下图所示：\n由于第0个流中将c复制回主机的操作要等待核函数执行完成，因此第1个流中将a和b复制到GPU的操作虽然是完全独立的，但却被阻塞了，这是因为GPU引擎是按照指定的顺序来执行工作。这种情况很好地说明了为什么在程序中使用了两个流却无法获得加速的窘境。这个问题的直接原因是我们没有意识到硬件的工作方式与CUDA流编程模型的方式是不同的。\n高效地使用多个CUDA流 从上面的说明可以得出，如果同时调度某个流的所有操作，那么很容易在无意中阻塞另一个流的复制操作或者核函数执行。要解决这个问题，在将操作放入流的队列时应采用宽度优先的方式，而非深度优先的方式。也就是说不是首先添加第0个流的所有四个操作（即a的复制、b的复制、核函数以及c的复制），然后不再添加第1个流的所有四个操作，而是将这两个流之间的操作交叉添加。\n首先，将a的复制操作添加到第0个流，然后将a的复制操作添加到第1个流。接着，将b的复制操作添加到第0个流，再将b的复制操作添加到第1个流。接下来，将核函数调用添加到第0个流，再将相同的操作添加到第1个流中。最后，将c的复制操作添加到第0个流中，然后将相同的操作添加到第1个流中。\n下面是实际的代码。我们的修改仅限于for循环中的两个流的处理，采用宽度优先方式将操作分配到两个流的代码如下：\nfor(int i = 0; i \u0026lt; FULL_DATA_SIZE; i += N * 2){ //将复制a的操作放入stream0和stream1的队列 HANDLE_ERROR(cudaMemcpyAsync(dev_a0, host_a + i, N * sizeof(int), cudaMemcpyHostToDevice, stream0)); HANDLE_ERROR(cudaMemcpyAsync(dev_a1, host_a + i + N, N * sizeof(int), cudaMemcpyHostToDevice, stream1)); //将复制b的操作放入stream0和stream1的队列 HANDLE_ERROR(cudaMemcpyAsync(dev_b0, host_b + i, N * sizeof(int), cudaMemcpyHostToDevice, stream0)); HANDLE_ERROR(cudaMemcpyAsync(dev_b1, host_b + i + N, N * sizeof(int), cudaMemcpyHostToDevice, stream1)); //将核函数的执行放入stream0和stream1的队列中 kernel\u0026lt;\u0026lt;\u0026lt;N / 256, 256, 0, stream0\u0026gt;\u0026gt;\u0026gt;(dev_a0, dev_b0, dev_c0); kernel\u0026lt;\u0026lt;\u0026lt;N / 256, 256, 0, stream1\u0026gt;\u0026gt;\u0026gt;(dev_a1, dev_b1, dev_c1); //将复制c的操作放入stream0和stream1的队列 HANDLE_ERROR(cudaMemcpyAsync(host_c + i, dev_c0, N * sizeof(int), cudaMemcpyDeviceToHost, stream0)); HANDLE_ERROR(cudaMemcpyAsync(host_c + i + N, dev_c1, N * sizeof(int), cudaMemcpyDeviceToHost, stream1)); } 此时，新的执行时间线将如下图所示：\n","date":"February 28, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E4%B8%83/","series":[],"smallImg":"","tags":[{"title":"CUDA","url":"/tags/cuda/"}],"timestamp":1709078400,"title":"CUDA学习(七)"},{"authors":[],"categories":[{"title":"Tutorial","url":"/categories/tutorial/"}],"content":"在某些情况中，对于单线程应用程序来说非常简单的任务，在大规模并行架构上实现时会变成一个复杂的问题。在本章中，我们将举例其中一些情况，并在这些情况中安全地完成传统单线程应用程序中的简单任务。\n原子操作简介 在编写传统的单线程应用程序时，程序员通常不需要使用原子操作。下面会介绍一下原子操作是什么，以及为什么在多线程程序中需要使用它们。我们先分析C或者C++的递增运算符：\nx++;\n在这个操作中包含了三个步骤：\n读取x中的值。 将步骤1中读到的值增加1。 将递增后的结果写回到x。 有时候，这个过程也称为读取-修改-写入操作。\n当两个线程都需要对x的值进行递增时，假设x的初始值为7，理想情况下，两个线程按顺序对x进行递增，第一个线程完成三个步骤后第二个线程紧接着完成三个步骤，最后得到的结果是9。但是，实际情况下会出现两个线程的操作彼此交叉进行，这种情况下得到的结果将小于9(比如两个线程同时读取x=7，计算完后写入，那样的话x最后会等于8)。\n因此，我们需要通过某种方式一次性地执行完读取-修改-写入这三个操作，并在执行过程中不会被其他线程中断。由于这些操作的执行过程不能分解为更小的部分，因此我们将满足这种条件限制的操作称为原子操作。\n计算直方图 本章将通过给出计算直方图的例子来介绍如何进行原子性计算。\n在CPU上计算直方图 某个数据的直方图表示每个元素出现的频率。在示例中，这个数据将是随机生成的字节流。我们可以通过工具函数big_random_block()来生成这个随机的字节流。在应用程序中将生成100MB的随机数据。\n#include \u0026#34;../common/book.h\u0026#34; #define SIZE (100 * 1024 * 1024) int main(){ unsigned char *buffer = (unsigned char*)big_random_block(SIZE); } 由于每个随机字节（8比特）都有256个不同的可能取值（从0x00到0xFF），因此在直方图中需要包含256个元素，每个元素记录相应的值在数据流中的出现次数。我们创建了一个包含256个元素的数组，并将所有元素的值初始化为0。\nunsigned int histo[256]; for(int i = 0; i \u0026lt; 256; i++){ histo[i] = 0; } 接下来需要计算每个值在buffer[]数据中的出现频率。算法思想是，每当在数组buffer[]中出现某个值z时，就递增直方图数组中索引为z的元素，这样就能计算出值z的出现次数。如果当前看到的值为buffer[i]，那么将递增数组中索引等于buffer[i]的元素。由于元素buffer[i]位于histo[buffer[i]]，我们只需一行代码就可以递增相应的计数器。在一个for循环中对buffer[]中的每个元素执行这个操作：\nfor(int i = 0; i \u0026lt; SIZE; i++){ histo[buffer[i]]++; } 接下来将验证直方图的所有元素相加起来是否等于正确的值。\nlong histoCount = 0; for(int i = 0; i \u0026lt; 256; i++){ histoCount += histo[i]; } printf(\u0026#34;Histogram Sum: %ld\\n\u0026#34;, histoCount); free(buffer); 在GPU上计算直方图 以下时计算直方图的GPU版本\nint main(){ unsigned char* buffer = (unsigned char*)big_random_block(SIZE); //初始化计时事件 cudaEvent_t start, stop; HANDLE_ERROR(cudaEventCreate(\u0026amp;start)); HANDLE_ERROR(cudaEventCreate(\u0026amp;stop)); HANDLE_ERROR(cudaEventRecord(start, 0)); //在GPU上为文件的数据分配内存 unsigned char *dev_buffer; unsigned int *dev_histo; HANDLE_ERROR(cudaMallc((void**)\u0026amp;dev_buffer, SIZE)); HANDLE_ERROR(cudaMemcpy(dev_buffer, buffer, SIZE, cudaMemcpyHostToDevice)); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_histo, 256 * sizeof(int))); HANDLE_ERROR(cudaMemset(dev_histo, 0, 256 * sizeof(int))); unsigned int histo[256]; HANDLE_ERROR(cudaMemcpy(histo, dev_histo, 256 * sizeof(int), cudaMemcpyDeviceToHost)); //得到停止事件并显示计时结果 HANDLE_ERROR(cudaEventRecord(stop, 0)); HANDLE_ERROR(cudaEventSynchronize(stop)); float elapsedTime; HANDLE_ERROR(cudaEventElapsedTime(\u0026amp;elapsedTime, start, stop)); printf(\u0026#34;Time to generate: %3.1f ms\\n\u0026#34;, elapsedTime); //下面是验证直方图的总和是否等于正确的值，因为是在CPU上运行，并不需要对此进行计时 long histoCount = 0; for(int i = 0; i \u0026lt; 256; i++){ histoCount += histo[i]; } printf(\u0026#34;Histogram Sum: %ld\\n\u0026#34;, histoCount); //验证GPU与CPU的搭配的是相同的计数值 for(int i = 0; i \u0026lt; SIZE; i++){ histo[buffer[i]]--; } for(int i = 0; i \u0026lt; 256; i++){ if(histo[i] != 0){ printf(\u0026#34;Failure at %d!\\n\u0026#34;, i); } } HANDLE_ERROR(cudaEventDestroy(start)); HANDLE_ERROR(cudaEventDestroy(stop)); cudaFree(dev_histo); cudaFree(dev_buffer); free(buffer); } cudaMemset()与C的memset()是相似的，不同之处在于cudaMemset()将返回一个错误码，将高速调用者在设置GPU内存时发生的错误。\n接下来会介绍GPU上计算直方图的代码。计算直方图的核函数需要的参数包括：\n一个指向输入数组的指针 输入数组的长度 一个指向输出直方图的指针 核函数执行的第一个计算就是计算输入数据数组中的偏移。每个线程的起始偏移都是0到线程数量减1之间的某个值，然后，对偏移的增量为已启动线程的总数。\n#include \u0026#34;../common/book.h\u0026#34; #define SIZE (100 * 1024 * 1024) __global__ void histo_kernel(unsigned char *buffer, long size, unsigned int *histo){ int i = threadIdx.x + blockIdx.x * blockDim.x; int stride = blockDim.x * gridDim.x; //当每个线程知道它的起始偏移i以及递增的数量，这段代码将遍历输入数组，并递增直方图中相应元素的值 while(i \u0026lt; size){ atomicAdd(\u0026amp;(histo[buffer[i]]), 1); i += stride; } } 函数调用atomicAdd(addr, y)将生成一个原子的操作序列，这个操作序列包括读取地址addr处的值，将y增加到这个值，以及将结果保存回地址addr。底层硬件将确保当执行这些操作时，其他任何线程都不会读取或写入地址addr上的值。\n然而，原子操作回导致性能降低，但是解决问题的方法有时会需要更多而非更少的原子操作。**这里的主要问题并非在于使用了过多的原子操作，而是有数千个线程在少量的内存地址上发生竞争。**要解决这个问题，我们需要将直方图计算分为两个阶段。\n第一个阶段，每个并行线程块将计算它所处理数据的直方图。每个线程块在执行这个操作时都是相互独立的，因此可以在共享内存中计算这些直方图，这将避免每次将写入操作从芯片发送到DRAM。但是这种方式仍然需要原子操作，因为线程块中的多个线程之间仍然会处理相同值的数据元素。**不过，现在只有256个线程在256个地址上发生竞争，这将极大地减少在使用全局内存时在数千个线程之间发生竞争的情况。**我们将使用共享内存缓冲区temp[]而不是全局内存缓冲区histo[]，而且需要随后调用__syncthreads()来确保提交最后的写入操作。 第二个阶段则是将每个线程块的临时直方图合并到全局缓冲区histo[]中。由于我们使用了256个线程，并且直方图中包含了256个元素，因此每个线程将自动把它计算得到的元素只增加到最终直方图的元素上（如果线程数量不等于元素数量，那么这个阶段将更为复杂）。我们并不保证线程块将按照何种顺序将各自的值相加到最终直方图中，但由于整数加法时可交换的，无论哪种顺序都会得到相同的结果。 __global__ void histo_kernel(unsigned char* buffer, long size, unsigned int *histo){ __shared__ unsigned int temp[256]; temp[threadIdx.x] = 0; __syncthreads(); int i = threadIdx.x + blockIdx.x * blockDim.x; int stride = blockDim.x * gridDim.x; while(i \u0026lt; size){ atomicAdd(\u0026amp;temp[buffer[i]], 1); i += offset; } __syncthreads(); atomicAdd(*(histo[threadIdx.x]), temp[threadIdx.x]); } ","date":"February 27, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E5%85%AD/","series":[],"smallImg":"","tags":[{"title":"CUDA","url":"/tags/cuda/"}],"timestamp":1708992000,"title":"CUDA学习(六)"},{"authors":[],"categories":[{"title":"Tutorial","url":"/categories/tutorial/"}],"content":"本章将学习如何分配和使用纹理内存(texture memory)。和常量内存一样，纹理内存是另一种类型的只读内存，在特定的访问模式中，纹理内存同样能够提升性能并减少内存流量。虽然纹理内存最初是针对传统的图形处理应用程序而设计的，但在某些GPU计算应用程序中同样非常有用。\n与常量内存相似的是，纹理内存同样缓存在芯片上，因此在某些情况中，它能够减少对内存请求并提供更高效的内存带宽。纹理缓存是专门为那些在内存访问模式中存在大量空间局部性(spatial locality)的图形应用程序而设计的。在某个计算应用程序中，这意味着一个线程读取的位置可能与邻近线程读取的位置“非常接近”。\n热传导模拟 本章用一个简单的热传导模拟的模型来介绍如何使用纹理内存。\n简单的传热模型 我们构造一个简单的二维热传导模拟。首先假设有一个矩形房间，将其分成一个格网，每个格网中随机散步一些“热源”，他们有着不同的温度。\n在给定了矩形格网以及热源分布后，我们可以计算格网中每个单元格的温度随时间的变化情况。为了简单，热源单元本身的温度将保持不变。在时间递进的每个步骤中，我们假设热量在某个单元及其邻接单元之间“流动”。如果某个单元的临界单元的温度更高，那么热量将从邻接单元传导到该单元，相反地，如果某个单元的温度比邻接单元的温度高，那么它将变冷。\n在热传导模型中，我们对单元中新温度的计算方法为，将单元与邻接单元的温差相加起来，然后加上原有温度。 $$ T_{NEW} = T_{OLD} + \\sum_{NEIGHBOURS}k(T_{NEIGHBORS} - T_{OLD}) $$ 在上面的计算单元温度的等式中，常量k表示模拟过程中热量的流动速率，k值越大，表示系统会更快地达到稳定温度，而k值越小，则温度梯度将存在更长时间。由于我们只考虑4个邻接单元(上、下、左、右)并且等式中的k和$$T_{OLD}$$都是常数，因此把上述公式展开表示为: $$ T_{NEW} = T_{OLD} + k(T_{TOP} + T_{BOTTOM} + T_{LEFT} + T_{RIGHT} - 4T_{OLD}) $$\n温度更新的计算 以下是更新流程的基本介绍:\n给定一个包含初始输入温度的格网，将其中作为热源的单元温度值复制到格网相应的单元中来覆盖这些单元之前计算出来的温度，确保“加热单元将保持恒温”的条件。这个复制操作在copy_const_kernel()中执行。 给定一个输入温度格网，根据新的公式计算出输出温度格网。这个更新操作在blend_kernel()中执行。 将输入温度格网和输出温度格网交换，为下一个步骤的计算做好准备。当模拟下一个时间步时，在步骤2中计算得到的输出温度格网将成为步骤1中的输入温度格网。 下面是两个函数的具体实现:\n__global__ void copy_const_kernel(float *iptr, const float *cptr){ //将threadIdx/BlockIdx映射到像素位置 int x = threadIdx.x + blockIdx.x * blockDim.x; int y = threadIdx.y + blockIdx.y * blockDim.y; int offset = x + y * blockDim.x * gridDim.x; if(cptr[offset] != 0){ iptr[offset] = cptr[offset]; } } //为了执行更新操作，可以在模拟过程中让每个线程都负责计算一个单元。 //每个线程都将读取对应单元及其邻接单元的温度值，执行更新运算，然后计算得到新值来更新温度。 __global__ void blend_kernel(float *outSrc, const float *inSrc){ //将threadIdx/BlockIdx映射到像素位置 int x = threadIdx.x + blockIdx.x * blockDim.x; int y = threadIdx.y + blockIdx.y * blockDim.y; int offset = x + y * blockDim.x * gridDim.x; int left = offset - 1; int right = offset + 1; if(x == 0) left++; if(x == DIM - 1) right--; int top = offset - DIM; int bottom = offset + DIM; if(y == 0) bottom += DIM; if(y == DIM - 1) top -= DIM; outSrc[offset] = inSrc[offset] + SPEED * (inSrc[top] + inSrc[bottom] + inSrc[left] + inSrc[right] - inSrc[offset] * 4); } 模拟过程动态演示 剩下的代码主要是设置好单元，然后显示热量的动画输出\n#include \u0026#34;cuda.h\u0026#34; #include \u0026#34;../common/book.h\u0026#34; #include \u0026#34;cpu_anim.h\u0026#34; #define DIM 1024 #define PI 3.1415926535897932f #define MAX_TEMP 1.0f #define MIN_TEMP 0.0001f #define SPEED 0.25f //更新函数中需要的全局变量 struct DataBlock{ unsigned char *output_bitmap; float *dev_inSrc; float *dev_outSrc; float *dev_constSrc; CPUAnimBitmap *bitmap; cudaEvent_t start, stop; float totalTime; float frames; } void anim_gpu(DataBlock *d, int ticks){ HANDLE_ERROR(cudaEventRecord(d-\u0026gt;start, 0)); dim3 blocks(DIM / 16, DIM / 16); dim3 threads(16, 16); CPUAnimBitmap *bitmap = d-\u0026gt;bitmap; for(int i = 0; i \u0026lt; 90; 0++){ copy_const_kernel\u0026lt;\u0026lt;\u0026lt;blocks, threads\u0026gt;\u0026gt;\u0026gt;(d-\u0026gt;dev_inSrc, d-\u0026gt;dev_constSrc); blend_kernel\u0026lt;\u0026lt;\u0026lt;blocks, threads\u0026gt;\u0026gt;\u0026gt;(d-\u0026gt;dev_inSrc, d-\u0026gt;dev_constSrc); swap(d-\u0026gt;dev_inSrc, d-\u0026gt;dev_outSrc); } float_to_color\u0026lt;\u0026lt;\u0026lt;blocks, threads\u0026gt;\u0026gt;\u0026gt;(d-\u0026gt;output_bitmap, d-\u0026gt;dev_inSrc); HANDLE_ERROR(cudaMemcpy(bitmap-\u0026gt;get_ptr(), d-\u0026gt;output_bitmap, bitmap-\u0026gt;image_size(), cudaMemcpyDeviceToHost)); HANDLE_ERROR(cudaEventRecord(d-\u0026gt;stop, 0)); HANDLE_ERROR(cudaEventSynchronize(d-\u0026gt;stop)); float elapsedTime; HANDLE_ERROR(cudaEventElapsedTime(\u0026amp;elapsedTime, d-\u0026gt;start, d-\u0026gt;stop)); d-\u0026gt;totalTime += elapsedTime; ++d-\u0026gt;frames; printf(\u0026#34;Averaged Time per frame: $3.1f ms \\n\u0026#34;, d-\u0026gt;totalTime / d-\u0026gt;frames); } void anim_exit(DataBlock *d){ cudaFree(d-\u0026gt;dev_inSrc); cudaFree(d-\u0026gt;dev_outSrc); cudaFree(d-\u0026gt;dev_constSrc); HANDLE_ERROR(cudaEventDestroy(d-\u0026gt;start)); HANDLE_ERROR(cudaEventDestroy(d-\u0026gt;stop)); } int main(){ DataBlock data; CPUAnimBitmap bitmap(DIM, DIM, \u0026amp;data); data.bitmap = \u0026amp;bitmap; data.totalTime = 0; data.frames = 0; HANDLE_ERROR(cudaEventCreate(\u0026amp;data.start)); HANDLE_ERROR(cudaEventCreate(\u0026amp;data.stop)); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;data.output_bitmap, bitmap.image_size())); //假设float类型的大小为4个字符(即rgba) HANDLE_ERROR(cudaMalloc((void**)\u0026amp;data.dev_inSrc, bitmap.image_size())); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;data.dev_outSrc, bitmap.image_size())); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;data.dev_constSrc, bitmap.image_size())); float *temp = (float*)malloc(bitmap.image_size()); for(int i = 0; i \u0026lt; DIM*DIM; i++){ temp[i] = 0; int x = i % DIM; int y =i / DIM; if((x \u0026gt; 300) \u0026amp;\u0026amp; (x \u0026lt; 600) \u0026amp;\u0026amp; (y \u0026gt; 310) \u0026amp;\u0026amp; (y \u0026lt; 601)){ temp[i] = MAX_TEMP; } temp[DIM * 100 + 100] = (MAX_TEMP + MIN_TEMP) / 2; temp[DIM * 700 + 100] = MIN_TEMP; temp[DIM * 300 + 300] = MIN_TEMP; temp[DIM * 200 + 700] = MIN_TEMP; for(int y = 800; y \u0026lt; 900; y++){ for(int x = 400; x \u0026lt; 500; x++){ temp[x * y * DIM] = MIN_TEMP; } } HANDLE_ERROR(cudaMemcpy(data.dev_constSrc, temp, bitmap.image_size(), cudaMemcpyHostToDevice)); free(temp); bitmap.anim_and_exit((void (*) (void*, int)) anim_gpu, (void (*) (void*)) anim_exit); } } 使用纹理内存 如果要使用纹理内存，首先要将输入的数据声明为texture类型的引用。我们使用浮点类型纹理的引用，因为温度数值是浮点类型。\n//这些变量将位于GPU上 texture\u0026lt;float\u0026gt; texConstSrc; texture\u0026lt;float\u0026gt; textIn; texture\u0026lt;float\u0026gt; textOut; 下一个需要注意的问题是，在为这三个缓冲区分配了GPU内存后，需要通过cudaBindTexture()将这些变量绑定到内存缓冲区。这相当于告诉CUDA运行时两件事情：\n我们希望将制定的缓冲区作为纹理来使用。 我们希望将纹理引用作为纹理的“名字”。 在热传导模拟中分配了这三个内存后，需要将这三个内存绑定到之前声明的纹理引用(texConstSrc, textIn, textOut)。\nHANDLE_ERROR(cudaMalloc((void**)\u0026amp;data.dev_inSrc, imageSize)); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;data.dev_outSrc, imageSize)); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;data.dev_constSrc, imageSize)); HANDLE_ERROR(cudaBindTexture(NULL, textConstSrc, data.dev_constSrc, imageSize)); HANDLE_ERROR(cudaBindTexture(NULL, textIn, data.dev_inSrc, imageSize)); HANDLE_ERROR(cudaBindTexture(NULL, textOut, data.dev_outStc, imageSize)); 此时，纹理变量已经设置好，可以启动核函数。然而，当读取核函数中的纹理时，需要通过特殊的函数来告诉GPU将读取请求转发到纹理内存而不是标准的全局内存。因此，当读取内存时不再使用方括号从缓冲区读取，而是将blend_kernel()函数内修改为使用tex1Dfetch()。\ntex1Dfetch()实际上是一个编译器内置函数。由于纹理引用必须声明为文件作用域内的全局变量，因此我们不再将输入缓冲区和输出缓冲区作为参数传递给blend_kernel()，因为编译器需要在编译时知道text1Dfetch()应该对哪些纹理采样。我们需要将一个布尔标志dstOut传递给blend_kernel()，这个标志会告诉我们使用那个缓冲区作为输入，以及哪个缓冲区作为输出。以下是对blend_kernel()的修改。\n__global__ void blend_kernel(float *dst, bool dstOut){ //将threadIdx/BlockIdx映射到像素位置 int x = threadIdx.x + blockIdx.x * blockDim.x; int y = threadIdx.y + blockIdx.y * blockDim.y; int offset = x + y * blockDim.x * gridDim.x; int left = offset - 1; int right = offset + 1; if(x == 0) left++; if(x == DIM - 1) right--; int top = offset - DIM; int bottom = offset + DIM; if(y == 0) bottom += DIM; if(y == DIM - 1) top -= DIM; //替换该行代码 // outSrc[offset] = inSrc[offset] + SPEED * (inSrc[top] + inSrc[bottom] + inSrc[left] + inSrc[right] - inSrc[offset] * 4); float t, l, c, r, b; if(dstOut){ t = text1Dfetch(textIn, top); l = text1Dfetch(textIn, left); c = text1Dfetch(textIn, offset); r = text1Dfetch(textIn, right); b = text1Dfetch(textIn, bottom); } else{ t = text1Dfetch(textOut, top); l = text1Dfetch(textOut, left); c = text1Dfetch(textOut, offset); r = text1Dfetch(textOut, right); b = text1Dfetch(textOut, bottom); } dst[offset] = c + SPEED * (t + b + r + l - 4 * c); } 由于核函数copy_const_kernel()将读取包含热源位置和温度的缓冲区，因此同样需要修改为从纹理内存而不是从全局内存中读取：\n__global__ void copy_const_kernel(float *iptr){ //将threadIdx/BlockIdx映射到像素位置 int x = threadIdx.x + blockIdx.x * blockDim.x; int y = threadIdx.y + blockIdx.y * blockDim.y; int offset = x + y * blockDim.x * gridDim.x; float c = text1Dfetch(textConstSrc, offset); if(c != 0){ iptr[offset] = c; } } 由于blend_kernel()的函数原型被修改为接收一个标志，并且这个标志表示在输入缓冲区与输出缓冲区之间的切换，因此需要对anim_gpu()函数进行相应的修改。现在，不是交换缓冲区，而是在每组调用之后通过设置dstOut = !dstOut来进行切换：\nvoid anim_gpu(DataBlock *d, int ticks){ HANDLE_ERROR(cudaEventRecord(d-\u0026gt;start, 0)); dim3 blocks(DIM / 16, DIM / 16); dim3 threads(16, 16); CPUAnimBitmap *bitmap = d-\u0026gt;bitmap; //for(int i = 0; i \u0026lt; 90; 0++){ // copy_const_kernel\u0026lt;\u0026lt;\u0026lt;blocks, threads\u0026gt;\u0026gt;\u0026gt;(d-\u0026gt;dev_inSrc, d-\u0026gt;dev_constSrc); // blend_kernel\u0026lt;\u0026lt;\u0026lt;blocks, threads\u0026gt;\u0026gt;\u0026gt;(d-\u0026gt;dev_inSrc, d-\u0026gt;dev_constSrc); // swap(d-\u0026gt;dev_inSrc, d-\u0026gt;dev_outSrc); //} //float_to_color\u0026lt;\u0026lt;\u0026lt;blocks, threads\u0026gt;\u0026gt;\u0026gt;(d-\u0026gt;output_bitmap, d-\u0026gt;dev_inSrc); //由于tex是全局并且有界的，因此我们必须通过一个标志来选择每次迭代中哪个是输入/输出 volatile bool dstOut = true; for(int i = 0; i \u0026lt; 90; i++){ float *in, *out; if(dstOut){ in = d-\u0026gt;dev_inSrc; out = d-\u0026gt;dev_outSrc; } else{ out = d-\u0026gt;dev_inSrc; in = d-\u0026gt;dev_outSrc; } copy_const_kernel\u0026lt;\u0026lt;\u0026lt;blocks, threads\u0026gt;\u0026gt;\u0026gt;(in); blend_kernel\u0026lt;\u0026lt;\u0026lt;blocks, threads\u0026gt;\u0026gt;\u0026gt;(out, dstOut); dstOut = !dstOut; } HANDLE_ERROR(cudaMemcpy(bitmap-\u0026gt;get_ptr(), d-\u0026gt;output_bitmap, bitmap-\u0026gt;image_size(), cudaMemcpyDeviceToHost)); HANDLE_ERROR(cudaEventRecord(d-\u0026gt;stop, 0)); HANDLE_ERROR(cudaEventSynchronize(d-\u0026gt;stop)); float elapsedTime; HANDLE_ERROR(cudaEventElapsedTime(\u0026amp;elapsedTime, d-\u0026gt;start, d-\u0026gt;stop)); d-\u0026gt;totalTime += elapsedTime; ++d-\u0026gt;frames; printf(\u0026#34;Averaged Time per frame: $3.1f ms \\n\u0026#34;, d-\u0026gt;totalTime / d-\u0026gt;frames); } 对热传导函数的最后一个修改就是在应用程序运行结束后的清理工作。不仅要释放全局缓冲区，还需要清楚与纹理的绑定：\nvoid anim_exit(DataBlock *d){ cudaUnbindTexture(textIn); cudaUnbindTexture(textOut); cudaUnbindTexture(texConstSrc); cudaFree(d-\u0026gt;dev_inSrc); cudaFree(d-\u0026gt;dev_outSrc); cudaFree(d-\u0026gt;dev_constSrc); HANDLE_ERROR(cudaEventDestroy(d-\u0026gt;start)); HANDLE_ERROR(cudaEventDestroy(d-\u0026gt;stop)); } 使用二维纹理内存 在多数情况下，二维内存空间是非常有用的。首先，要修改纹理引用的声明。默认的纹理引用都是一维的，因此我们需要增加代表维数的参数2，这表示声明的是一个二维纹理引用。\ntexture\u0026lt;float, 2\u0026gt; texConstSrc; textture\u0026lt;float, 2\u0026gt; texIn; textture\u0026lt;float, 2\u0026gt; texOut; 二维纹理将简化blend_kernel()方法的实现。虽然我们需要将tex1Dfeth()调用修改为text2D()调用，但却不再需要通过线性化offset变量以计算top、left、right和bottom等偏移。当使用二维纹理时，可以直接通过x和y来访问纹理。而且当使用tex2D()时，我们不再需要担心发生溢出问题。如果x或y小于0，那么tex2D()将返回0处的值。同理，如果某个值大于宽度，那么tex2D()将返回位于宽度处的值。这些建华带来的好处之一就是核函数的代码将变得更加简单。\n__global__ void blend_kernel(float *dst, bool dstOut){ //将threadIdx/BlockIdx映射到像素位置 int x = threadIdx.x + blockIdx.x * blockDim.x; int y = threadIdx.y + blockIdx.y * blockDim.y; int offset = x + y * blockDim.x * gridDim.x; float t, l, c, r, b; if(dstOut){ t = text2D(textIn, x, y - 1); l = text2D(textIn, x - 1, y); c = text2D(textIn, x, y); r = text2D(textIn, x + 1, y); b = text2D(textIn, x, y + 1); } else{ t = text2D(textOut, x, y - 1); l = text2D(textOut, x - 1, y); c = text2D(textOut, x, y); r = text2D(textOut, x + 1, y); b = text2D(textOut, x, y + 1); } dst[offset] = c + SPEED * (t + b + r + l - 4 * c); } 我们也需要对copy_const_kernel()中进行相应的修改。与核函数blend_kernel()类似的是，我们不再需要通过offset来访问纹理，而是只需使用x和y来访问热源的常量。\n__global__ void copy_const_kernel(float *iptr){ //将threadIdx/BlockIdx映射到像素位置 int x = threadIdx.x + blockIdx.x * blockDim.x; int y = threadIdx.y + blockIdx.y * blockDim.y; int offset = x + y * blockDim.x * gridDim.x; float c = text2D(textConstSrc, x, y); if(c != 0){ iptr[offset] = c; } } 在main()需要对纹理绑定调用进行修改，并告诉运行时：缓冲区将被视为二维纹理而不是一维纹理：\nint main(){ DataBlock data; CPUAnimBitmap bitmap(DIM, DIM, \u0026amp;data); data.bitmap = \u0026amp;bitmap; data.totalTime = 0; data.frames = 0; HANDLE_ERROR(cudaEventCreate(\u0026amp;data.start)); HANDLE_ERROR(cudaEventCreate(\u0026amp;data.stop)); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;data.output_bitmap, bitmap.image_size())); //假设float类型的大小为4个字符(即rgba) HANDLE_ERROR(cudaMalloc((void**)\u0026amp;data.dev_inSrc, bitmap.image_size())); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;data.dev_outSrc, bitmap.image_size())); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;data.dev_constSrc, bitmap.image_size())); cudaChannelFormatDesc desc = cudaCreateChannelDesc\u0026lt;float\u0026gt;(); HANDLE_ERROR(cudaBindTexture2D(NULL, texConstSrc, data.dev_constSrc, desc, DIM, DIM, sizeof(float * DIM))); HANDLE_ERROR(cudaBindTexture2D(NULL, texin, data.dev_inSrc, desc, DIM, DIM, sizeof(float * DIM))); HANDLE_ERROR(cudaBindTexture2D(NULL, texOut, data.dev_outSrc, desc, DIM, DIM, sizeof(float * DIM))); float *temp = (float*)malloc(bitmap.image_size()); for(int i = 0; i \u0026lt; DIM*DIM; i++){ temp[i] = 0; int x = i % DIM; int y =i / DIM; if((x \u0026gt; 300) \u0026amp;\u0026amp; (x \u0026lt; 600) \u0026amp;\u0026amp; (y \u0026gt; 310) \u0026amp;\u0026amp; (y \u0026lt; 601)){ temp[i] = MAX_TEMP; } temp[DIM * 100 + 100] = (MAX_TEMP + MIN_TEMP) / 2; temp[DIM * 700 + 100] = MIN_TEMP; temp[DIM * 300 + 300] = MIN_TEMP; temp[DIM * 200 + 700] = MIN_TEMP; for(int y = 800; y \u0026lt; 900; y++){ for(int x = 400; x \u0026lt; 500; x++){ temp[x * y * DIM] = MIN_TEMP; } } HANDLE_ERROR(cudaMemcpy(data.dev_constSrc, temp, bitmap.image_size(), cudaMemcpyHostToDevice)); free(temp); bitmap.anim_and_exit((void (*) (void*, int)) anim_gpu, (void (*) (void*)) anim_exit); } } 虽然我们需要通过不同的函数来告诉运行时绑定一维纹理还是二维纹理，但是可以通过同一个函数cudaUnbindTexture()来取消纹理绑定。所以执行释放操作的函数anim_exit()可以保持不变。\n","date":"February 23, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E4%BA%94/","series":[],"smallImg":"","tags":[{"title":"CUDA","url":"/tags/cuda/"}],"timestamp":1708646400,"title":"CUDA学习(五)"},{"authors":[],"categories":[{"title":"Tutorial","url":"/categories/tutorial/"}],"content":"这一章会介绍如何在CUDA C中使用常量内存、了解常量内存的性能特性以及学习如何使用CUDA事件来测量应用程序的性能。\n常量内存 到目前为止，我们知道CUDA C程序中可以使用全局内存和共享内存。但是，CUDA C还支持另一种类型的内存，即常量内存。常量内存用于保存在核函数执行期间不会发生变化的数据。在某些情况下，用常量内存来替换全局内存能有效地减少内存带宽。\n在GPU上实现光线跟踪 我们给出一个简单的光线跟踪应用程序示例来学习。由于OpenGL和DirectX等API都不是专门为了实现光线跟踪而设计的，因此我们必须使用CUDA C来实现基本的光线跟踪器。本示例构造的光线跟踪器非常简单，旨在学习常量内存的使用上(并不能通过示例代码来构建一个功能完备的渲染器)。这个光线跟踪器只支持一组包含球状物体的场景，并且相机被固定在了Z轴，面向原点。此外，示例代码也不支持场景中的任何照明，从而避免二次光线带来的复杂性。代码也不计算照明效果，而只是为每个球面分配一个颜色值，如果它们是可见的，则通过某个预先计算的值对其着色。\n光线跟踪器将从每个像素发射一道光线，并且跟踪到这些光线会命中哪些球面。此外，它还将跟踪每道命中光线的深度。当一道光线穿过多个球面时，只有最接近相机的球面才会被看到。这个代码的光线跟踪器会把相机看不到的球面隐藏起来。\n通过一个数据结构对球面建模，在数据结构中包含了球面的中心坐标(x, y, z)，半径radius，以及颜色值(r, g, b)。\n#define INF 2e10f struct sphere{ float r, g, b; float radius; float x, y, z; __device__ float hit(float ox, float oy, float *n){ float dx = ox - x; float dy = oy - y; if(dx * dx + dy * dy \u0026lt; radius * radius){ float dz = sqrtf(radius * radius - dx * dx - dy * dy); *n = dz / sqrtf(radius * radius); return dz + z; } return -INF; } } 这个结构中定义了一个方法hit(float ox, float oy, float *n)。对于来自(ox, oy)处像素的光线，这个方法将计算光线是否与这个球面相交。如果光线与球面相交，那么这个方法将计算从相机到光线命中球面处的距离。我们需要这个信息，因为当光线命中多个球面时，只有最接近相机的球面才会被看见。\nmain()函数遵循了与前面示例大致相同的代码结构。\n#include \u0026#34;cuda.h\u0026#34; #include \u0026#34;../common/book.h\u0026#34; #include \u0026#34;cpu_bitmap.h\u0026#34; #define rnd(x) (x * rand() / RAND_MAX) #define SPHERES 20 Sphere *s; int main(){ //记录起始时间 cudaEvent_ start, stop; HANDLE_ERROR(cudaEventCreate(\u0026amp;start)); HANDLE_ERROR(cudaEventCreate(\u0026amp;stop)); HANDLE_ERROR(cudaEventRecord(start, 0)); CPUBitmap bitmap(DIM, DIM); unsigned char *dev_bitmap; //在GPU上分配内存以计算输出位图 HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_bitmap, bitmap.image_size())); //为Sphere数据集分配内存 HANDLE_ERROR(cudaMalloc((void**)\u0026amp;s, sizeof(Sphere) * SPHERES)); } 在分配输入数据和输出数据的内存后，我们将随机地生成球面的中心坐标，颜色以及半径。\n//分配临时内存，对其初始化，并复制到GPU上的内存，然后释放临时内存 Sphere *temp_s = (Sphere*) malloc(sizeof(Sphere) * SPHERES); for(int i = 0; i \u0026lt; SPHERES; i++){ temp_s[i].r = rnd(1.0f); temp_s[i].g = rnd(1.0f); temp_s[i].b = rnd(1.0f); temp_s[i].x = rnd(1000.0f) - 500; temp_s[i].y = rnd(1000.0f) - 500; temp_s[i].z = rnd(1000.0f) - 500; temp_s[i].radius = rnd(100.0f) + 20; } 当前，程序将生成一个包含20个球面的随机数组，但这个数量值是通过一个#define宏指定的，因此可以相应的做出调整。\n通过cudaMemcpy()将这个球面数组复制到GPu，然后释放临时缓冲区。\nHANDLE_ERROR(cudaMemcpy(s, temps, sizeof(Sphere) * SPHERES, cudaMemcpyHostToDevice)); free(temp_s); 现在，输入数据位于GPU上，并且我们已经为输出数据分配好了空间，因此可以启动核函数。\n//从球面数据汇总生成一张位图 dim3 grids(DIM / 16, DIM / 16); dim3 threads(16, 16); kernel\u0026lt;\u0026lt;\u0026lt;grids, threads\u0026gt;\u0026gt;\u0026gt;(dev_bitmap); 这个核函数将执行光线跟踪计算并从输入的一组球面中为每个像素计算颜色数据。最后，我们把输出图像从GPU中复制回来，并显示它。我们还要释放所有已经分配但还未释放的内存。\n//将位图从GPU复制回到CPU以显示 HANDLE_ERROR(cudaMemcpy(bitmap.get_ptr(), dev_bitmap, bitmap.image_size(), cudaMemcpyDeviceToHost)); bitmap.display_and_exit(); //释放内存 cudaFree(dev_bitmap); cudaFree(s); 下面的代码将介绍如何实现核函数的光线跟踪算法。每个线程都会为输出影像中的一个像素计算颜色值，因此我们遵循一种惯用的方式，计算每个线程对应的x坐标和y坐标，并且根据这两个坐标来计算输出缓冲区的偏移。此外，我们还将把图像坐标(x, y, z)偏移DIM/2，这样z轴将穿过图像的中心。\n__global__ void kernel(unsigned char *ptr){ //将threadIdx/BlockIdx映射到像素位置 int x = threadIdx.x + blockIdx.x * blockDim.x; int y = threadIdx.y + blockIdx.y * blockDim.y; int offset = x + y * blockDim.x * gridDim.x; float ox = x - DIM / 2; float oy = y - DIM / 2; //由于每条光线都需要判断与球面相交的情况，因此我们现在对球面数组进行迭代，并判断每个球面的命中情况。 float r = 0, g = 0, b = 0; float maxz = -INF; for(int i = 0; i \u0026lt; SPHERES; i++){ float n; float t = s[i].hit(ox, oy, \u0026amp;n); if(t \u0026gt; maxz){ float fscale = n; r = s[i].r * fscale; g = s[i].g * fscale; b = s[i].b * fscale; } } //在判断了每个球面的相交情况后，可以将当前颜色值保存到输出图像中 ptr[offset * 4 + 0] = (int)(r * 255); ptr[offset * 4 + 1] = (int)(g * 255); ptr[offset * 4 + 1] = (int)(b * 255); ptr[offset * 4 + 3] = 255; } 通过常量内存来实现光线跟踪 上述代码中并没有提到常量内存。下面的代码将使用常量内存来修改这个例子。由于常量内存无法修改，因此显然无法用常量内存来保存输出图像的数据。在这个示例中只有一个输入数据，即球面数组，因此应该将这个数据保存到常量内存中。\n声明数组时，要在前面加上__constant__修饰符。\n__constant__ Sphere s[SPHERES]; 最初的示例中，我们声明了一个指针，然后通过cudaMalloc()来为指针分配GPU内存。当我们将其修改为常量内存时，同样要将这个声明修改为在常量内存中静态地分配空间。我们不再需要对球面数组调用cudaMalloc()或cudaFree()。而是在编译时为这个数组提交一个固定的大小。将main()函数修改为常量内存的代码如下:\nint main(){ CPUBitmap bitmap(DIM, DIM); unsigned char *dev_bitmap; //在GPU上分配内存以计算输出位图 HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_bitmap, bitmap.image_size())); //分配临时内存，对其初始化，并复制到GPU上的内存，然后释放临时内存 Sphere *temp_s = (Sphere*) malloc(sizeof(Sphere) * SPHERES); for(int i = 0; i \u0026lt; SPHERES; i++){ temp_s[i].r = rnd(1.0f); temp_s[i].g = rnd(1.0f); temp_s[i].b = rnd(1.0f); temp_s[i].x = rnd(1000.0f) - 500; temp_s[i].y = rnd(1000.0f) - 500; temp_s[i].z = rnd(1000.0f) - 500; temp_s[i].radius = rnd(100.0f) + 20; } HANDLE_ERROR(cudaMemcpyToSymbol(s, temp_s, sizeof(Sphere) * SPHERES)); free(temp_s); //从球面数据中生成一张位图 dim3 grids(DIM / 16, DIM / 16); dim3 threads(16, 16); kernel\u0026lt;\u0026lt;\u0026lt;grids, threads\u0026gt;\u0026gt;\u0026gt;(dev_bitmap); //将位图从GPU复制回到CPU以显示 HANDLE_ERROR(cudaMemcpy(bitmap.get_ptr(), dev_bitmap, bitmap.image_size(), cudaMemcpyDeviceToHost)); bitmap.display_and_exit(); //释放内存 cudaFree(dev_bitmap); } cudaMemcpyToSymbol()与参数为cudaMemcpyHostToDevice()的cudaMemcpy()之间唯一差异时cudaMemcpyToSymbol()会复制到常量内存，而cudaMemcpy()会复制到全局内存。\n使用事件来测量性能 如何判断常量内存对程序性能有着多大影响？最简单的方式就是判断哪个版本的执行事件更短。使用CPU计时器或者操作系统中的某个计时器会带来各种延迟。为了测量GPU在某个任务上话费的时间，我们将使用CUDA的事件API。\nCUDA中的事件本质上是一个GPU时间戳，这个时间戳是在用户指定的时间点上记录的。由于GPU本身支持记录时间戳，因此就避免了当使用CPU定时器来统计GPU执行的事件时可能遇到的诸多问题。比如，下面的代码开头告诉CUDA运行时记录当前的时间，首先创建一个事件，然后记录这个事件。\ncudaEvent_t start; cudaEventCreate(\u0026amp;start); cudaEventRecord(start, 0); 要统计一段代码的执行时间，不仅要创建一个起始事件，还要创建一个结束事件。当在GPU上执行某个工作时，我们不仅要告诉CUDA运行时记录起始时间，还要记录结束时间:\ncudaEvent_t start, stop; cudaEventCreate(\u0026amp;start); cudaEventCreate(\u0026amp;stop); cudaEventRecord(start, 0); //在GPU执行一些工作 cudaEventRecord(stop, 0); cudaEventSynchronize(stop); //表示stop事件之前的所有GPU工作已经完成，可以安全读取在stop中保存的时间戳 下面是对光线跟踪器进行性能测试的代码:\nint main(){ //记录起始时间 cudaEvent_t start, stop; cudaEventCreate(\u0026amp;start); cudaEventCreate(\u0026amp;stop); cudaEventRecord(start, 0); CPUBitmap bitmap(DIM, DIM); unsigned char *dev_bitmap; //在GPU上分配内存以计算输出位图 HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_bitmap, bitmap.image_size())); //分配临时内存，对其初始化，并复制到GPU上的内存，然后释放临时内存 Sphere *temp_s = (Sphere*) malloc(sizeof(Sphere) * SPHERES); for(int i = 0; i \u0026lt; SPHERES; i++){ temp_s[i].r = rnd(1.0f); temp_s[i].g = rnd(1.0f); temp_s[i].b = rnd(1.0f); temp_s[i].x = rnd(1000.0f) - 500; temp_s[i].y = rnd(1000.0f) - 500; temp_s[i].z = rnd(1000.0f) - 500; temp_s[i].radius = rnd(100.0f) + 20; } HANDLE_ERROR(cudaMemcpyToSymbol(s, temp_s, sizeof(Sphere) * SPHERES)); free(temp_s); //从球面数据中生成一张位图 dim3 grids(DIM / 16, DIM / 16); dim3 threads(16, 16); kernel\u0026lt;\u0026lt;\u0026lt;grids, threads\u0026gt;\u0026gt;\u0026gt;(dev_bitmap); //将位图从GPU复制回到CPU以显示 HANDLE_ERROR(cudaMemcpy(bitmap.get_ptr(), dev_bitmap, bitmap.image_size(), cudaMemcpyDeviceToHost)); //获得结束时间，并显示计时结果 HANDLE_ERROR(cudaEventRecord(stop, 0)); HANDLE_ERROR(cudaEventSynchronize(stop)); float elapsedTime; HANDLE_ERROR(cudaEventElapsedTime(\u0026amp;elapsedTime, start, stop)); printf(\u0026#34;Time to generate: %3.1f ms\\n\u0026#34;, elapsedTime); HANDLE_ERROR(cudaEventDestroy(start)); HANDLE_ERROR(cudaEventDestroy(stop)); //显示位图 bitmap.display_and_exit(); //释放内存 cudaFree(dev_bitmap); } ","date":"February 18, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E5%9B%9B/","series":[],"smallImg":"","tags":[{"title":"CUDA","url":"/tags/cuda/"}],"timestamp":1708214400,"title":"CUDA学习(四)"},{"authors":[],"categories":[{"title":"Tutorial","url":"/categories/tutorial/"}],"content":"这一章将介绍线程块以及线程之间的通信机制和同步机制。\n在GPU启动并行代码的实现方法是告诉CUDA运行时启动核函数的多个并行副本。我们将这些并行副本称为线程块(Block)。\nCUDA运行时将把这些线程块分解为多个线程。当需要启动多个并行线程块时，只需将尖括号中的第一个参数由1改为想要启动的线程块数量。\n在尖括号中，第二个参数表示CUDA运行时在每个线程块中创建的线程数量。假设尖括号中的变量为\u0026laquo;\u0026lt;N, M\u0026raquo;\u0026gt;总共启动的线程数量可以按照以下公式计算: $$ N个线程块 * M个线程/线程块 = N*M个并行线程 $$\n使用线程实现GPU上的矢量求和 在之前的代码中，我们才去的时调用N个线程块，每个线程块对应一个线程add\u0026lt;\u0026lt;\u0026lt;N, 1\u0026gt;\u0026gt;\u0026gt;(dev_a, dev_b, dev_c);。\n如果我们启动N个线程，并且所有线程都在一个线程块中，则可以表示为add\u0026lt;\u0026lt;\u0026lt;1, N\u0026gt;\u0026gt;\u0026gt;(dev_a, dev_b, dev_c);。此外，因为只有一个线程块，我们需要通过线程索引来对数据进行索引(而不是线程块索引)，需要将int tid = blockIdx.x;修改为int tid = threadIdx.x;\n在GPU上对更长的矢量求和 对于启动核函数时每个线程块中的线程数量，硬件也进行了限制。具体来说，最大的线程数量不能超过设备树形结构中maxThreadsPerBlock域的值。对目前的GPU来说一个线程块最多有1024个线程。如果要通过并行线程对长度大于1024的矢量进行相加的话，就需要将线程与线程块结合起来才能实现。\n此时，计算索引可以表示为:\nint tid = threadIdx.x + blockIdx.x * blockDim.x; blockDim保存的事线程块中每一维的线程数量，由于使用的事一维线程块，因此只用到blockDim.x。\n此外，gridDim是二维的，而blockDim是三维的。\n假如我们使用多个线程块处理N个并行线程，每个线程块处理的线程数量为128，那样可以启动N/128个线程块。然而问题在于，当N小于128时，比如127，那么N/128等于0，此时将会启动0个线程块。所以我们希望这个除法能够向上取整。我们可以不用调用 ceil()函数，而是将计算改为(N+127)/N。因此，这个例子调用核函数可以写为:\nadd\u0026lt;\u0026lt;\u0026lt;(N + 127) / 128, 128\u0026gt;\u0026gt;\u0026gt;(dev_a, dev_b, dev_c); 当N不是128的整数倍时，将启动过多的线程。然而，在核函数中已经解决了这个问题。在访问输入数组和输出数组之前，必须检查线程的便宜是否位于0到N之间。\nif(tid \u0026lt; N){ c[tid] = a[tid] + b[tid]; } 因此，当索引越过数组的边界时，核函数将自动停止执行计算。核函数不会对越过数组边界的内存进行读取或者写入。\n在GPU上对任意长度的矢量求和 当矢量的长度很长时，我们可以让每一个线程执行多个矢量相加。例如\n__global__ void add(int *a, int *b, int *c){ int tid = threadIdx.x + blockIdx.x * blockDim.x; while(tid \u0026lt; N){ c[tid] = a[tid] + b[tid]; tid += blockDim.x * gridDim.x; } } 当每个线程计算完当前索引上的任务后，接着就需要对索引进行递增，其中递增的步长为线程格中正在运行的线程数量。这个数值等于每个线程块中的线程数量乘上线程格中线程块的数量，即blockDim.x * gridDim.x。\n共享内存和同步 CUDA C编译器对共享内存中的变量与普通变量分别采取不同的处理方式。对于在GPU上启动的每个线程块，CUDA C编译器都将创建该变量的一个副本，线程块中的每个线程都共享这块内存，但线程却无法看到也不能修改其他线程块的变量副本。这就实现了一个非常好的方式，使得一个线程块中的多个线程能够在计算上进行通信和协作。\n而且，共享内存缓冲区驻留在物理GPU上，而不是驻留在GPU之外的系统内存中。因此，在访问共享内存时的延迟要远远低于访问普通缓冲区的延迟，使得共享内存像每个线程块的高速缓存或者中间结果暂存器那样高效。\n如果想要实现线程之间通信，那么还需要一种机制来实现线程之间的同步。例如，如果线程A将一个值写入到共享内存，并且我们希望线程B对这个值进行一些操作，那么只有当线程A的写入操作完成之后，线程B才能开始执行它的操作。如果没有同步，那么将发生竞态条件(race condition)。\n下面将通过一个矢量的点积运算来详细介绍共享内存和同步。矢量点积运算为矢量相乘结束后将值相加起来以得到一个标量输出值。例如对两个包含4个元素的矢量进行点积运算: $$ (x_1, x_2, x_3, x_4) * (y_1, y_2, y_3, y_4) = x_1y_1 + x_2y_2 + x_3y_3 + x_4y_4 $$ 由于最终结果是所有乘积的总和，因此每个线程要保存它所计算的乘积的加和。下面代码实现了点积函数的第一个步骤:\n#include \u0026#34;../common/book.h\u0026#34; #define imin(a, b) (a \u0026lt; b ? a : b) const int N = 33 * 1024; const int threadsPerBlock = 256; __global__ void dot(float *a, float *b, float *c){ __shared__ float cache[threadsPerBlock]; int tid = threadIdx.x + blockIdx.x * blockDim.x; int cacheIndex = threadIdx.x; float temp = 0; while(tid \u0026lt; N){ temp += a[tid] * b[tid]; tid += blockDim.x * gridDim.x; } //设置cache中相应位置上的值 cache[cacheIndex] = temp; } 代码中声明了一个共享内存缓冲区，名字为cache。这个缓冲区将保存每个线程计算的加和值。我们将cache数组的大小声明为threadsPerBlock，这样线程块中每个线程都能将它计算的临时结果保存到某个位置上。之前在分配全局内存时，我们为每个执行核函数的线程都分配了足够的内存，即线程块的数量乘以threadsPerBlock。但对于共享变量来说，由于编译器将为每个线程块生成共享变量的一个副本，因此只需根据线程块中线程的数量来分配内存。\n我们需要将cache中所有的值相加起来。在执行这个运算时，需要通过一个线程来读取保存在cache中的值。由于race condition，我们需要使用下面的代码来确保对所有共享数组cache[]的写入操作在读组cache之前完成:\n//对线程块中的线程进行同步 __syncthreads(); 这个函数调用将确保线程块中的每个线程都执行完__syncthreads()前面的语句后，才会执行下一条语句。\n这时，我们可以将其中的值相加起来(称为归约Reduction)。代码的基本思想是每个线程将cache[]中的两个值相加起来，然后将结果保存回cache[]。由于每个线程都将两个值合并为一个值，那么在完成这个步骤后，得到的结果数量就是计算开始时数值数量的一半。下一个步骤中我们对这一半数值执行相同的操作，在将这种操作执行log2(threadsPerBlock)步骤后，就能得到cache[]中所有值的总和。对于这个示例来说，我们在每个线程块中使用了256个线程，因此需要8次迭代将cache[]中的256个值归约为1个值。这个归约过程的实现可以表示为以下代码:\n//对于归约运算来说，以下代码要求threadsPerBlock必须时2的指数 int i = blockDim.x / 2; while(i != 0){ if(cacheIndex \u0026lt; i){ cache[cacheIndex] += cache[cacheIndex + i]; } __syncthreads(); i /= 2; } 再结束了while()循环后，每个线程块都得到了一个值，这个值位于cache[]的第一个元素中，并且就等于该线程块中两两元素乘积的加和。然后，我们将这个值保存到全局内存并结束核函数:\nif(cacheIndex == 0){ c[blockIdx.x] = cache[0]; } 只让cacheIndex为0的线程执行保存操作时因为每个线程块只有一个值写入到全局内存，因此每个线程块只需要一个线程来执行这个操作。最后，由于每个线程块都只写入一个值到全局数据c[]中，因此可以通过blockIdx来索引这个值。\n点积运算的最后一个步骤就是计算c[]中所有元素的总和。像GPU这种大规模的并行机器在执行最后的归约步骤时，通常会浪费计算资源，因为此时的数据集往往会非常小。因此，我们可以将执行控制返回给主机，并且由CPU来完成最后一个加法步骤。\n下面给出了完整的代码实现:\n#include \u0026#34;../common/book.h\u0026#34; #define imin(a, b) (a \u0026lt; b? a : b) const int N = 33 * 1024; const int threadsPerBlock = 256; const int blocksPerGrid = imin(32, (N + threadsPerBlock - 1) / threadsPerBlock); __global__ void dot(float *a, float *b, float *c){ __shared__ float cache[threadsPerBlock]; int tid = threadIdx.x + blockIdx.x * blockDim.x; int cacheIndex = threadIdx.x; float temp = 0; while(tid \u0026lt; N){ temp += a[tid] * b[tid]; tid += blockDim.x * gridDim.x; } //设置cache中相应位置上的值 cache[cacheIndex] = temp; //对线程块中的线程进行同步 __syncthreads(); //对于归约来说，以下代码要求threadsPerBlock必须是2的指数 int i = blockDim.x / 2; while(i != 0){ if(cacheIndex \u0026lt; i){ cache[cacheIndex] += cache[cacheIndex + i]; } __syncthreads(); //循环中更新了变量cache，所以需要在下一次循环前进行同步。该同步语句需要所有的线程都必须运行才行。如果有线程不能运行这一处代码，会导致其他线程永远等待。 i /= 2; } if(cacheIndex == 0){ c[blockIndex.x] = cache[0]; } } int main(){ float *a, *b, c, *partial_c; float *dev_c, *dev_b, *dev_partial_c; //在CPU上分配内存 a = (float*) malloc(N*sizeof(float)); b = (float*) malloc(N*sizeof(float)); partial_c = (float*) malloc(blocksPerGrid * sizeof(float)); //在GPU上分配内存 HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_a, N * sizeof(float))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_b, N * sizeof(float))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_partial_c, blocksPerGrid * sizeof(float))); //填充主机内存 for(int i = 0; i \u0026lt; N; i++){ a[i] = i; b[i] = i * 2; } //将数组\u0026#34;a\u0026#34;和\u0026#34;b\u0026#34;复制到GPU HANDLE_ERROR(cudaMemcpy(dev_a, a, N * sizeof(float), cudaMemcpyHostToDevice)); HANDLE_ERROR(cudaMemcpy(dev_b, b, N * sizeof(float), cudaMemcpyHostToDevice)); dot\u0026lt;\u0026lt;\u0026lt;blocksPerGrid, threadsPerBlock\u0026gt;\u0026gt;\u0026gt;(dev_a, dev_b, dev_partial_c); //将数组\u0026#34;c\u0026#34;从GPU复制到CPU HANDLE_ERROR(cudaMemcpy(partial_c, dev_partial_c, blocksPerGrid * sizeof(float), cudaMemcpyDeviceToHost)); //在CPU上完成最终的求和运算 c = 0; for(int i = 0; i \u0026lt; blocksPerGrid; i++){ c += partial_c[i]; } #define sum_squares(x) (x * (x + 1) * (2 * x + 1) / 6) printf(\u0026#34;Does GPU value %.6g = %.6g? \\n\u0026#34;, c, 2 * sum_square((float) (N - 1))); //释放GPU上的内存 cudaFree(dev_a); cudaFree(dev_b); cudaFree(dev_partial_c); //释放CPU上的内存 free(a); free(b); free(partial_c); } ","date":"February 12, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E4%B8%89/","series":[],"smallImg":"","tags":[{"title":"CUDA","url":"/tags/cuda/"}],"timestamp":1707696000,"title":"CUDA学习(三)"},{"authors":[],"categories":[{"title":"Tutorial","url":"/categories/tutorial/"}],"content":"这一部分将介绍CUDA的并行编程方式\n矢量求和运算 假设有两组数据，我们需要将这两组数据中对应的元素两两相加，并将结果保存在第三个数组中。\n基于CPU的矢量求和 首先，下面的代码是通过传统的C代码来实现这个求和运算\n#include \u0026#34;../common/book.h\u0026#34; #define N 10 void add(int *a, int *b, int *c){ int tid = 0;\t//这是第0个CPU，因此索引从0开始 while(tid \u0026lt; N){ c[tid] = a[tid] + b[tid]; tid += 1;\t//由于只有一个CPU，因此每次递增1 } } int main(){ int a[N], b[N], c[N]; //在CPU上为数组\u0026#34;a\u0026#34;和\u0026#34;b\u0026#34;赋值 for(int i = 0; i \u0026lt; N; i++){ a[i] = -i; b[i] = i * i; } add(a, b, c); //显示结果 for(int i = 0; i \u0026lt; N; i++){ printf(\u0026#34;%d + %d = %d\\n\u0026#34;, a[i], b[i], c[i]); } return 0; } add()中使用while循环而不是for循环是为了代码能够在拥有多个CPU或者多个CPU核的系统上并行运行，比如双核处理器上可以将每次递增的大小改为2。\n//第一个CPU核 void add(int *a, int *b, int *c){ int tid = 0; while(tid \u0026lt; N){ c[tid] = a[tid] + b[tid]; tid += 2; } } //第二个CPU核 void add(int *a, int *b, int *c){ int tid = 1; while(tid \u0026lt; N){ c[tid] = a[tid] + b[tid]; tid += 2; } } 基于GPU的矢量求和 下面是基于GPU的矢量求和代码\n#include \u0026#34;../common/book.h\u0026#34; #define N 10 __global__ add(int *dev_a, int *dev_c, int *dev_c){ int tid = blockIdx.x; //计算该索引处的数据 if(tid \u0026lt; N){ c[tid] = a[tid] + b[tid]; } } int main(){ int a[N], b[N], c[N]; int *dev_a, *dev_b, *dev_c; //在GPU上分配内存 HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_a, N * sizeof(int))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_b, N * sizeof(int))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_c, N * sizeof(int))); //在CPU上为数组\u0026#34;a\u0026#34;和\u0026#34;b\u0026#34;赋值 for(int i = 0; i \u0026lt; N; i++){ a[i] = -i; b[i] = i * i; } //将数组\u0026#34;a\u0026#34;和\u0026#34;b\u0026#34;复制到GPU HANDLE_ERROR(cudaMemcpy(dev_a, a, N * sizeof(int), cudaMemcpyHostToDevice)); HANDLE_ERROR(cudaMemcpy(dev_b, b, N * sizeof(int), cudaMemcpyHostToDevice)); add\u0026lt;\u0026lt;\u0026lt;N, 1\u0026gt;\u0026gt;\u0026gt;(dev_a, dev_b, dev_c); //将数组\u0026#34;c\u0026#34;从GPU复制到CPU HANDLE_ERROR(cudaMemcpy(c, dev_c, N * sizeof(int), cudaMemcpyDeviceToHost)); //显示结果 for(int i = 0; i \u0026lt; N; i++){ printf(\u0026#34;%d + %d = %d\\n\u0026#34;, a[i], b[i], c[i]); } //释放在GPU上分配的内存 cudaFree(dev_a); cudaFree(dev_b); cudaFree(dev_c); return 0; } 在示例代码中，调用add函数的尖括号内的数值是\u0026laquo;\u0026lt;N, 1\u0026raquo;\u0026gt;，其中第一个参数表示设备在执行核函数时使用的并行线程块的数量。比如如果制定的事kernel\u0026laquo;\u0026lt;256, 1\u0026raquo;\u0026gt;()，那么将有256个线程块在GPU上运行。\n在add函数里面，我们可以使用blockIdx.x获取具体的线程块(blockIdx是一个内置变量，不需要定义它)，通过这种方式可以让不同的线程块并行执行数组的矢量相加。\n下一章将会详细解释线程块以及线程之间的通信机制和同步机制。\n","date":"February 8, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E4%BA%8C/","series":[],"smallImg":"","tags":[{"title":"CUDA","url":"/tags/cuda/"}],"timestamp":1707350400,"title":"CUDA学习(二)"},{"authors":[],"categories":[{"title":"Tutorial","url":"/categories/tutorial/"}],"content":"参考书目: GPU高性能编程CUDA实战\n书目网页链接: https://hpc.pku.edu.cn/docs/20170829223652566150.pdf\n该博客参考于上述书籍，虽然书有一点老，但是作为初学者而言仍然能学到很多东西。\n本书所包含的代码都在下面的连接中，可以下载来学习: https://developer.nvidia.com/cuda-example\nCUDA C简介 首先来看一个CUDA C的示例:\n#include \u0026#34;../common/book.h\u0026#34; int main(){ prinf(\u0026#34;Hello World!\\n\u0026#34;); return 0; } 这个示例只是为了说明，CUDA C与熟悉的标准C在很大程度上是没有区别的。\n核函数调用 在GPU设备上执行的函数通常称为核函数(Kernel)\n#include \u0026lt;iostream\u0026gt; __global__ void kernel(){ } int main(){ kernel\u0026lt;\u0026lt;\u0026lt;1, 1\u0026gt;\u0026gt;\u0026gt;(); printf(\u0026#34;Hello World!\\n\u0026#34;); return 0; } 跟之前的代码相比多了两处\n一个空的函数kernel()，并且带有修饰符__global__。 对这个空函数的调用，并且带有修饰字符\u0026laquo;\u0026lt;1, 1\u0026raquo;\u0026gt;。 这个__global__可以认为是告诉编译器，函数应该编译为在设备而不是在主机上运行。函数kernel()将被交给编译器设备代码的编译器，而main()函数将被交给主机编译器。\n传递参数 以下是对上述代码的进一步修改，可以实现将参数传递给核函数\n#include \u0026lt;iostream\u0026gt; #include \u0026#34;book.h\u0026#34; __global__ void add(int a, int b, int* c){ *c = a + b; printf(\u0026#34;c is %d\\n\u0026#34;, *c); } int main(void){ int c = 0; int* dev_c; printf(\u0026#34;original c is %d\\n\u0026#34;, c); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_c, sizeof(int))); add\u0026lt;\u0026lt;\u0026lt;1, 1\u0026gt;\u0026gt;\u0026gt;(2, 7, dev_c); HANDLE_ERROR(cudaMemcpy(\u0026amp;c, dev_c, sizeof(int), cudaMemcpyDeviceToHost)); cudaFree(dev_c); printf(\u0026#34;2 + 7 = %d\\n\u0026#34;, c); return 0; } 其中\u0026quot;book.h\u0026quot;包含了HANDLE_ERROR，也可以不使用\u0026quot;book.h\u0026quot;而是在代码中添加HANDLE_ERROR函数。\n#include \u0026lt;iostream\u0026gt; static void HandleError( cudaError_t err, const char *file, int line ) { if (err != cudaSuccess) { printf( \u0026#34;%s in %s at line %d\\n\u0026#34;, cudaGetErrorString( err ), file, line ); exit( EXIT_FAILURE ); } } #define HANDLE_ERROR( err ) (HandleError( err, __FILE__, __LINE__ )) __global__ void add(int a, int b, int* c){ *c = a + b; printf(\u0026#34;c is %d\\n\u0026#34;, *c); } int main(void){ int c = 0; int* dev_c; printf(\u0026#34;original c is %d\\n\u0026#34;, c); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_c, sizeof(int))); add\u0026lt;\u0026lt;\u0026lt;1, 1\u0026gt;\u0026gt;\u0026gt;(2, 7, dev_c); HANDLE_ERROR(cudaMemcpy(\u0026amp;c, dev_c, sizeof(int), cudaMemcpyDeviceToHost)); printf(\u0026#34;2 + 7 = %d\\n\u0026#34;, c); cudaFree(dev_c); return 0; } cudaMalloc()用来分配内存，这个函数的调用行为非常类似于标准C函数的malloc()，但该函数作用是告诉CUDA运行时在设备上分配内存。 第一个参数是一个指针，指向用于保存新分配内存地址的变量。第二个参数是分配内存的大小。 该函数返回的类型是void*。 不能使用标准C的free()函数来释放cudaMallocc()分配的内存。要释放cudaMalloc()分配的内存，需要调用cudaFree()。 HANDLE_ERROR()是定义的一个宏，作为辅助代码的一部分，用来判断函数调用是否返回了一个错误值，如果是的话，将输出相应的错误消息。 在主机代码中可以通过调用cudaMemcpy()来访问设备上的内存。 第一个参数是目标(target)指针，第二个参数是源(source)指针，第三个参数分配内存大小。第四个参数则是指定设备内存指针。 第四个参数一般有cudaMemcpyDeviceToHost，cudaMemcpyHostToDevice, cudaMemcpyDeviceToDevice三种。cudaMemcpyDeviceToHost说明我们将设备内存指针的数据传递给主机内存指针，此时第一个参数指针是在主机上，第二个参数指针是在设备上。cudaMemcpyHostToDevice说明我们将主机内存指针的数据传递给设备内存指针，此时第一个参数指针是在设备上，第二个参数指针是在主机上。此外还可以通过传递参数cudaMemcpyDeviceToDevice莱高速运行时这两个指针都在设备上。如果源指针和目标指针都在主机上，则可以直接调用memcpy()函数。 查询设备 我们可以使用cudaGetDeviceCount()来查询设备数量(比如GPU数量)。\nint count; HANDLE_ERROR(cudaGetDeviceCount(\u0026amp;count)); CUDA设备属性包含很多信息，可以在书上或者NVIDIA官方网站上查到。\n","date":"February 4, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E4%B8%80/","series":[],"smallImg":"","tags":[{"title":"CUDA","url":"/tags/cuda/"}],"timestamp":1707004800,"title":"CUDA学习(一)"},{"authors":[],"categories":[],"content":"","date":"January 1, 1","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/about/","series":[],"smallImg":"","tags":[],"timestamp":-62135596800,"title":"About"},{"authors":[],"categories":[],"content":"","date":"January 1, 1","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/contact/","series":[],"smallImg":"","tags":[],"timestamp":-62135596800,"title":"Contact Us"},{"authors":[],"categories":[],"content":"","date":"January 1, 1","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/offline/","series":[],"smallImg":"","tags":[],"timestamp":-62135596800,"title":"Offline"}]
