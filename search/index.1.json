[{"authors":[],"categories":[{"title":"Draft","url":"/categories/draft/"}],"content":"本文章主要记录使用博客期间遇到的有关hugo bootstrap skeleton主题的一些问题以及解决方法。\n图片设置 md文件的图片引用默认路径以及md文件的front matter的“image []”信息的默认路径 是在项目root路径的static路径里面。为了方便，一些网站需要的图片可以都放到static路径下面。\n","date":"May 13, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/posts/hugo_theme_usage_notes/hugo_bootstrap_skeleton/","series":[],"smallImg":"","tags":[{"title":"Hugo Bootstrap Skeleton","url":"/tags/hugo-bootstrap-skeleton/"}],"timestamp":1715587244,"title":"Hugo Bootstrap Skeleton Notes"},{"authors":[],"categories":[{"title":"Note","url":"/categories/note/"}],"content":"本章将介绍vscode的C/C++插件的具体使用方法。\nvscode的C/C++插件是一个强大的工具，它提供了代码的IntelliSense (智能识别)、running、debugging等功能。不过，每次创建一个新的项目的时候，有时候需要重新配置相关功能的文件，下面将介绍如何配置这些功能的配置文件。\n一般来说，这些配置文件会在C/C++扩展安装好后配置完成。可以在vscode的settings选项中选择C/C++相关的设置进行更改。此外，也可以在当前工作路径下创建.vscode路径，然后在路径里面添加新的配置来覆盖默认的配置信息。\nIntelliSense 在工作路径下，可以使用⇧⌘P(macos) 或者ctrl shift p来打开命令选项，输入C/C++: Edit Configurations (UI)，即可在.vscode路径下创建配置智能识别的配置文件c_cpp_properties.json。有时候找不到该指令，则需要手动创建。\nc_cpp_properties.json提供了必要的设置来配置 IntelliSense，这包括告诉 IntelliSense 哪些目录包含了项目的头文件（通过 includePath 设置）、使用的编译器路径（compilerPath）、预处理器定义（defines）、C/C++ 标准版本（如 cStandard 和 cppStandard）、以及 IntelliSense 模式（intelliSenseMode）等。\nDebug 使用⇧⌘P(macos) 或者ctrl shift p来打开命令选项，输入C/C++ Add Debug Configuration，即可在.vscode路径下创建配置文件launch.json。launch.json 用于配置调试器，debug程序 。\nTask 使用⇧⌘P(macos) 或者ctrl shift p来打开命令选项，输入Tasks: Configure Task，即可在.vscode路径下创建配置文件tasks.json。tasks.json文件用于配置和管理构建任务，如编译代码(添加需要的compiler和compiler flag等)，运行脚本，打包程序等。\n键盘快捷方式 如果在命令选项中找找不到相关命令，可以在界面左下角的Manage中打开Keyboard Shortcuts(键盘快捷方式)，然后在里面输入对应的命令，之后设置快捷键以方便之后的调用。\n","date":"May 9, 2024","img":"https://KasterMist.com/images/vscode.jpeg","lang":"en","langName":"English","largeImg":"","permalink":"/posts/vscode/1/","series":[],"smallImg":"","tags":[{"title":"VsCode","url":"/tags/vscode/"}],"timestamp":1715239064,"title":"Vscode 插件: C/C++"},{"authors":[],"categories":[{"title":"Note","url":"/categories/note/"}],"content":"本章节将介绍vim的常用语法。\n在命令行中可以输入vimtutor来进行vim学习。\nvim学习参考视频链接: https://www.bilibili.com/video/BV1PL411M7bg/?spm_id_from=333.788\u0026vd_source=302ea1a1118a80c10a5b35e58bd9c8bf\n创建或编辑一个文件: vim filename (如果filename不存在，则创建该文件)\nvim的三种模式 普通模式 (Normal mode) / 命令模式 (Command mode): 默认模式，用于导航、删除、复制等操作。 插入模式 (Insert mode): 用于输入文本。 视图模式 (Visual Mode): 用于文本搜索、筛选。 在普通模式下按下i即可进入插入模式，按下v即可进入视图模式。在插入模式或者视图模式下按下esc即可退出该模式并进入普通模式。\n模式转换的快捷键使用\n普通模式\u0026ndash;\u0026gt;插入模式\n在下方插入一行: o 在上方插入一行: O(大写的o) 在当前光标后插入: a 在当前光标前插入: i 在行尾插入: A 在行首插入: I 特殊模式 replace mode: 输入R即可进入replace模式，此时输入的字符会替换当前字符(即先删除当前字符再输入新的字符) 光标移动 基础部分\n上下左右: 左: h 下: j 上: k 右: l 移动到第一行: gg 移动到做后一行/指定行: G 行数+大写G跳到指定行 进阶部分\n移动到下一个单词的开头: w 移动到下一个单词的结尾: e 移动到上一个单词的开头: b 移动行首: 0 (数字零) 移动到第一个非空字符: ^ 移动到行尾: $ 移动到匹配的括号处: % 移动到变量定义处: gd 移动到前一个没有匹配的左大括号处: [{ 移动到下一个没有匹配的右大括号处: ]} 修改大小写(如果是大写，则修改为小写。如果是小写，则修改为大写): ~ 保存与退出 保存: :w 退出: :q 保存+退出: :wq 强制退出(不保存): :q! 在vim里面运行命令行: :!后带命令，可以将:后的!理解为允许vim执行外部命令。 在visual mode中选取多行后，可以通过:w filename将选取的行保存到filename当中。 提取、合并文件 在当前位置插入另外文件的内容: :r filename :r也可以读取外部命令的输出，比如可以通过:r!ls来将ls的输出放置到当前光标下面 复制粘贴, 替换修改 复制: yy 粘贴: p 使用dd删除的行使用p也可以进行粘贴 将当前光标下的字符替换为想要的字符: r后加上一个字符 字符c的功能: 更改字符 使用方式: c [number] motion，motion则是之前常用的动作参数。使用后会自动转换到插入模式。 从光标处删除两个单词并自动转换到插入模式进行修改: c2w 从光标处删除到行尾柄并自动转换到插入模式进行修改:c$ 该变文本直到一个单词的末尾并自动转换到插入模式进行修改: ce 定位 输入Ctrl-G即可得知当前光标所在行位置以及文件信息 光标跳转到最后一行: G 光标跳转到第一行: gg 跳转到指定行: number G 撤销 撤销: u 撤销一整行的修改: U 重写: Ctrl-R 删除 删除整行: dd 删除当前字符: x 删除到行尾: D 在visual mode中选取文本内容后可以通过输入d删除选中的文本内容。 组合快捷键 删除两个单词: d2w 删除单词，执行两次: 2dw 删除两个单词，执行两次: 2d2w 在视图模式下选中后5行删除: d5j 搜索替换 在当前光标下搜索下一个匹配的信息: / + 匹配的信息 在当前光标下搜索上一个匹配的信息: ? + 匹配的信息 搜索之后跳转到下一个匹配的信息: n 搜索之后跳转到上一个匹配的信息: N 快速搜索当前光标的单词: 向后 * 向前 #，之后也可以使用n和N来改变方向 将range范围内的from替换为to： :[range]s/from/to/[flags] 在要查找的内容后面加上“\\c”（不区分大小写）或“\\C”（区分大小写），比如/+匹配的信息后面加上\\c或\\C或者在:[range]s/from/to/[flags]的from后加上\\c或\\C range列表\nRange Description Example 21 line 21 :21s/old/new/g 1 first line :1s/old/new/g $ last line :$s/old/new/g % all lines, same as 1,$ :%s/old/new/g 21,25 lines 21 to 25 :21,25s/old/new/g 21,$ lines 21 to end :21,$s/old/new/g .,$ current line to end :.,$s/old/new/g .+1,$ line after current line to end :.+1,$s/old/new/g .,.+5 six lines (current to current +5 inclusive) :.,.+5s/old/new/g .,.5 same (.5 is intepreted as .+5) :.,.5s/old/new/g 有些特殊符号需要在前面加上\\才能识别。\n需要注意的是，如果同一行有多个能匹配到的位置，替换的话只会替换第一个匹配的信息。添加flag: g可以实现每一行中所有匹配的替换(比如上面range列表中的最后的/g)。\nflag list\nflag 作用 \u0026amp; 复用上次替换命令的flags g 替换每行的所有匹配值(默认没有g的情况下只会替换每行的第一个匹配值) c 替换前需确认 e 替换失败时不报错 i 大小写不敏感 I 大小写敏感 此外，使用sed可以直接将某个文件里面的某个信息替换为另一个信息: sed -i \u0026quot;[range]s/from/to/[flags]\u0026quot; filename就是将filename文件中的from替换为to。-i表示在文件内更改。否则更改结果只会在终端中打印出来。\n分窗口 生成水平的窗口: :sp 生成垂直窗口: :vsp 移动到另一个窗口操作: Ctrl-W + [hjkl] 滚动窗口 Ctrl+E - 向下滚动窗口一行，不移动光标。 Ctrl+Y - 向上滚动窗口一行，不移动光标。 Ctrl+D - 向下滚动半个屏幕。 Ctrl+U - 向上滚动半个屏幕。 Ctrl+F - 向下滚动一个整屏幕。 Ctrl+B - 向上滚动一个整屏幕。 zz - 将当前行移至窗口中央，光标位置不变。 zt - 将当前行移至窗口顶部，光标位置不变。 zb - 将当前行移至窗口底部，光标位置不变。 生成标签 (便于跳转) 生成的标签可以是小写字母a-z或者大写字母a-z，也可以是数字0-9。小写字母的标记，仅用于当前缓冲区；而大写字母的标记和数字0-9的标记，则可以跨越不同的缓冲区。小写字母的标签可以被delmarks!删除，大写字母和0-9不行。大写字母和0-9只能通过delmarks character来进行删除\n生成一个标签a: ma 跳转到标签a所在位置: ``a` 跳转到标签a所在的行首: 'a 查找所有的标签: :marks 删除标签a: :delmarks a 删除a-z的标签: :delmarks a-z 删除A-Z的标签: :delmarks A-Z 删除所有标签(不包括大写的标签): :delmarks! 注释代码 可以使用visual block模式来注释多行代码\nvisual block: Ctrl V 在visual block模式下通过[hjkl]选中多行后，使用I来进行插入，例如输入//然后Esc即可实现多行注释。 使用注释插件\nhttps://github.com/tpope/vim-commentary\nmkdir -p ~/.vim/pack/tpope/start cd ~/.vim/pack/tpope/start git clone https://tpope.io/vim/commentary.git vim -u NONE -c \u0026#34;helptags commentary/doc\u0026#34; -c q Use gcc to comment out a line (takes a count), gc to comment out the target of a motion.\n代码补全 在vim中自带了基础的自动补全功能。但该功能的局限之处在于，只能补全之前已经出现过的单词。当写好了单词一部分后，输入Ctrl-N，自动补全功能会出现提供匹配列表、完成补全、匹配失败等三种不同的情况。\n代码跳转 可以下载ctags来跳转到某对象的定义位置。\n在代码所在路径下输入ctags -R .可以创建代码关联的文件tag。\n默认情况下在一个代码文件里面使用关联只能在当前路径下寻找关联，在～/.vimrc里面添加set tags=./tags;,tags可以寻找tag文件路径下所有的位置是否有关联。\nvim打开文件后，在对应的声明的地方按Ctrl-]就可以自动跳转到对象的定义的文件的对应位置。\n直接查找某个对象(比如class_name)的定义的文件以及对应位置: vim -t class_name\nvim相关的插件 插件网站: https://vimawesome.com/\nvim-plug插件管理工具\ngithub链接: https://github.com/junegunn/vim-plug?tab=readme-ov-file\n安装教程: https://github.com/junegunn/vim-plug/wiki/tutorial\nvim-plug是一个基于Rust编写的vim插件管理工具，可以轻松下载需要的vim有关的插件。\n安装方式 (Unix):\ncurl -fLo ~/.vim/autoload/plug.vim --create-dirs \\ https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim 之后在打开~/.vimrc文件添加下面的信息：\n\u0026#34; Plugins will be downloaded under the specified directory. call plug#begin(has(\u0026#39;nvim\u0026#39;) ? stdpath(\u0026#39;data\u0026#39;) . \u0026#39;/plugged\u0026#39; : \u0026#39;~/.vim/plugged\u0026#39;) \u0026#34; Declare the list of plugins. Plug \u0026#39;tpope/vim-sensible\u0026#39; Plug \u0026#39;junegunn/seoul256.vim\u0026#39; \u0026#34; List ends here. Plugins become visible to Vim after this call. call plug#end() 之后重启vim就可以使用Plugin插件。在vim打开文件内输入:Pluginstall即可下载~/.vimrc中声明的插件。输入:PluginUpdate可以更新~/.vimrc中新添加的插件。输入:PlugClean可以清楚~/.vimrc中被删除的插件。\nfzf.vim https://github.com/junegunn/fzf.vim\n该插件包含了fzf的很多功能，并且移植到了vim中。可以使用诸如:Ag等功能。详情可以查看上面的源码链接。\nNERDTree NERDTree可以在vim打开文件后在左边栏显示当前路径下的文件。\nvim打开文件后输入:NERDTree即可在左边栏显示当前路径下的文件信息。左边栏可以选择目录中不同文件，按ENTER即可显示选中的文件信息。 光标左右界面跳转: Ctrl-WW 在对应的vim界面命令行模式下输入退出命令，q!或wq，即可退出对应的界面。 在~/.vimrc中添加autocmd VimEnter * NERDTree即可在vim打开文件后自动开启NERDTree插件。 EasyComplete https://zhuanlan.zhihu.com/p/366496399\n超轻量级的vim代码补全工具，如果想要更全面的补全功能，可以尝试coc\nvim-colors-solarized https://vimawesome.com/plugin/vim-colors-solarized-ours\nvim 文本编辑器的精确配色方案\n","date":"April 13, 2024","img":"https://KasterMist.com/images/vim.png","lang":"en","langName":"English","largeImg":"","permalink":"/posts/linux/vim/","series":[],"smallImg":"","tags":[{"title":"Linux","url":"/tags/linux/"},{"title":"Linux Command","url":"/tags/linux-command/"}],"timestamp":1712966400,"title":"Vim"},{"authors":[],"categories":[{"title":"Note","url":"/categories/note/"}],"content":"基础命令 查询文件和子目录 ls 查询文件和子目录的最简单的命令是ls。它可以列出当前目录的文件和子目录。常用的指令有:\nls /path/to/directory 列出指定目录中的文件和子目录 ls -a列出隐藏文件和子目录 ls -l以详细格式列出文件和子目录，包含读写权限、创建时间等信息 查询当前工作目录的绝对路径 pwd 查询当前工作目录的绝对路径是pwd\n改变当前工作目录 cd 使用cd可更改当前工作目录，可以在当前目录下使用cd subpath进入子目录，也可以使用cd absolutePath进入绝对路径目录下。\n使用cd ..可移动到上一级目录。\n创建目录 mkdir mkdir 命令用于创建一个新的目录。\nmkdir directory在当前目录下创建一个directory的子目录 mkdir -p /path/to/new/directory如果要递归创建目录的话，需要加上-pflag 删除目录 rmdir rmdir只能用于删除空的目录，如果是非空目录，则需要时用rm\nrmdir -v /path/subpath会删除path里面的subpath子目录，同时显示详细的输出。 rmdir -p /path/subpath会首先删除path里面的subpath子目录，之后会尝试删除path目录。 创建空文件 touch touch filename创建一个名叫filename的空文件。 touch -c filename如果文件已经存在，则不创建该文件。这样可以避免意外覆盖现有文件。 复制文件和目录 cp 语法: cp [option] source destination\ncp old_path/old_file.txt new_path/new_file.txt将old_path的old_file.txt以new_file.txt的名字复制到new_path中。如果不更改名字的话，destination可以只写new_path cp -r old_path/old_sub_path new_path/new_subpath递归复制整个目录 移动或者重命名文件和目录 mv 语法: mv [options] source destination\nmv old_path/old_file.txt new_path/new_file.txt将old_path的old_file.txt以new_file.txt的名字移动到new_path中。 mv -i old_path/old_file.txt new_path/new_file.txt在覆盖目标位置的任何现有文件前提示。这样可以防止意外覆盖数据。 mv old_file.txt new_file.txt可以通过mv在同一路径下重命名old_file.txt为new_file.txt。 移除文件和目录 rm rm在使用时需要格外小心，因为恢复使用rm删除的文件和目录会非常困难。\nrm -r name递归删除目录，包括目录里面的所有内容。\nrm -f name强制删除并抑制所有提示。\nrm -i name在删除每个文件或目录前提示确认，以防意外删除。\n比如使用rm -rf path可以强制删除path路径和path里面的所有内容。\n在目录层次结构中搜索文件 find 语法: find [path] [critical]\n示例\nfind . -name example.txt: 查找当前目录及其子目录中所有名为 example.txt 的文件 find /home -type f: 查找 /home 目录中所有的普通文件 find / -type f -size +1M: 查找文件大小大于 1MB 的文件 find / -name name: 在/路径下查找名字为name的文件 使用条件匹配搜索文本 grep 语法: grep [options] pattern [files]\n示例\ngrep \u0026quot;a\u0026quot; example.txt搜索example.txt中\u0026quot;a\u0026quot;这个单词。 grep -c \u0026quot;a\u0026quot; example.txt搜索example.txt中\u0026quot;a\u0026quot;这个单词出现的次数。 比较文件 diff 语法: diff [options] file1 file2\n用于比较两个文件的差异\ndiff original.txt updated.txt比较 original.txt和updated.txt两个文件的差异，会输出产生差异的不同行。\n字数统计 wc wc -l example.txt: 只打印行计数\nwc -w example.txt: 只打印字数\nwc -c example.txt: 只打印字节数\n历史命令 history 使用history可以查看之前输入过的命令语句。每个命令前面都有一个编号。你可以使用 ! 加上命令编号来执行历史记录中的命令，比如!123来执行第123号的命令。还可以使用 !! 来执行最后一个命令，或者使用 !string 来执行最近包含字符串 string 的命令。\n使用 history -c 命令来清除命令历史记录。\n文件权限命令 更改文件模式或访问权限 chmod 文件的权限包括：只读(r)，写入(w)，执行(x)。\n模式：指定要修改的权限模式。权限模式可以使用数字表示，也可以使用符号表示。\n数字表示：使用三位数字（0-7）表示权限。每一位数字代表一个权限位，分别对应于读（4）、写（2）和执行（1）权限。例如，755 表示所有者具有读、写和执行权限，组和其他用户具有读和执行权限。 符号表示：使用符号来表示权限。符号表示包括以下几个部分： 操作符：可以是 +（添加权限）、-（删除权限）或 =（设置权限）。 权限范围：可以是 u（所有者）、g（组）、o（其他用户）或 a（所有用户）。 权限类型：可以是 r（读取权限）、w（写入权限）或 x（执行权限）。 例如，u+x 表示为所有者添加执行权限，go-w 表示删除组和其他用户的写入权限。\n示例:\nchmod 644 example.txt: 将文件 example.txt 的权限设置为所有者可读写、组和其他用户只读。 chmod -R 777 documents: 将目录 documents 及其子目录中所有文件的权限设置为所有者可读写执行，组和其他用户可读写执行 (-R用于递归地修改目录及其子目录中的文件权限)。 chmod +x script.sh: 为 script.sh 添加执行权限。 管理命令 查看当前进程信息 ps 用于显示当前正在运行的进程信息。它可以显示当前用户的进程、所有用户的进程或者系统的所有进程。\n显示linux进程 top 动态显示系统的进程信息和资源使用情况。它会实时更新显示当前正在运行的进程列表，并且会以交互式的方式展示系统的 CPU 使用情况、内存使用情况等\n交互式进程浏览 htop htop 命令是 top 命令的改进版本，提供了更加友好和直观的界面，并且支持更多的交互操作。它可以显示更多的进程信息，并且可以通过键盘快捷键进行排序、过滤、查找等操作。\n向进程发送终结信号 kill kill PID: 通过输入PID(进程ID)或程序的二进制名称来终结进程。\nkill -9 name: 通过输入进程名称来终结进程，需要添加-9选项。\n示例: 查找一个进程并终结\nps aux | grep example_process: 使用 ps 命令查找名为 example_process 的进程，述命令会显示包含 example_process 关键词的进程信息，并输出其 PID。 kill PID: 在获取到PID后，即可通过kill PID的方式来终结进程。 报告虚拟内存统计数据 vmstat 打印有关内存、交换、I/O 和 CPU 活动的详细报告。其中包括已用/可用内存、交换入/出、磁盘块读/写和 CPU 进程/闲置时间等指标。\nvmstat -n 5: 每隔5秒更新一次信息。 vmstat -a: 显示活动和非活动内存。 vmstat -s: 显示事件计数器和内存统计信息。 vmstat -S: 以 KB 而不是块为单位输出。 报告CPU和I/O统计数据 iostat 监控并显示 CPU 利用率和磁盘 I/O 指标。其中包括 CPU 负载、IOPS、读/写吞吐量等。\niostat -c: 显示CPU使用率信息。 iostat -t: 为每份报告打印时间戳。 iostat -x: 显示服务时间和等待计数等扩展统计信息。 iostat -d: 显示每个磁盘/分区的详细统计信息，而不是合计总数。 iostat -p: 显示特定磁盘设备的统计信息。 显示可用和已用内存量 free free -b: 以字节为单位显示输出。 free -k: 以KB为单位显示输出结果。 free -m: 以MB为单位显示输出，而不是以字节为单位。 free -h: 以GB、MB等人类可读格式打印统计数据，而不是字节。 有用的Unix插件 fzf 参考资料:https://zhuanlan.zhihu.com/p/41859976\ngithub源码: https://github.com/junegunn/fzf\nfzf是一种非常好用的下拉查找工具，通常需要与其他的命令组合。下面是一些常用的功能:\n单独使用fzf命令会展示当前目录下所有文件列表，可以用键盘上下键或者鼠标点出来选择。 使用vim组合fzf来查找并打开当前目录下的文件: vim $(fzf) 切换当前工作目录: cd $(find * -type d | fzf)，其实现逻辑如下: 使用find命令找出所有的子目录 把子目录列表pipe到fzf上进行选择 再把结果以子命令的形式传给cd 可以将cmd | fzf理解为将列出的结果以fzf下拉查找工具的方式来实现，比如ls | fzf就是会通过下拉查找的方式查看当前路径下(不包括子路径)的文件和文件夹。$(fzf)意味着通过fzf选取的信息将输入到变量当中，比如vim $(fzf)就是通过fzf选取文件后再用vim打开。 切换git分支: git checkout $(git branch -r | fzf) 使用fzf插件补全shell命令 fzf自带一种插件可以通过输入**来自动生成下拉框窗口来补全信息。比如使用cd **然后tab可以使用下拉菜单选择路径。可以在https://github.com/junegunn/fzf搜索“Fuzzy completion for bash and zsh”找到有关的信息。\n如果用的是homebrew，则先通过brew install fzf，然后执行$(brew --prefix)/opt/fzf/install。之后在~/.zshrc的plugins=(...)中添加fzf。之后执行source ~/.zshrc，即可使用。\n我们也可以对FZF_DEFAULT_OPTS进行设置，来自定义fzf的界面，比如FZF_DEFAULT_OPTS=\u0026quot;--height 40% --layout=reverse --preview '(highlight -O ansi {} || cat {}) 2\u0026gt; /dev/null | head -500'\u0026quot;\nag 一个类似于 grep 的代码搜索工具，比grep有更高的性能。ag 在搜索时会自动忽略 .gitignore 中的文件和目录，从而提高搜索效率。它支持正则表达式，高亮显示匹配结果，并且可以直接在你的编辑器中使用。\n在当前目录以及子目录搜索文本: ag \u0026quot;search pattern\u0026quot; 在特定文件类型中搜索: ag \u0026quot;search pattern\u0026quot; --cpp只在HTML文件中搜索 忽略特定文件或目录: ag \u0026quot;search pattern\u0026quot; --ignore dir/*忽略特定目录下的搜索 ag \u0026quot;search pattern\u0026quot; --ignore *.log 忽略所有.log文件 搜索特定目录: ag \u0026quot;search pattern\u0026quot; /path/to/directory 仅显示文件名: ag \u0026quot;search pattern\u0026quot; -l 与grep一样，可以配合其他语句执行，比如cat \u0026quot;filename\u0026quot; | ag \u0026quot;search pattern\u0026quot; 引号可加可不加 ","date":"April 4, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/posts/linux/linux-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","series":[],"smallImg":"","tags":[{"title":"Linux","url":"/tags/linux/"},{"title":"Linux Command","url":"/tags/linux-command/"}],"timestamp":1712188800,"title":"Linux 常用命令"},{"authors":[],"categories":[{"title":"Tutorial","url":"/categories/tutorial/"}],"content":"前面几章主要是对参考书目的内容进行一个概括。本章将根据参考书目的内容对所学到的所有函数进行一个整理和总结，以便复习和参考。英伟达的官方网站包含了所有的CUDA函数，可参考https://developer.download.nvidia.cn/compute/DevZone/docs/html/C/doc/html/index.html\n基本语法 这部分将介绍一下CUDA的基本语法，即参数的创建、传递、释放。\ncudaMalloc cudaMalloc函数的语法结构如下表示：\ncudaError_t cudaMalloc(void** devPtr, size_t size) 与C的malloc相似，cudaMalloc在设备上分配“size”字节大小的线性内存，并在*devPtr中返回一个指向所分配内存的指针。所分配的内存对于任何类型的变量都是适当对齐的。如果分配成功，则返回cudaSuccess，如果分配失败，则会返回cudaErrorMemoryAllocation。\n注意，void** devPtr表示需要传递指针的地址。\ncudaMemcpy cudaMemcpy函数的语法如下所示：\ncudaError_t cudaMemcpy(void* dst, const void* src, size_t, count, enum cudaMemcpyKind kind) 将“count”字节从src指向的内存区域复制到dst指向的内存区。\ndst是目标位置destination，src是源位置source。\nkind用于规定复制的方向，总共有以下几种：\ncudaMemcpyHostToHost cudaMemcpyHostToDevice cudaMemcpyDeviceToHost cudaMemcpyDeviceToDevice 返回的值有：\ncudaSuccess cudaErrorInvalidValue cudaErrorInvalidDevicePointer cudaErrorInvalidMemcpyDirection 注：此函数还可能返回以前异步启动的错误代码。\ncudaFree cudaFree函数的语法如下所示\ncudaError_t cudaFree(void* devPtr) 释放devPtr指向的内存。与cudaMalloc()或者cudaMallocPitch()一一对应，对之前分配的内存进行释放。\n返回的值有：\ncudaSuccess cudaErrorInvalidDevicePointer cudaErrorInitializationError 注：此函数还可能返回以前异步启动的错误代码。\n核函数 核函数前面需要加上__global__，此方法会将函数标记为设备代码“Device Code”。函数格式可以为：\n__global__ kernel(arguments) 进行函数调用的格式可以为：\nkernel\u0026lt;\u0026lt;\u0026lt;blocksPerGrid, threadsPerBlock\u0026gt;\u0026gt;\u0026gt;(arguments) 下面将详细介绍threads，blocks相关概念。也可以参考以下链接：https://zhuanlan.zhihu.com/p/675603584\nCUDA里面用Grid和Block作为线程组织的组织单位，一个Grid可包含了N个Block，一个Block包含N个thread。\n我们一般用得到的参数是gridDim, blockDim, blockIdx和threadIdx。\ngridDim: dim3类型，表示blocks在grid里面数量的维度。 blockDim：dim3类型，表示threads在block里面数量的维度。 blockIdx：dim3类型，表示blocks在grid里面的索引。 threadIdx：dim3类型，表示threads在block里面的索引。 上图给了一个参考的thread和block的结构。在上面的结构中，一个grid有2*3个block块，每个block块各有15*15个thread。x轴是从左往右，y轴是从上到下(示例图片并没有显示z轴，即z轴最大值为1)。\n如何获取某个thread的坐标，可以分为以下几步：\n计算thread在block中的位置 $$ threadInBlock = threadIdx.x + threadIdx.y * blockDim.x + threadIdx.z * blockDim.x * blockDim.y $$\n计算该block在grid中的位置 $$ blockInGrid = blockIdx.x + blockIdx.y * gridDim.x + blockIdx.z * gridDim.x * gridDim.y $$\n计算每个block的线程，计算得出某个thread的位置索引 $$ oneBlockSize = blockDim.x * blockDim.y * blockDim.z $$\n$$ idx = threadInBlock + oneBlockSize * blockInGrid $$\n查询设备 cudaGetDevice cudaError_t cudaGetDevice(int* device) 将当前主机线程调用的设备索引返回给device的地址。\ncudaGetDeviceCount cudaError_t cudaGetDeviceCount(int* count) 返回可执行的设备数量到count的地址。如果没有这样的设备，则返回cudaErrorNoDevice。如果无法加载驱动程序来确定是否存在任何此类设备，则返回cudaErrorInsufficientDriver。\ncudaGetDeviceProperties cudaError_t cudaGetDeviceProperties(struct cudaDeviceProp* prop, int device) 将device的属性的信息传递给prop。cudaDeviceProp结构可以参考下面的链接：\nhttps://developer.download.nvidia.cn/compute/DevZone/docs/html/C/doc/html/group__CUDART__DEVICE_g5aa4f47938af8276f08074d09b7d520c.html#g5aa4f47938af8276f08074d09b7d520c\n内存 共享内存(Shared Memory) 共享内存是在GPU的每个线程块（block）中共享的内存空间，用于线程之间的通信和数据共享。 共享内存的访问速度比全局内存更快，因为它位于芯片上，与处理器更近。 共享内存的使用需要程序员显式地将数据从全局内存复制到共享内存中，并在使用完毕后将数据写回全局内存。 共享内存在内核中的声明是在内核函数的参数列表之外使用 __shared__ 关键字。 全局内存(Global Memory) 全局内存是GPU中所有线程都可以访问的主要内存池，在设备内存中分配。 全局内存的访问速度相对较慢，因为它位于芯片之外，需要通过总线等方式与GPU核心通信。 全局内存通常用于存储大规模的数据，如数组、结构体等。 全局内存可以通过 cudaMalloc 分配内存，并使用 cudaMemcpy 在主机内存和设备内存之间进行数据传输。 常量内存(Constant Memory) 常量内存是GPU上的一种只读内存，用于存储在GPU核心中被所有线程共享的常量数据。\n常量内存通常用于存储对所有线程都是常量的数据，比如常量数组、常量参数等。\n常量内存的优势在于其高速缓存和对齐的特性，可以加速访问频繁的常量数据。\n常量内存的访问速度比全局内存更快，但相对来说容量较小。\n常量内存通常在内核启动之前被初始化，并且其内容在内核执行期间不会改变。可以使用 CUDA 的 __constant__ 修饰符定义常量内存，使用 cudaMemcpyToSymbol 将数据从主机内存拷贝到常量内存中。\n以下是GPT生成的一个常量内存的使用例子：\n#include \u0026lt;cuda_runtime.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; // 定义常量数组大小 #define ARRAY_SIZE 10 // 声明常量内存 __constant__ int constantArray[ARRAY_SIZE]; // CUDA内核函数 __global__ void kernel(int *result) { // 获取线程索引 int idx = threadIdx.x; // 使用常量内存中的数据进行计算 result[idx] = constantArray[idx] * idx; } int main() { int hostArray[ARRAY_SIZE]; int *devResult; // 初始化常量数组 for (int i = 0; i \u0026lt; ARRAY_SIZE; ++i) { hostArray[i] = i + 1; } // 分配设备端内存 cudaMalloc((void**)\u0026amp;devResult, ARRAY_SIZE * sizeof(int)); // 将常量数组拷贝到设备端常量内存中 cudaMemcpyToSymbol(constantArray, hostArray, ARRAY_SIZE * sizeof(int)); // 调用内核函数 kernel\u0026lt;\u0026lt;\u0026lt;1, ARRAY_SIZE\u0026gt;\u0026gt;\u0026gt;(devResult); // 同步CUDA流，确保内核执行完成 cudaDeviceSynchronize(); // 将结果拷贝回主机端 int result[ARRAY_SIZE]; cudaMemcpy(result, devResult, ARRAY_SIZE * sizeof(int), cudaMemcpyDeviceToHost); // 打印结果 printf(\u0026#34;Result:\\n\u0026#34;); for (int i = 0; i \u0026lt; ARRAY_SIZE; ++i) { printf(\u0026#34;%d \u0026#34;, result[i]); } printf(\u0026#34;\\n\u0026#34;); // 释放设备端内存 cudaFree(devResult); return 0; } 操作 原子操作 atomicAdd(addr, val)\n读取地址addr处的值，将y增加到这个值，以及将结果保存回地址addr。\n原子锁 int *mutex; void lock(void){ if(*mutex == 0){ *mutex = 1; //将1保存到锁 } } 当mutex为0时，将数值1保存到锁，后续访问的时候如果mutex为1则无法进行后续任务。然而，如果在线程读取到0并且还没有修改这个值之前，另一个线程将1写入到互斥体，则两个线程都会执行后面的操作。要实现正确的操作，整个运算都要以原子方式来进行。\n下面是lock的实现\nstruct Lock{ int *mutex; Lock(void){ int state = 0; HANDLE_ERROR(cudaMalloc((void**)\u0026amp; mutex, sizeof(int))); HANDLE_ERROR(cudaMemcpy(mutex, \u0026amp;state, sizeof(int), cudaMemcpyHostToDevice)); } ~Lock(void){ cudaFree(mutex); } __device__ void lock(void){ while(atomicCAS(mutex, 0, 1) != 0); } __device__ void unlock(void){ atomicExch(mutex, 0); } } atomicCas()是一个原子操作。调用atomicCAS()将返回位于mutex地址上的值。因此while循环会不断运行，直到atomicCAS发现mutex的值为0。当发现为0时，比较操作成功，线程将把1写入到mutex。\n通过atomicExch(mutex, 0)来重制mutex的值，将其与第二个参数进行交换，并返回它读到的值。\n事件 线程同步 \\_\\_syncthreads __syncthreads()函数的功能是确保同一个线程块的线程执行完该语句之前的所有语句。使用__syncthreads()需要注意的点是要确保所有线程都能够执行该语句，否则其他线程就会永远等待那些执行不了该语句的线程，从而停止下一步的执行。\n注：CUDA并没有提供一个可以同步所有线程（包括不同线程块）的函数。这是因为CUDA的并行模型设计使得在不同线程块之间进行同步更加困难和昂贵。通常情况下，CUDA编程模型假设各个线程块是独立执行的，并且不会直接相互影响。\n","date":"March 11, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E5%8D%81-%E4%B9%A6%E7%B1%8D%E4%B8%AD%E7%9A%84%E5%87%BD%E6%95%B0%E6%95%B4%E5%90%88/","series":[],"smallImg":"","tags":[{"title":"CUDA","url":"/tags/cuda/"}],"timestamp":1710115200,"title":"CUDA学习(十)--书籍中的函数整合"},{"authors":[],"categories":[{"title":"Tutorial","url":"/categories/tutorial/"}],"content":"这部分是基于原子操作章节进行的高级操作介绍，即实现锁定数据结构。\n原子操作只能确保每个线程在完成读取-修改-写入的序列之前，将不会有其他的线程读取或者写入目标内存。然而，原子操作并不能确保这些线程将按照何种顺序执行。例如当有三个线程都执行加法运算时，加法运行的执行顺序可以为(A + B) + C，也可以为A + (B + C)。这对于整数来说是可以接受的，因为中间结果可能被截断，因此(A + B) + C通常并不等于A + (B + c)。因此，浮点数值上的原子数学运算是否有用就值得怀疑🤔。因此在早期的硬件中，浮点数学运算并不是优先支持的功能。\n然而，如果可以容忍在计算结果中存在某种程度的不确定性，那么仍然可以完全在GPU上实现归约运算。我们首先需要通过某种方式来绕开原子浮点数学运算。在这个解决方案中仍将使用原子操作，但不是用于算数本身。\n原子锁 基本思想是，分配一小块内存作为互斥体，互斥体这个资源可以是一个数据结构，一个缓冲区，或者是一个需要以原子方式修改的内存位置。当某个线程从这个互斥体中读到0时，表示没有其他线程使用这块内存。因此，该线程就可以锁定这块内存，并执行想要的修改，而不会收到其他线程的干扰。要锁定这个内存位置，线程将1写入互斥体，这将防止其他竞争的线程锁定这个内存。然后，其他竞争线程必须等待直到互斥体的所有线程将0写入到互斥体后才能尝试修改被锁定的内存。实现锁定过程的代码可以像下面这样:\nvoid lock(void){ if(*mutex == 0){ *mutex = 1; //将1保存到锁 } } 不过这段代码中存在一个严重的问题。如果在线程读取到0并且还没有修改这个值之前，另一个线程将1写入到互斥体，那么会发生什么情况？也就是说，这两个线程都将检查mutex上的值，并判断其是否为0。然后，它们都将1写入到这个位置，并且都执行后面的语句。这会产生严重的后果。\n我们想要完成的操作是：将mutex的值与0相比较，如果mutex等于0，则将1写入到这个位置。要正确实现这个操作，整个运算都需要以原子方式执行，这样就可以确保当线程当线程检查和更新mutex值时，不会有其他的线程进行干扰。在CUDA中，这个操作可以通过函数atomicCAS()来实现，这是一个原子的比较-交换操作(Compare-and-Swap)。函数atomicCAS()的参数包括一个指向目标内存的指针，一个与内存中的值进行比较的值，以及一个当比较相等时保存到目标内存上的值。通过这个操作，我们可以实现一个GPU锁定函数，如下：\n__device__ void lock(void){ while(atomicCAS(mutex, 0, 1) != 0); } 调用atomicCAS()将返回位于mutex地址上的值。**因此while循环会不断运行，直到atomicCAS发现mutex的值为0。当发现为0时，比较操作成功，线程将把1写入到mutex。本质上来看，这个线程将在while循环中不断重复，直到它成功地锁定这个数据结构。**我们将使用这个锁定机制来实现GPU散列表。下面是一种实现方式：\nstruct Lock{ int *mutex; Lock(void){ int state = 0; HANDLE_ERROR(cudaMalloc((void**)\u0026amp; mutex, sizeof(int))); HANDLE_ERROR(cudaMemcpy(mutex, \u0026amp;state, sizeof(int), cudaMemcpyHostToDevice)); } ~Lock(void){ cudaFree(mutex); } __device__ void lock(void){ while(atomicCAS(mutex, 0, 1) != 0); } __device__ void unlock(void){ atomicExch(mutex, 0); } } 代码中通过atomicExch(mutex, 0)来重制mutex的值，将其与第二个参数进行交换，并返回它读到的值。然而，为什么不用跟简单的方法，例如*mutex = 0;呢，因为原子事务和常规的内存操作将采用不同的执行路径。如果同时使用原子操作和标准的全局内存操作，那么将使得unlock()与后面的lock()调用看上去似乎没有被同步。虽然这种混合使用的方式仍可以实现正确的功能，但是为了增加应用程序的可读性，对于所有对互斥体的访问都应使用相同的方式。因此，在使用原子语句来锁定资源后，同样应使用原子语句来解锁资源。\n我们想要在最早的点积运算示例中加上原子锁。Lock结构位于lock.h中，在修改后的点积示例中将包含这个头文件。\n#include \u0026#34;../common/book.h\u0026#34; #include \u0026#34;lock.h\u0026#34; #define imin(a, b) (a \u0026lt; b ? a : b) const int N = 33 * 1024 * 1024; const int threadsPerBlock = 256; const int blocksPerGrid = imin(32, (N + threadsPerBlock - 1) / threadsPerBlock); 核函数也有所不同，在修改后的点积函数中，将Lock类型的变量，以及输入矢量和输出缓冲区传递给核函数。Lock将被用于在最后的累加步骤中控制对输出缓冲区的访问。另一处修改是float *c。之前float *c是一个包含N个浮点数的缓冲区，其中每个线程块都将其计算得到的临时结果保存到相应的元素中。这个缓冲区将被复制到CPU以便计算最终的点积值。然而，现在的参数c将不再指向一个缓冲区，而是指向一个浮点数值，这个值表示a和b中矢量的点积。\n__global__ void dot(Lock lock, float *a, float *b, float *c){ __shared__ float cache[threadsPerBlock]; int tid = threadIds.s + blockIdx.x * blockDim.x; int cacheIndex = threadIdx.x; float temp = 0; while(tid \u0026lt; N){ temp += a[tid] * b[tid]; tid += blockDim.x * gridDim.x; } //设置cache中相应位置上的值 cache[cacheIndex] = temp; //对线程块中的线程进行同步 __syncthreads(); //对于归约运算来说，以下代码要求threadPerBlock必须是2的幂 int i = blockDim.x / 2; while(i != 0){ if(cacheIndex \u0026lt; i){ cache[cacheIndex] += cache[cacheIndex + i]; } __syncthreads(); i /= 2; } 现在到执行到这里时，每个线程块中的256个线程都已经把各自计算的乘积相加起来，并且保存到cache[0]中。现在，每个线程块都需要将其临时结果相加到c执行的内存位置上。为了安全地执行这个操作，我们将使用锁来控制对该内存位置的访问，因此每个线程在更新*c的值之前，要先获取这个锁。在线程块的临时结果与c处的值相加后，将解锁互斥体，这样其他的线程可以继续累加它们的值。在将临时值与最终结果相加后，这个线程块将不再需要任何计算，因此从核函数中返回。\nif(cacheIndex == 0){ lock.lock(); *c += cache[0]; lock.unlock(); } } 下面是main函数：\nint main(){ float *a, *b, c = 0; float *dev_a, *dev_b, *dev_c; //在CPU上分配内存 a = (float*)malloc(N * sizeof(float)); b = (float*)malloc(N * sizeof(float)); //在GPU上分配内存 HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_a, N * sizeof(float))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_b, N * sizeof(float))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_c, N * sizeof(float))); //用数据填充主机内存 for(int i = 0; i \u0026lt; N; i++){ a[i] = i; b[i] = i * 2; } //将数组“a“和”b“复制到GPU HANDLE_ERROR(cudaMemcpy(dev_a, a, N * sizeof(float), cudaMemcpyHostToDevice)); HANDLE_ERROR(cudaMemcpy(dev_b, b, N * sizeof(float), cudaMemcpyHostToDevice)); //将“0”复制到dev_c HANDLE_ERROR(cudaMemcpy(dev_c, \u0026amp;c, N * sizeof(float), cudaMemcpyHostToDevice)); //声明Lock，调用核函数，并将结果复制回CPU。 Lock lock; dot\u0026lt;\u0026lt;\u0026lt;blocksPerGrid, threadsPerBlock\u0026gt;\u0026gt;\u0026gt;(lock, dev_a, dev_b, dev_c); //将数组“c”从GPU复制到CPU HANDLE_ERROR(cudaMemcpy(\u0026amp;c, dev_a, sizeof(float), cudaMemcpyDeviceToHost)); #define sum_squares(x) (x * (x + 1) * (2 * x + 1) / 6) printf(\u0026#34;Does GPU value %.6g = %.6g?\\n\u0026#34;, c, 2 * sum_squares((float)(N - 1))); //释放GPU上的内存 cudaFree(dev_a); cudaFree(dev_b); cudaFree(dev_c); //释放CPU上的内存 free(a); free(b); } 散列表 (Hash Table) 接下来我们将介绍hash table的CPU以及GPU实现。\n我们先介绍一下什么是hash table。hash table是一种保存键-值二元组的数据结构，一个字典就可以被视为一个hash table。我们在使用hash table时，需要将查找某个键对应的值的时间降到最低。我们希望这是一个常量时间，即不管hash table多大，搜索某个键对应的值所需时间应该是不变的。\nhash table根据值相应的键，把值放入到“桶(bucket)”中。这种将键映射到值的方法通常被称为散列函数(Hash Function)。对于理想的hash function，每个键都会被映射到一个不同的桶。然而，当多个键被映射到同一个桶中时，我们需要将桶中的所有值保存到一个链表里，每当同一个桶添加新的值时，就将新的值添加到链表的末尾。\nCPU散列表 我们将分配一个长度为N的数组，并且数组中的每个元素都表示一个键-值二元链表。下面是实现的数据结构：\n#include \u0026#34;../common/book.h\u0026#34; struct Entry{ unsigned int key; void* value; Entry* next; }; struct Table{ size_t count; Entry** entries; Entry* pool; Entry* firstFree; } Entry结构中包含了键和值。在应用程序中，键是无符号整数。与键相关的值可以是任意数据类型，因此我们将value声明为一个void*变量。程序重点是介绍如何创建hash table数据结构，因此在value域中并不保存任何内容，仅保证完整性。结构Entry最后的一个成员是指向下一个Entry节点的指针。在遇到冲突时，在同一个桶中将包含多个Entry节点，因此我们决定将这些对象保存为一个链表。所以每个对象都指向桶中的下一个节点，从而形成一个节点链表。最后一个Entry节点的next指针为NULL。\n本质上，table结构本身是一个“桶”数组，这个桶数组是一个长度为count的数组，其中entries中的每个桶都是指向某Entry的指针。如果每添加一个Entry节点时都分配新的内存，那么将对程序性能产生负面的影响。为了避免这种情况，hash table将在成员pool中维持一个可用Entry节点的数组。firstFree指向下一个可用的Entry节点，因此当需要将一个节点添加到hash table时，只需使用由firstFree指向的Entry然后递增这个指针，就能避免新分配内存，而且只需要free()一次就能释放所有这些节点。\n下面是其他的支持代码：\nvoid initialize_table(Table \u0026amp;table, int entries, int elements) { table.count = entries; table.entries = (Entry**)calloc( entries, sizeof(Entry*) ); table.pool = (Entry*)malloc( elements * sizeof( Entry ) ); table.firstFree = table.pool; } 在hash table初始化的过程中，主要的操作包括为桶数组entries分配内存，我们也为节点池分配了内存，并将指针firstFree初始化为指向节点池数组中的第一个节点。程序末尾释放内存时将释放桶数组和空闲节点池:\nvoid free_table(Table \u0026amp;table){ free(table.entries); free(table.pool); } 在本示例中，我们采取了无符号整数作为键，并且需要将这些键映射到桶数组的索引。也就是说，将节点e保存在table.entries[e.key]中。然而，我们需要确保键的取值范围将小于桶数组的长度。下面是解决方法：\nsize_t hash(unsigned int key, size_t count){ return key % count; } 这里的实现方式是将键对数组长度取模，实际情况会更复杂，这里仅仅作为示例程序来展示。我们将随机生成键，如果我们假设随机数值生成器生成的值大致是平均的，那么这个hash function应该将这些键均匀地映射到hash table的所有桶中。真正的实际情况中我们可能需要创建更为复杂的hash function。\n将键值二元数组添加到hash table中包括三个基本的步骤:\n将键放入hash function中计算出新节点所属的桶。 从节点池中取出一个预先分配的Entry节点，初始化该节点的key和value等变量。 将这个节点插入到计算得到的桶的链表首部。 下面是实现的代码:\nvoid add_to_table(Table \u0026amp;table, unsigned int key, void* value){ // step 1 size_t hashValue = hash(key, table.count); // step 2 Entry* location = table.firstFree++; location-\u0026gt;key = key; location-\u0026gt;value = value; // step 3 location-\u0026gt;next = table.entries[hashValue]; table.entries[hashValue] = location; } 步骤3可能会有点难理解。链表的第一个节点是储存在了table.entries[hashValue]中，我们需要在链表的头节点中插入一个新的节点(如果在链表的末尾插入新的节点的话则需要对链表进行遍历直到末尾，增加了复杂度): 首先将新节点的next指针设置为指向链表的第一个节点，然后再将新节点保存到桶数组中(桶数组保存的是链表的第一个节点)，这样就完成了。\n为了判断这段代码能否工作，我们实现了一个函数对hash table执行完好性检查。检查过程中首先遍历这张表并查看每个节点。将节点的键放入到hash function进行计算，并确认这个节点被保存到正确的桶中。在检查了每个节点后，还要验证hash table中的节点数量确实等于添加到hash table的节点数量。如果这些数值并不相等，那么要么是无意中将一个节点添加到多个桶，要么没有正确的插入节点。\n#define SIZE (100 * 1024 * 1024) #define ELEMENTS (size / sizeof(unsigned int)) void verify_table(const Table \u0026amp;table){ int count = 0; for(size_t i = 0; i \u0026lt; table; i++){ Entry* current = table.entries[i]; while(current != NULL){ count++; if(hash(current-\u0026gt;value, table.count) != i){ printf(\u0026#34;%d hashed to %ld, but was located at %ld\\n\u0026#34;, current-\u0026gt;value, hash(current-\u0026gt;value, table.count), i); current = current-\u0026gt;next; } } if(count != ELEMENTS){ printf(\u0026#34;%d elements found in hash table. Should be %ld\\n\u0026#34;, count, ELEMENTS); } else{ printf(\u0026#34;All %d elements found in hash table.\\n\u0026#34;, count); } } } 由于大部分的功能实现都放到了函数中，因此main()函数就相对比较简单:\n#define HASH_ENTRIES 1024 int main(){ unsigned* buffer = (unsigned int*)big_random_block(SIZE); clock_t start, stop; start = clock(); Table table; initialize_table(table, HASH_ENTRIES, ELEMENTS); for(int i = 0; i \u0026lt; ELEMENTS; i++){ add_to_table(table, buffer[i], (void*)NULL); } stop = clock(); float elapsedTime = (float)(stop - start) / (float)CLOCK_PER_SEC * 1000.0f; printf(\u0026#34;Time to hash: %3.1f ms\\n\u0026#34;, elapsedTime); verify_table(table); free_table(table); free(buffer); } 我们首先分配了一大块内存来保存随机数值。这些随机生成的无符号整数将被作为插入到hash table中的键。在生成了这些数值后，接下来将读取系统时间以便统计程序的性能。我们对hash table进行初始化，然后通过for循环将每个随机键插入到hash table。在添加了所有的键后，再次读取系统时间，通过之前读取的系统时间与这次系统时间就可以计算出在初始化和添加键上花费的时间。最后，我们通过完整性检查函数来验证hash table，并且释放了分配的内存。\n多线程环境下的hash table 多线程环境下的hash table可能会遇到race condition。那么如何在GPU上构建一个hash table呢？在点积示例中，每次只有一个线程可以安全地将它的值与最终结果相加。如果每个桶都有一个原子锁，那么我们可以确保每次只有一个线程对指定的桶进行修改。\nGPU hash table 在有了某种方法来确保对hash table实现安全的多线程访问，我们就可以实现GPU的hash table的应用程序。我们需要使用Lock，还需要把hash function声明为一个__device__函数。\n#include \u0026#34;../common/book.h\u0026#34; #include \u0026#34;lock.h\u0026#34; struct Entry{ unsigned int key; void* value; Entry* next; } struct Table{ size_t count; Entry** entries; Entry* pool; } __device__ __host__ size_t hash(unsigned int value, size_t count){ return value % count; } 当__host__与__device__关键字一起使用时，将告诉NVIDIA编译器同时生成函数在设备和主机上的版本。设备版本的函数将在设备上运行，并且只能从设备代码中调用。主机版本的函数将在主机上运行，并且只能从主机代码中调用。__host__与__device__关键字一起使用可以让这个函数既可以在设备上使用又可以在主机上使用。\ninitialize_table()和free_table()与CPU版本差别不大，只是数组的初始化以及释放的代码修改成的GPU版本:\nvoid initialize_table(Table \u0026amp;table, int entries, int elements){ table.count = entries; HANDLE_ERROR(cudaMalloc((void**)\u0026amp;table.entries, entries * sizeof(Entry*))); HANDLE_ERROR(cudaMemset(table.entries, 0, entries * sizeof(Entry*))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;table.pool, elements * sizeof(Entry))); } void free_table(Table \u0026amp;table){ cudaFree(table.pool); cudaFree(table.entries); } verify_table()的CPU版本和GPU版本相同，仅仅需要在开头增加一个函数将hash table从GPU复制到CPU。下面是将hash table从GPU复制到CPU的代码:\nvoid copy_table_to_host(const Table \u0026amp;table, Table \u0026amp;hostTable){ hostTable.count = table.count; hostTable.entries = (Entry**)calloc(table.count, sizeof(Entry*)); hostTable.pool = (Entry*)malloc(ELEMENTS * sizeof(Entry)); HANDLE_ERROR(cudaMemcpy(hostTable.entries, table.entries, table.count * sizeof(Entry*), cudaMemcpyDeviceToHost)); HANDLE_ERROR(cudaMemcpy(hostTable.pool, table.pool, ELEMENTS * sizeof(Entry), cudaMemcpyDeviceToHost)); 在复制的数据中，有一部分数据是指针。我们不能简单地将这些指针复制到主机，因为这些指针指向的地址是在GPU上，它们在主机上并不是有效的指针。然而，这些指针的相对偏移量仍然是有效的。每个指向Entry节点的GPU指针都指向数组table.pool[]中的某个位置，但为了在主机上使用hash table，我们需要它们指向数组hostTable.pool[]中相同的Entry。\n给定一个GPU的指针X，需要将这个指针相对于table.pool的偏移与hostTable.pool相加，从而获得一个有效的主机指针，新指针应该按照以下公式计算: $$ (X - table.pool) + hostTable.pool $$ 对于每个被复制的Entry指针，都要执行这个更新操作：包括hostTable.entries中的Entry指针，以及hash table的节点池中每个Entry的next指针:\nfor(int i = 0; i \u0026lt; table.count; i++){ if(hostTable.entries[i] != NULL){ hostTable.entries[i] = (Entry*)((size_t)hostTable.entries[i] - (size_t)table.pool + (size_t)hostTable.pool); } } for(int i = 0; i \u0026lt; ELEMENTS; i++){ if(hostTable.pool[i].next != NULL){ hostTable.pool[i].next = (Entry*)((size_t)hostTable.pool[i].next - (size_t)table.pool + (size_t)hostTable.pool); } } } 在介绍完了数据结构、hash function、初始化过程、内存释放过程以及验证代码后，还剩下的重要部分就是CUDA C原子语句的使用。核函数add_to_table()的参数包括一个键数组、一个值数组、hash table本身以及一个lock数组。这些数组将被用于锁定hash table中的每个桶。由于输入的数据是两个数组，并且在线程中需要对这两个数组进行索引，因此还需要将索引线性化:\n__global_ void add_to_table(unsigned int* keys, void** values, Table table, Lock* lock){ int tid = threadIdx.x + blockIdx.x * blockDim.x; int stride = blockDim.x * gridDim.x; 线程会像点积示例那样遍历输入数组。对于数组key[]中的每个键，线程都将通过hash function计算出这个键值二元数组属于哪个桶。在计算出目标桶之后，线程会锁定这个桶，添加它的键值二元组，然后解锁这个桶。\nwhile(tid \u0026lt; ELEMENTS){ unsigned int key = keys[tid]; size_t hashValue = hash(key, table.count); for(int i = 0; i \u0026lt; 32; i++){ if((tid % 32) == i){ Entry* location = \u0026amp;(table.pool[tid]); location-\u0026gt;key = key; location-\u0026gt;value = values[tid]; lock[hashVaue].lock(); location-\u0026gt;next = table.entries[hashValue]; table.entries[hashValue] = location; lock[hashValue].unlock(); } } tid += stride; } } for循环和后面的if语句看上去是不必要的。然而，代码中的线程束是一个包含32线程的集合，并且这些线程以步调一致的方式执行。每次在线程束中只有一个线程可以获取这个锁。如果让线程束中的所有32给线程都同时竞争这个锁，那么将会发生严重的问题。这种情况下，最好的方式就是在软件中执行一部分工作，遍历线程束中的线程，并给每个线程一次机会来获取数据结构的锁，执行它的工作，然后释放锁。\nmain函数的执行流程跟CPU版本的相似:\nint main(){ unsigned int* buffer = (unsigned int*)big_random_block(SIZE); cudaEvent_t start, stop; HANDLE_ERROR(cudaEventCreate(\u0026amp;start)); HANDLE_ERROR(cudaEventCreate(\u0026amp;stop)); HANDLE_ERROR(cudaEventRecord(start, 0)); unsigned int* dev_keys; void** dev_values; HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_keys, SIZE)); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_values, SIZE)); HANDLE_ERROR(cudaMemcpy(dev_keys, buffer, SIZE, cudaMemcpyHostToDevice)); Table table; initialize_table(table, HADH_ENTRIES, ELEMENTS); // 声明一个锁数组，数组中的每个锁对应于桶数组中的每个桶。并将它们复制到GPU上。 Lock lock[HASH_ENTRIES]; Lock* dev_lock; HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_lock, HASH_ENTRIES * sizeof(Lock))); HANDLE_ERROR(cudaMemcpy(dev_lock, lock, HASH_ENTRIES * sizeof(Lock), cudaMemcpyHostToDevice)); // 将键添加到hash table，停止性能计数器，验证hash table的正确性，执行释放工作 add_to_table\u0026lt;\u0026lt;\u0026lt;60, 256\u0026gt;\u0026gt;\u0026gt;(dev_keys, dev_values, tavle, dev_lock); HANDLE_ERROR(cudaEventRecord(stop, 0)); HANDLE_ERROR(cudaEventSynchronize(stop)); float elapsedTime; HANDLE_ERROR(cudaEventElapsedTime(\u0026amp;elapsedTime, start, stop)); printf(\u0026#34;Time to hash: 3%3.1f ms\\n\u0026#34;, elapsedTime); verify_table(table); HANDLE_ERROR(cudaEventDestroy(start)); HANDLE_ERROR(cudaEventDestroy(stop)); free_table(table); cudaFree(Dev_lock); cudaFree(dev_keys); cudaFree(dev_values); free(buffer); } ","date":"March 8, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E4%B9%9D/","series":[],"smallImg":"","tags":[{"title":"CUDA","url":"/tags/cuda/"}],"timestamp":1709856000,"title":"CUDA学习(九)"},{"authors":[],"categories":[{"title":"Tutorial","url":"/categories/tutorial/"}],"content":"本章将介绍如何分配和使用零拷贝内存(Zero-Copy Memory)，如何在同一个应用程序中使用多个GPU，以及如何分配和使用可移动的固定内存(Portable Pinned Memory)。\n零拷贝主机内存 上一章介绍了固定内存（页锁定内存），这种新型的主机内存能够确保不会交换出物理内存。我们通过cudaHostAlloc()来分配这种内存，并且传递参数cudaHostAllocDefault来获得默认的固定内存。本章会介绍在分配固定内存时可以使用其他参数值。除了cudaHostAllocDefault外，还可以传递的标志之一是cudaHostAllocMapped。通过cudaHostAllocMapped分配的主机内存也是固定的，它与通过cudaHostAllocDefault分配的固定内存有着相同的属性，特别是当它不能从物理内存中交换出去或者重新定位时。但这种内存除了可以用于主机与GPU之间的内存复制外，还可以打破主机内存规则之一：可以在CUDA C核函数中直接访问这种类型的主机内存。由于这种内存不需要复制到GPU，因此也被称为零拷贝内存。\n通过零拷贝内存实现点积运算 通常，GPU只能访问GPU内存，而CPU也只能访问主机内存。但在某些环境中，打破这种规则或许能带来更好的效果。下面仍然给出一个矢量点积运算来进行介绍。这个版本不将输入矢量显式复制到GPU，而是使用零拷贝内存从GPU中直接访问数据。我们将编写两个函数，其中一个函数是对标准主机内存的测试，另一个函数将在GPU上执行归约运算，并使用零拷贝内存作为输入缓冲区和输出缓冲区。首先是点积运算的主机内存版本:\nfloat malloc_test(int size){ //首先创建计时事件，然后分配输入缓冲区和输出缓冲区，并用数据填充输入缓冲区。 cudaEvent_t start, stop; float *a, *b, c, *partial_c; float *dev_a, *dev_b, *dev_partial_c; float elapsedTime; HANDLE_ERROR(cudaEventCreate(\u0026amp;start)); HANDLE_ERROR(cudaEventCreate(\u0026amp;stop)); //在CPU上分配内存 a = (float*)malloc(size * sizeof(float)); b = (float*)malloc(size * sizeof(float)); partial_c = (float*)malloc(blocksPerGrid * sizeof(float)); //在GPU上分配内存 HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_a, size * sizeof(float))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_b, size * sizeof(float))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_partial_c, blocksPerGrid * sizeof(float))); //用数据填充主机内存 for(int i = 0; i \u0026lt; size; i++){ a[i] = i; b[i] = i * 2; } //启动计时器，将输入数据复制到GPU，执行点积核函数，并将中间计算结果复制回主机。 HANDLE_ERROR(cudaEventRecord(start, 0)); //将数组“a“和”b”复制到GPU HANDLE_ERROR(cudaMemcpy(dev_a, a, size * sizeof(float), cudaMemcpyHostToDevice)); HANDLE_ERROR(cudaMemcpy(dev_b, b, size * sizeof(float), cudaMemcpyHostToDevice)); dot\u0026lt;\u0026lt;\u0026lt;blocksPerGrid, threadsPerBlock\u0026gt;\u0026gt;\u0026gt;(size, dev_a, dev_b, dev_partial_c); //将数组“c”从GPU复制到CPU HANDLE_ERROR(cudaMemcpy(partial_c, dev_partial_c, blocksPerGrid * sizeof(float), cudaMemcpyDeviceToHost)); //停止计时器 HANDLE_ERROR(cudaEventRecord(stop, 0)); HANDLE_ERROR(cudaEventSynchronize(stop)); HANDLE_ERROR(cudaEventElapsedTime(\u0026amp;elapsedTime, start, stop)); //将中间计算结果相加起来，并释放输入缓冲区和输出缓冲区 //结束CPU上的计算 c = 0; for(int i = 0; i \u0026lt; blocksPerGrid; i++){ c += partial_c[i]; } HANDLE_ERROR(cudaFree(dev_a)); HANDLE_ERROR(cudaFree(dev_b)); HANDLE_ERROR(cudaFree(dev_partial_c)); //释放CPU上的内存 free(a); free(b); free(partial_c); //释放事件 HANDLE_ERROR(cudaEventDestroy(start)); HANDLE_ERROR(cudaEventDestroy(stop)); printf(\u0026#34;Value calculated: %f\\n\u0026#34;, c); return elapsedTime; } 使用零拷贝内存的版本是非常类似的多，只是在内存分配上有所不同：\nfloat cuda_host_alloc_test(int size){ cudaEvent_t start, stop; float *a, *b, c, *partial_c; float *dev_a, *dev_b, *dev_partial_c; float elapsedTime; HANDLE_ERROR(cudaEventCreate(\u0026amp;start)); HANDLE_ERROR(cudaEventCreate(\u0026amp;stop)); //在CPU上分配内存 HANDLE_ERROR(cudaHostAlloc((void**)\u0026amp;a, size * sizeof(float), cudaHostAllocWriteCombined | cudaHostAllocMapped)); HANDLE_ERROR(cudaHostAlloc((void**)\u0026amp;b, size * sizeof(float), cudaHostAllocWriteCombined | cudaHostAllocMapped)); HANDLE_ERROR(cudaHostAlloc((void**)\u0026amp;partial_c, blocksPerGrid * sizeof(float), cudaHostAllocMapped)); //用数据填充主机内存 for(int i = 0; i \u0026lt; size; i++){ a[i] = i; b[i] = i * 2; } 使用cudaHostAlloc()时，通过参数flags来指定内存的其他行为。cudaHostAllocMapped这个标志告诉运行时将从GPU中访问这块内存。这个标志意味着分配零拷贝内存。对于这两个输入缓冲区，我们还制定了标志cudaHostAllocWriteCombined。这个标志运行时应该将内存分配为“合并式写入（Write-Combined）”内存。这个标志并不会改变应用程序的功能，但却可以显著地提升GPU读取内存时的性能。然而，当CPU也要读取这块内存时，“合并式写入”会显得低效，因此在决定是否使用这个标志之前，必须首先考虑应用程序的可能访问模式。\n在使用标志cudaHostAllocMapped来分配主机内存后，就可以从GPU中访问这块内存。然而，GPU的虚拟内存空间与CPU是不同的，因此在**GPU上访问它们与在CPU上访问它们有着不同的地址。调用cudaHostAlloc()将返回这块内存在CPU上的指针，因此需要调用cudaHostGetDevicePointer()来获得这块内存在GPU上的有效指针。**这些指针将被传递给核函数，并在随后由GPU对这块内存执行读取和写入等操作。即使dev_a、dev_b和dev_partial_c都位于主机上，但对于核函数来说，它们看起来就像GPU内存一样，这正是由于调用了cudaHostGetDevicePointer()。由于部分计算结果已经位于主机上，因此就不再需要通过cudaMemcpy()将它们从设备上复制回来。\nHANDLE_ERROR(cudaHostGetDevicePointer(\u0026amp;dev_a, a, 0)); HANDLE_ERROR(cudaHostGetDevicePointer(\u0026amp;dev_b, b, 0)); HANDLE_ERROR(cudaHostGetDevicePointer(\u0026amp;dev_partial_c, partial_c, 0)); //启动计时器以及核函数 HANDLE_ERROR(cudaEventRecord(start, 0)); dot\u0026lt;\u0026lt;\u0026lt;blocksPerGrid, threadsPerBlock\u0026gt;\u0026gt;\u0026gt;(size, dev_a, dev_b, dev_partial_c); //不再需要通过cudaMemcpy()将它们从设备上复制回来 HANDLE_ERROR(cudaThreadSynchronize()); HANDLE_ERROR(cudaEventRecord(stop, 0)); HANDLE_ERROR(cudaEventSynchronize(stop)); HANDLE_ERROR(cudaEventElapsedTime(\u0026amp;elapsedTime, start, stop)); //结束GPU上的操作 c = 0; for(int i = 0; i \u0026lt; blocksPerGrid; i++){ c += partial_c[i]; } //在使用cudaHostAlloc()的点积运算代码中，唯一剩下的事情就是执行释放操作 HANDLE_ERROR(cudaFreeHost(a)); HANDLE_ERROR(cudaFreeHost(b)); HANDLE_ERROR(cudaFreeHost(partial_c)); //释放事件 HANDLE_ERROR(cudaEventDestroy(start)); HANDLE_ERROR(cudaEventDestroy(stop)); printf(\u0026#34;Value calculated: %f\\n\u0026#34;, c); return elapsedTime; } 无论cudaHostAlloc()中使用什么标志，总是按照相同的方式来释放内存，即只需调用cudaFreeHost()。剩下的工作就是观察main()如何将这些代码片段组合在一起。\nint main(){ cudaDeviceProp prop; int which Device; HANDLE_ERROR(cudaGetDevice(\u0026amp;whichDevice)); HANDLE_ERROR(cudaGetDeviceProperties(\u0026amp;prop, whichDevice)); if(prop.canMapHostMemory != 1){ printf(\u0026#34;Device, cannot map memory. \\n\u0026#34;); } //如果设备支持零拷贝内存，那么接下来就是将运行时置入能分配零拷贝内存的状态 //通过调用cudaSetDeviceFlags()来实现这个操作，并且传递标志值cudaDeviceMapHost来表示我们希望设备映射主机内存 HANDLE_ERROR(cudaSetDeviceFlags(cudaDeviceMapHost)); //运行两个测试，分别显示二者的执行时间，并推出应用程序： float elapsedTime = malloc_test(N); printf(\u0026#34;Time using cudaMalloc: %3.1f ms\\n\u0026#34;, elapsedTime); elapsedTime = cuda_host_alloc_test(N); printf(\u0026#34;Time using cudaHostAlloc: %3.1f ms\\n\u0026#34;, elapsedTime); } 下面是给出的核函数\n#define imin(a, b) (a \u0026lt; b ? a : b) const int N = 33 * 1024 * 1024; const int threadsPerBlock = 256; const int blocksPerGrid = imin(32, (N + threadsPerBlock - 1) / threadsPerBlock); __global__ void dot(int size, float *a, float *b, float *c){ __shared__ float cache[threadsPerBlock]; int tid = threadIdx.x + blockIdx.x * blockDim.x; int cacheIndex = threadIdx.x; float temp = 0; while(tid \u0026lt; size){ temp += a[tid] * b[tid]; tid += blockDim.x * gridDim.x; } //设置cache中的值 cache[cacheIndex] = temp; //同步这个线程块中的线程 __syncthreads(); //对于归约运算， threadsPerBlock必须为2的幂 int i = blockDim.x / 2; while(i != 0){ if(cacheIndex \u0026lt; i){ cache[cacheIndex] += cache[cacheIndex + i]; } __syncthreads(); i /= 2; } if(cacheIndex == 0){ c[blockIdx.x] = cache[0]; } } 使用多个GPU 我们将把点积应用程序修改为使用多个GPU。为了降低编码难度，我们将在上一个结构中把计算点积所需的全部数据都相加起来。\nstruct DataStruct{ int deviceID; int size; float *a; float *b; float returnValue; } 这个结构包含了在计算点积时使用的设备标识，以及输入缓冲区的大小和指向两个输入缓冲区的指针a和b。最后，它还包含了一个成员用于保存a和b的点积运算结果。\n要使用N个GPU，我们首先需要准确地知道N值是多少。因此，在应用程序的开头调用cudaDeviceCount()，从而判断在系统中安装了多少个支持CUDA的处理器。\nint main(){ int deviceCount; HANDLE_ERROR(cudaGetDeviceCount(\u0026amp;deviceCount)); if(deviceCount \u0026lt; 2){ printf(\u0026#34;We need at least two compute 1.0 or greater devices, but only found %d\\n\u0026#34;, deviceCount); } //为输入缓冲区分配标准的主机内存，并按照之前的方式填充 float *a = (float*)malloc(sizeof(float) * N); HANDLE_NULL(a); float *b = (float*)malloc(sizeof(float) * N); HANDLE_NULL(b); //用数据填充主机内存 for(int i = 0; i \u0026lt; N; i++){ a[i] = i; b[i] = i * 2; } 通过CUDA运行API来使用多个GPU时，要意识到每个GPU都需要由一个不同的CPU线程来控制。由于之前只是用了单个GPU，因此不需要担心这个问题。我们将多线程代码的大部分复杂性都移入到辅助代码文件book.h中。在精简了代码后，我们需要做的就是填充一个结构来执行计算。虽然在系统中可以有任意数量的GPU，但为了简单，在这里只使用两个：\nDataStruct data[2]; data[0].deviceID = 0; data[0].size = N / 2; data[0].a = a; data[0].b = b; data[1].deviceID = 1; data[1].size = N / 2; data[1].a = a + N / 2; data[1].b = b + N / 2; 我们将其中一个DataStruct变量传递给辅助函数start_thread()。此外，还将一个函数指针传给了start_thread()，新创建的线程将调用这个函数，这个示例中的线程函数为routine()。函数start_thread()将创建一个新的线程，这个线程将调用routine()，并将DataStruct变量作为参数传递进去。在应用程序的默认线程中也将调用routine()(因此只多创建了一个线程)。\nCUTThread thread = start_thread(routine, \u0026amp;(data[0])); routine(\u0026amp;(data[1])); //通过调用end_thread()，主应用程序线程将等待另一个线程执行完成。 end_thread(thread); //由于这两个线程都在main()的这个位置上执行完成，因此可以安全地释放内存并显示结果。 free(a); free(b); //我们要将每个线程的计算结果相加起来。 printf(\u0026#34;Value calculated: %f\\n\u0026#34;, data[0].returnValue + data[1].returnValue); } 在声明routine()时指定该函数带有一个void*参数，并返回void*，这样在start_thread()部分代码保持不变的情况下可以任意实现线程函数。\nvoid* routine(void *pvoidData){ DataStruct *data = (DataStruct*)pvoidData; HANDLE_ERROR(cudaSetDevice(data-\u0026gt;deviceID)); 除了调用cudaSetDevice()来指定希望使用的CUDA设备外，routine()的实现非常类似于之前提到的malloc_test()。我们为输入数据和临时计算结果分别分配了内存，随后调用cudaMemcpy()将每个输入数组复制到GPU。\nint size = data-\u0026gt;size; float *a, *b, c, *partial_c; float *dev_a, *dev_b, *dev_partial_c; //在CPU上分配内存 a = data-\u0026gt;a; b = data-\u0026gt;b; partial_c = (float*)malloc(blocksPerGrid * sizeof(float)); //在GPU上分配内存 HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_a, size * sizeof(float))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_b, size * sizeof(float))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_partial_c, blocksPerGrid * sizeof(float))); //将数组“a”和“b”复制到GPU上 HANDLE_ERROR(cudaMemcpy(dev_a, a, size * sizeof(float), cudaMemcpyHostToDevice)); HANDLE_ERROR(cudaMemcpy(dev_b, b, size * sizeof(float), cudaMemcpyHostToDevice)); //启动点积核函数，复制回计算结果，并且结束CPU上的操作 dot\u0026lt;\u0026lt;\u0026lt;blocksPerGrid, threadsPerBlock\u0026gt;\u0026gt;\u0026gt;(size, dev_a, dev_b, dev_partial_c); //将数组“c”从GPU复制回CPU HANDLE_ERROR(cudaMemcpy(partial_c, dev_partial_c, blocksPerGrid * sizeof(float), cudaMemcpyDeviceToHost)); //结束CPU上的操作 c = 0; for(int i = 0; i \u0026lt; blocksPerGrid; i++){ c += partial_c[i]; } HANDLE_ERROR(cudaFree(dev_a)); HANDLE_ERROR(cudaFree(dev_b)); HANDLE_ERROR(cudaFree(dev_partial_c)); //释放CPU侧的内存 free(partial_c); data-\u0026gt;returnValue = c; } 可移动的固定内存 我们可以将固定内存分配为可移动的，这意味着可以在主机线程之间移动这块内存，并且每个线程都将其视为固定内存。要达到这个目的，需要使用cudaHostAlloc()来分配内存，并且在调用时使用一个新的标志：cudaHostAllocPortable。这个标志可以与其他标志一起使用，例如cudaHostAllocWriteCombined和cudaHostAllocMapped。这意味着在分配主机内存时，可将其作为可移动、零拷贝以及合并式写入等的任意组合。\n为了说明可移动固定内存的作用，我们将进一步修改使用多GPU的点积运算应用程序。\nint main(){ int deviceCount; HANDLE_ERROR(cudaGetDeviceCount(\u0026amp;deviceCount)); if(deviceCount \u0026lt; 2){ printf(\u0026#34;We need at least two compute 1.0 or greater devices, but only found %d\\n\u0026#34;, deviceCount); } cudaDeviceProp prop; for(int i = 0; i \u0026lt; 2; i++){ HANDLE_ERROR(cudaGetDeviceProperties(\u0026amp;prop, i)); if(prop.canMapHostMemory != 1){ printf(\u0026#34;Devide %d cannot map memory.\\n\u0026#34;, i); } } float *a, *b; HANDLE_ERROR(cudaSetDevice(0)); HANDLE_ERROR(cudaSetDeviceFlags(cudaDeviceMapHost)); HANDLE_ERROR(cudaHostAlloc((void**)\u0026amp;a, N * sizeof(float), cudaHostAllocWriteCombined | cudaHostAllocPortable | cudaHostAllocMapped)); HANDLE_ERROR(cudaHostAlloc((void**)\u0026amp;b, N * sizeof(float), cudaHostAllocWriteCombined | cudaHostAllocPortable | cudaHostAllocMapped)); 在使用cudaHostAlloc()分配页锁定内存时，首先要通过调用cudaSetDevice()来初始化设备。我们将新介绍的标志cudaHostAllocPortable传递给这两个内存分配操作。由于这些内存是在调用了cudaSetDevice(0)之后才分配的，因此，如果没有将这些内存指定为可移动的内存，那么只有第0个CUDA设备会把这些内存视为固定内存。\n继续之前的应用程序，为输入矢量生成数据，并采用之前的示例的方式来准备DataStruct结构。\n//用数据填充主机内存 for(int i = 0; i \u0026lt; N; i++){ a[i] = i; b[i] = i * 2; } //为使用多线程做好准备 DataStruct data[2]; data[0].deviceID = 0; data[0].offset = 0; data[0].size = N / 2; data[0].a = a; data[0].b = b; data[1].deviceID = 1; data[1].offset = N / 2; data[1].size = N / 2; data[1].a = a; data[1].b = b; //创建第二个线程，并调用routine()开始在每个设备上执行计算 CUTThread thread = start_thread(routine, \u0026amp;(data[1])); routine(\u0026amp;(data[0])); end_thread(thread); //由于主机内存时由CUDA运行时分配的，因此需要用cudaFreeHost()而不是free()来释放它 HANDLE_ERROR(cudaFreeHost(a)); HANDLE_ERROR(cudaFreeHost(b)); printf(\u0026#34;Value calculated: %f\\n\u0026#34;, data[0].returnValue + data[1].returnValue); } 为了在多GPU应用程序中支持可移动的固定内存和零拷贝内存，我们需要对routine()的代码进行两处修改。\nvoid* routine(void *pvoidData){ DataStruct *data = (DataStruct*)pvoidData; if(data-\u0026gt;deviceID != 0){ HANDLE_ERROR(cudaSetDevice(data-\u0026gt;deviceID)); HANDLE_ERROR(cudaSetDeviceFlags(cudaDeviceMapHost)); } 在多GPU版本的代码中，我们需要在routine()中调用cudaSetDevice()，从而确保每个线程控制一个不同的GPU。另一方面，在这个示例中，我们已经在主线程中调用了一次cudaSetDevice()。这么做的原因时为了在main()中分配固定内存。因此，我们只希望在还没有调用cudaSetDevice()的设备上调用cudaSetDevice()和cudaSetDeviceFlags()。也就是，如果devideID不是0，那么将调用这两个函数。虽然在第0个设备上再次调用这些函数会使代码更简洁，但是这种做法是错误的。一旦在某个线程上设置了这些设备，那么将不能再次调用cudaSetDevice()，即便传递的是相同的设备标识符。\n除了使用可移动的固定内存外，我们还使用了零拷贝内存，一边从GPU中直接访问这些内存。因此，我们使用cudaHostGetDevicePointer()来获得主机内存的有效设备指针，这与前面零拷贝示例中采用的方法一样。然而，你可能会注意到使用了标准的GPU内存来保存临时计算结果。这块内存同样是通过cudaMalloc()来分配的。\nint size = data-\u0026gt;size; float *a, *b, c, *partial_c; float *dev_a, *dev_b, *dev_partial_c; //在CPU上分配内存 a = data-\u0026gt;a; b = data-\u0026gt;b; partial_c = (float*)malloc(blocksPerGrid * sizeof(float)); HANDLE_ERROR(cudaHostGetDevicePointer(\u0026amp;dev_a, a, 0)); HANDLE_ERROR(cudaHostGetDevicePointer(\u0026amp;dev_b, b, 0)); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_partial_c, blocksPerGrid * sizeof(float))); //计算GPU读取数据的偏移量“a”和“b” dev_a += data-\u0026gt;offset; dev_b += data-\u0026gt;offset; dot\u0026lt;\u0026lt;\u0026lt;blocksPerGrid, threadsPerBlock\u0026gt;\u0026gt;\u0026gt;(size, dev_a, dev_b, dev_partial_c); //将数组“c“从GPU复制回CPU HANDLE_ERROR(cudaMemcpy(partial_c, dev_partial_c, blocksPerGrid * sizeof(float), cudaMemcpyDeviceToHost)); //结束在CPU上的操作 c = 0; for(int i = 0; i \u0026lt; blocksPerGrid; i++){ c += partial_c[i]; } HANDLE_ERROR(cudaFree(dev_partial_c)); //释放CPU上的内存 free(partial_c); data-\u0026gt;returnValue = c; } ","date":"February 29, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E5%85%AB/","series":[],"smallImg":"","tags":[{"title":"CUDA","url":"/tags/cuda/"}],"timestamp":1709164800,"title":"CUDA学习(八)"},{"authors":[],"categories":[{"title":"Tutorial","url":"/categories/tutorial/"}],"content":"在并行环境中，任务可以是任意操作。例如，应用程序可以执行两个任务：其中一个线程重绘程序的GUI，而另一个线程通过网络下载更新包。这些任务并行执行，彼此之间没有任何共同的地方。虽然GPU上的任务并行性并不像CPU上的任务并行性那样灵活，但仍然可以进一步提高程序在GPU上的运行速度。本章将介绍CUDA流，以及如何通过流在GPU上同时执行多个任务。\n页锁定(Page-Locked)主机内存 CUDA提供了自己独有的机制来分配主机内存：cudaHostAlloc()。malloc()分配的内存与cudaHostAlloc()分配的内存之间存在一个重要差异。C库函数malloc()将分配标准的、可分页的(Pagable)主机内存，而cudaHostAlloc()将分配页锁定的主机内存。页锁定内存也称为固定内存(Pinned Memory)或者不可分页内存，它有一个重要属性：操作系统将不会对这块内存分页并交还到磁盘上，从而确保了该内存始终驻留在物理内存中。因此，操作系统能够安全地使某个应用程序访问该内存的物理地址，因为这块内存将不会被破坏或者重新定位。\n由于GPU知道内存的物理地址，因此可以通过“直接内存访问（Direct Memory Access，DMA）”技术来在GPU和主机之间复制数据。由于DMA在执行复制时无需CPU的介入，这也就意味着，CPU很可能在DMA的执行过程中将目标内存交换到磁盘上，或者通过更新操作系统的分页来重新定位目标内存的物理地址。CPU可能会移动可分页的数据，这就可能对DMA操作造成延迟。因此，在DMA复制过程中使用固定内存时非常重要的。\n事实上，当使用可分页内存进行复制时，CUDA驱动程序仍然会通过DAM把数据传输给GPU。因此，复制操作将执行两遍，第一遍从可分页内存复制到一块“临时的”页锁定内存，然后再从这个页锁定内存复制到GPU上。因此，**每当从可分页内存中执行复制操作时，复制速度将受限于PCIE（高速串行计算机扩展总线标准）传输速度和系统前端总线速度相对较低的一方。当在GPU和主机间复制数据时，这种差异会使页锁定主机内存的性能比标准可分页内存的性能要高达约2倍。**计时PCIE的速度与前端总线的速度相等，由于可分页内存需要更多一次由CPU参与的复制操作，因此会带来额外的开销。\n然而，使用cudaHostAlloc()分配固定内存时，将失去虚拟内存的所有功能。特别是在应用程序中使用每个页锁定内存时都需要分配物理内存，因为这些内存不能交换到磁盘上。这意味着与使用标准的malloc()调用相比，系统将更快地耗尽内存。因此，应用程序在物理内存较少的机器上会运行失败，而且意味着应用程序将影响在系统上运行的其他应用程序的性能。建议仅对cudaMemcpy()调用中的源内存或者目标内存才使用页锁定内存，并且在不再需要使用它们时立即释放，而不是等到应用程序关闭时才释放。\n下面给的例子来说明如何分配固定内存，以及它相对于标准可分页内存的性能优势。这个例子主要是测试cudaMemcpy()在可分配内存和页锁定内存上的性能。我们要做的就是分配一个GPU缓冲区以及一个大小相等的主机缓冲区，然后两个缓冲区之间执行一些复制操作（从主机到设备、从设备到主机）。为了获得精确的时间统计，我们为复制操作的起始时刻和结束时刻分别设置了CUDA事件。\nfloat cuda_malloc_test(int size, bool up){ cudaEvent_t start, stop; int *a, *dev_a; float elapsedTime; HANDLE_ERROR(cudaEventCreate(\u0026amp;start)); HANDLE_ERROR(cudaEventCreate(\u0026amp;stop)); a = (int*)malloc(size * sizeof(*a)); HANDLE_NULL(a); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_a, size * sizeof(\u0026amp;dev_a))); HANDLE_ERROR(cuddaEventRecord(start, 0)); //为size个整数分别分配主机缓冲区和GPU缓冲区，然后执行100次复制操作，并由参数up来指定复制方向，在完成复制操作后停止计时器 for(int i = 0; i \u0026lt; 100; i++){ if(up){ HANDLE_ERROR(cudaMemcpy(dev_a, a, size * sizeof(*dev_a), cudaMemcpyHostToDevice)); } else{ HANDLE_ERROR(cudaMemcpy(a, dev_a, size * sizeof(*dev_a), cudaMemcpyDeviceToHost)); } } HANDLE_ERROR(cudaEventRecord(stop, 0)); HANDLE_ERROR(cudaEventSynchronize(stop)); HANDLE_ERROR(cudaEventElapsedTime(\u0026amp;elapsedTime, start, stop)); free(a); HANDLE_ERROR(cudaFree(dev_a)); HANDLE_ERROR(cudaEventDestroy(start)); HANDLE_ERROR(cudaEventDestroy(stop)); return elapsedTime; } 函数cuda_malloc_test()通过标准的C函数malloc()来分配可分页主机内存，在分配固定内存时则使用了cudaHostAlloc()。\nfloat cuda_host_alloc_test(int size, bool up){ cudaEvent_t start, stop; int *a, *dev_a; float elapsedTime; HANDLE_ERROR(cudaEventCreate(\u0026amp;start)); HANDLE_ERROR(cudaEventCreate(\u0026amp;stop)); HANDLE_ERROR(cudaHostAlloc((void**)\u0026amp;a, size * sizeof(*a), cudaHostAllocDefault)); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_a, size * sizeof(*dev_a))); HANDLE_ERROR(cudaEventRecord(start, 0)); for(int i = 0; i \u0026lt; 100; i++){ if(up){ HANDLE_ERROR(cudaMemcpy(dev_a, a, size * sizeof(*dev_a), cudaMemcpyHostToDevice)); } else{ HANDLE_ERROR(cudaMemcpy(a, dev_a, size * sizeof(*dev_a), cudaMemcpyDeviceToHost)); } } HANDLE_ERROR(cudaEventRecord(stop, 0)); HANDLE_ERROR(cudaEventSynchronize(stop)); HANDLE_ERROR(cudaEventElapsedTime(\u0026amp;elapsedTime, start, stop)); HANDLE_ERROR(cudaFreeHost(a)); HANDLE_ERROR(cudaFree(dev_a)); HANDLE_ERROR(cudaEventDestroy(start)); HANDLE_ERROR(cudaEventDestroy(stop)); return elapsedTime; } cudaHostAlloc()分配的内存与malloc()分配的内存在使用方式是相同的，与malloc()不同之处在于最后一个参数cudaHostAllocDefault。最后一个参数的取值范围是一组标志，我们可以通过这些标志来修改cudaHostAlloc()的行为，并分配不同形式的固定主机内存。就目前而言，只需使用默认值。最后需要使用cudaFreeHost()来释放内存。\nmain()函数的代码如下：\n#include \u0026#34;../common/book.h\u0026#34; #define SIZE (10 * 1024 * 1024) int main(){ float elapsedTime; float MB = (float)100 * SIZE * sizeof(int) / 1024 / 1024; elapsedTime = cuda_malloc_test(SIZE, true); printf(\u0026#34;Time using cudaMalloc: %3.1fms\\n\u0026#34;, elapsedTime); printf(\u0026#34;\\tMB/s during copy up: %3.1f\\n\u0026#34;, MB / (elapsedTime / 1000)); //要执行相反方向的性能，可以执行相同的调用，只需要将第二个参数指定为false elapsedTime = cuda_malloc_test(SIZI, false); printf(\u0026#34;Time using cudaMalloc: %3.1f ms \\n\u0026#34;, elapsedTime); printf(\u0026#34;\\tMB/s during copy down: %3.1f\\n\u0026#34;, MB / (elapsedTime / 1000)); //测试cudaHostAlloc()的性能 elapsedTime = cuda_host_malloc_test(SIZE, true); printf(\u0026#34;Time using cudaHostMalloc: %3.1fms\\n\u0026#34;, elapsedTime); printf(\u0026#34;\\tMB/s during copy up: %3.1f\\n\u0026#34;, MB / (elapsedTime / 1000)); elapsedTime = cuda_host_malloc_test(SIZI, false); printf(\u0026#34;Time using cudaHostMalloc: %3.1f ms \\n\u0026#34;, elapsedTime); printf(\u0026#34;\\tMB/s during copy down: %3.1f\\n\u0026#34;, MB / (elapsedTime / 1000)); } CUDA流 之前介绍过cudaEventRecord()，并没有详细解释这个函数的第二个参数，这个第二个参数是用于指定插入事件的流(Stream)。CUDA流表示一个GPU操作队列，并且该队列中的操作将以指定的顺序执行。我们可以在流中添加一些操作，例如核函数启动、内存复制，以及时间的启动和结束等。你可以将每个流视为GPU上的一个任务，并且这些任务可以并行执行。我们将首先介绍如何使用流，然后介绍如何使用流来加速应用程序。\n使用单个CUDA流 仅当使用多个流时才能显现出流的真正威力。不过我们先用一个流来说明用法。下面的示例中，我们将计算a中三个值和b中三个值的平均值。\n#include \u0026#34;../common/book.h\u0026#34; #define N (1024 * 1024) #define FULL_DATA_SIZE (N * 20) __global__ void kernel(int *a, int *b, int *c){ int idx = threadIdx.x + blockIdx.x * blockDim.x; if(idx \u0026lt; N){ int idx1 = (idx + 1) % 256; int idx2 = (idx + 2) % 256; float as = (a[idx] + a[idx1] + a[idx2]) / 3.0f; float bs = (b[idx] + b[idx1] + b[idx2]) / 3.0f; c[idx] = (as + bs) / 2; } } int main(){ //选择一个一个支持设备重叠功能的设备。支持设备重叠功能的GPU能够在执行一个CUDA C核函数的同时， //还能在设备与主机之间执行复制操作。\tcudaDeviceProp prop; int whichDevice; HANDLE_ERROR(cudaGetDevice(\u0026amp;whichDevice)); HANDLE_ERROR(cudaGetDeviceProperties(\u0026amp;prop, whichDevice)); if(!prop.deviceOverlap){ printf(\u0026#34;Device will not handle overlaps, so no speed up from streams\\n\u0026#34;); } cudaEvent_t start, stop; float elapsedTime; //启动计时器 HANDLE_ERROR(cudaEventCreate(\u0026amp;start)); HANDLE_ERROR(cudaEventCreate(\u0026amp;stop)); HANDLE_ERROR(cudaEventRecord(start, 0)); //初始化流 cudaStream_t stream; HANDLE_ERROR(cudaStreamCreate(\u0026amp;stream)); //数据分配操作 int *host_a, *host_b, *host_c; int *dev_a, *dev_b, *dev_c; //在GPU上分配内存 HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_a, N * sizeof(int))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_b, N * sizeof(int))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_c, N * sizeof(int))); //分配由流使用的页锁定内存 HANDLE_ERROR(cudaHostAlloc((void**)\u0026amp;host_a, FULL_DATA_SIZE * sizeof(int), cudaHostAllocDefault)); HANDLE_ERROR(cudaHostAlloc((void**)\u0026amp;host_b, FULL_DATA_SIZE * sizeof(int), cudaHostAllocDefault)); HANDLE_ERROR(cudaHostAlloc((void**)\u0026amp;host_c, FULL_DATA_SIZE * sizeof(int), cudaHostAllocDefault)); for(int i = 0; i \u0026lt; FULL_DATA_SIZE; i++){ host_a[i] = rand(); host_b[i] = rand(); } 程序将使用主机上的固定内存。我们还会使用一种新的cudaMemcpy()函数，并且在这个新函数中需要页锁定主机内存。在分配完输入内存后，调用C的库函数rand()并用随机证书填充主机内存。\n执行计算的方式是将两个输入缓冲区复制到GPU，启动核函数，然后将输出缓冲区复制回主机。不过，本示例做出了小的调整。首先，我们不将输入缓冲区整体都复制到GPU，而是将输入缓冲区划分为更小的块，并在每个块上执行一个包含三个步骤的过程。我们将一部分输入缓冲区复制到GPU，在这部分缓冲区上运行核函数，然后将输出缓冲区的这部分结果复制回主机。下面给出了一个需要这种方法的情形：GPU的内存远少于主机内存，由于整个缓冲区无法一次性填充到GPU，因此需要分块进行计算。执行“分块”计算的代码如下所示：\n//在整体数据上循环，每个数据块的大小为N for(int i = 0; i \u0026lt; FULL_DATA_SIZE; i += N){ //将锁定内存以异步的方式复制到设备上 HANDLE_ERROR(cudaMemcpyAsync(dev_a, host_a + i, N * sizeof(int), cudaMemcpyHostToDevice, stream)); HANDLE_ERROR(cudaMemcpyAsync(dev_b, host_b + i, N * sizeof(int), cudaMemcpyHostToDevice, stream)); kernel\u0026lt;\u0026lt;\u0026lt;N / 256, 256, 0, stream\u0026gt;\u0026gt;\u0026gt;(dev_a, dev_b, dev_c); //将数据从设备复制到锁定内存 HANDLE_ERROR(cudaMemcpyAsync(host_c + i, dev_c, N * sizeof(int), cudaMemcpyDeviceToHost. stream)); } 上述代码使用cudaMemcpyAsync()在GPU与主机之间复制数据。cudaMemcpy()的行为类似于C库函数memcpy()，尤其是这个函数将以同步的方式进行。这意味着当函数返回时，复制操作就已经完成，并且在输出缓冲区中包含了复制进去的内容。异步函数的行为与同步函数相反，在调用cudaMemcpyAsync()时，只是放置一个请求，表示在流中执行一次内存复制操作，这个流时通过参数stream来指定的。当函数返回时，我们无法确保复制操作是否已经启动，更无法保证它是否已经结束。**我们能够得到的保证是，复制操作肯定会将肯定会当下一个被放入流中的操作之前执行。**任何传递给cudaMemcpyAsync()的主机内存指针都必须已经通过cudaHostAlloc()分配好内存。也就是，你只能以异步方式对页锁定内存进行复制操作。\n在核函数调用的尖括号中还可以带有一个流参数。此时核函数调用将是异步的。从技术上来说，当循环迭代完一次时，有可能不会启动任何内存复制或核函数执行。我们能够确保的是，第一次放入流中的复制操作将在第二次复制操作之前执行。此外，第二个复制操作将在核函数启动之前完成。而核函数将在第三次复制操作开始之前完成。\n当for循环结束时，在队列中应该包含了许多等待GPU执行的工作。如果想要确保GPU执行完了计算和内存复制等操作，那么就需要将GPU与主机同步。也就是说，主机在继续执行之前，要首先等待GPU执行完成。可以调用cudaStreamSynchronize()并制定想要等待的流。当程序执行到stream与主机同步之后的代码时，所有的计算和复制操作都已经完成，因此停止计时器，收集性能数据，并释放输入缓冲区和输出缓冲区。\n//将计算结果从页锁定内存复制到主机内存 HANDLE_ERROR(cudaStreamSynchronize(stream)); HANDLE_ERROR(cudaEventRecord(stop, 0)); HANDLE_ERROR(cudaEventSynchronize(stop)); HANDLE_ERROR(cudaEventElapsedTime(\u0026amp;elapsedTime, start, stop)); printf(\u0026#34;Time taken: 3.1%f ms\\n\u0026#34;, elapsedTime); //释放流和内存 HANDLE_ERROR(cudaFreeHost(host_a)); HANDLE_ERROR(cudaFreeHost(host_b)); HANDLE_ERROR(cudaFreeHost(host_c)); HANDLE_ERROR(cudaFree(dev_a)); HANDLE_ERROR(cudaFree(dev_b)); HANDLE_ERROR(cudaFree(dev_c)); //销毁对GPU操作进行排队的流 HANDLE_ERROR(cudaStreamDestroy(stream)); } 使用多个CUDA流 我们将上面的示例改为使用两个不同的流。我们将实现：在第0个流执行核函数的同时，第1个流将输入缓冲区复制到GPU。然后在第0个流将计算结果复制回主机的同时，第1个流将执行核函数。接着，第1个流将计算结果复制回主机，同时第0个流开始在下一块数据上执行核函数。假设内存复制操作和核函数执行的事件大致相同，那么应用程序的执行时间线将如图下所示（后续图片中函数调用cudaMemcpyAsync()被简写为复制）：\n核函数的代码保持不变。与使用单个流的版本一样，我们将判断设备是否支持计算与内存复制操作的重叠。如果设备支持重叠，那么就像前面一样创建CUDA事件并对应用程序计时。创建两个流的方式与之前代码中创建单个流的方式是一样的。\n#include \u0026#34;../common/book.h\u0026#34; #define N (1024 * 1024) #define FULL_DATA_SIZE (N * 20) __global__ void kernel(int *a, int *b, int *c){ int idx = threadIdx.x + blockIdx.x * blockDim.x; if(idx \u0026lt; N){ int idx1 = (idx + 1) % 256; int idx2 = (idx + 2) % 256; float as = (a[idx] + a[idx1] + a[idx2]) / 3.0f; float bs = (b[idx] + b[idx1] + b[idx2]) / 3.0f; c[idx] = (as + bs) / 2; } } int main(){ cudaDeviceProp prop; int whichDevice; HANDLE_ERROR(cudaGetDevice(\u0026amp;whichDevice)); HANDLE_ERROR(cudaGetDeviceProperties(\u0026amp;prop, whichDevice)); if(!prop.deviceOverlap){ printf(\u0026#34;Device will not handle overlaps, so no speed up from streams\\n\u0026#34;); } cudaEvent_t start, stop; float elapsedTime; //启动计时器 HANDLE_ERROR(cudaEventCreate(\u0026amp;start)); HANDLE_ERROR(cudaEventCreate(\u0026amp;stop)); HANDLE_ERROR(cudaEventRecord(start, 0)); //初始化流 cudaStream_t stream0, stream1; HANDLE_ERROR(cudaStreamCreate(\u0026amp;stream0)); HANDLE_ERROR(cudaStreamCreate(\u0026amp;stream1)); //数据分配操作 int *host_a, *host_b, *host_c; int *dev_a0, *dev_b0, *dev_c0; //为第0个流分配的GPU内存 int *dev_a1, *dev_b1, *dev_c1; //为第1个流分配的GPU内存 //在GPU上分配内存 HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_a0, N * sizeof(int))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_b0, N * sizeof(int))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_c0, N * sizeof(int))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_a1, N * sizeof(int))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_b1, N * sizeof(int))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_c1, N * sizeof(int))); //分配由流使用的页锁定内存 HANDLE_ERROR(cudaHostAlloc((void**)\u0026amp;host_a, FULL_DATA_SIZE * sizeof(int), cudaHostAllocDefault)); HANDLE_ERROR(cudaHostAlloc((void**)\u0026amp;host_b, FULL_DATA_SIZE * sizeof(int), cudaHostAllocDefault)); HANDLE_ERROR(cudaHostAlloc((void**)\u0026amp;host_c, FULL_DATA_SIZE * sizeof(int), cudaHostAllocDefault)); for(int i = 0; i \u0026lt; FULL_DATA_SIZE; i++){ host_a[i] = rand(); host_b[i] = rand(); } 之后，程序在输入数据块上循环。然而，由于现在使用了两个流，因此在for()循环的迭代中需要处理的数据量也是原来的两倍。在stream()中，我们首先将a和b的异步复制操作放入GPU的队列，然后将一个核函数执行放入队列，接下来再将一个复制回c的操作放入队列：\n//在整体数据上循环，每个数据块的大小为N for(int i = 0; i \u0026lt; FULL_DATA_SIZE; i += N * 2){ //将锁定内存以异步方式复制到设备上 HANDLE_ERROR(cudaMemcpyAsync(dev_a0, host_a + i, N * sizeof(int), cudaMemcpyHostToDevice, stream0)); HANDLE_ERROR(cudaMemcpyAsync(dev_b0, host_b + i, N * sizeof(int), cudaMemcpyHostToDevice, stream0)); kernel\u0026lt;\u0026lt;\u0026lt;N / 256, 256, 0, stream0\u0026gt;\u0026gt;\u0026gt;(dev_a0, dev_b0, dev_c0); //将数据从设备复制回锁定内存 HANDLE_ERROR(cudaMemcpyAsync(host_c + i, dev_c0, N * sizeof(int), cudaMemcpyDeviceToHost, stream0)); //在将这些操作放入stream0队列后，再把下一个数据块上的相同操作放入stream1的队列中 //将锁定内存以异步方式复制到设备上 HANDLE_ERROR(cudaMemcpyAsync(dev_a1, host_a + i + N, N * sizeof(int), cudaMemcpyHostToDevice, stream1)); HANDLE_ERROR(cudaMemcpyAsync(dev_b1, host_b + i + N, N * sizeof(int), cudaMemcpyHostToDevice, stream1)); kernel\u0026lt;\u0026lt;\u0026lt;N / 256, 256, 0, stream1\u0026gt;\u0026gt;\u0026gt;(dev_a1, dev_b1, dev_c1); //将数据从设备复制回锁定内存 HANDLE_ERROR(cudaMemcpyAsync(host_c + i + N, dev_c1, N * sizeof(int), cudaMemcpyDeviceToHost, stream0)); } 这样，在for循环的迭代过程中，将交替地把每个数据块放入这两个流的队列，直到所有待处理的输入数据都被放入队列。在结束了for循环后，在停止应用程序的计时器之前，首先将GPU与GPU进行同步，由于使用了两个流，因此要对二者都进行同步。\nHANDLE_ERROR(cudaStreamSynchronize(stream0)); HANDLE_ERROR(cudaStreamSynchronize(stream1)); HANDLE_ERROR(cudaEventRecord(stop, 0)); HANDLE_ERROR(cudaEventSynchronize(stop)); HANDLE_ERROR(cudaEventElapsedTime(\u0026amp;elapsedTime, start, stop)); printf(\u0026#34;Time taken: 3.1%f ms\\n\u0026#34;, elapsedTime); //释放流和内存 HANDLE_ERROR(cudaFreeHost(host_a)); HANDLE_ERROR(cudaFreeHost(host_b)); HANDLE_ERROR(cudaFreeHost(host_c)); //销毁两个流，释放两倍的GPU内存 HANDLE_ERROR(cudaFree(dev_a0)); HANDLE_ERROR(cudaFree(dev_b0)); HANDLE_ERROR(cudaFree(dev_c0)); HANDLE_ERROR(cudaFree(dev_a1)); HANDLE_ERROR(cudaFree(dev_b1)); HANDLE_ERROR(cudaFree(dev_c1)); //销毁对GPU操作进行排队的流 HANDLE_ERROR(cudaStreamDestroy(stream0)); HANDLE_ERROR(cudaStreamDestroy(stream1)); } GPU的工作调度机制 我们可以将流视为有序的操作序列，其中既包含内存复制操作，又包含核函数调用。下图将展示任务调度情形。\n从图中得到，第0个流对A的内存复制需要在对B的内存复制之前完成，而对B的复制又要在核函数A启动之前完成。然而，一旦这些操作放入到硬件的内存复制引擎和核函数执行引擎的队列中时，这些依赖性将丢失，因此CUDA驱动程序需要确保硬件的执行单元不破坏流内部的依赖性。\n从之前的代码中可以得知，应用程序基本上是对a调用一次cudaMemcpyAsync()，对b调用一次cudaMemcpyAsync()，然后再是执行核函数以及调用cudaMemcpyAsync()将c复制回主机。应用程序首先将第0个流的所有操作放入队列，然后是第1个流的所有操作。CUDA驱动程序负责按照这些操作的顺序把他们调度到硬件上执行，这就维持了流内部的依赖性。下图体现了这些依赖性，其中从复制操作到核函数的箭头表示，复制操作要等核函数执行完成之后才能开始。\n假定理解了GPU的工作调度远离后，我们可以得到关于这些操作在硬件上执行的时间线，如下图所示：\n由于第0个流中将c复制回主机的操作要等待核函数执行完成，因此第1个流中将a和b复制到GPU的操作虽然是完全独立的，但却被阻塞了，这是因为GPU引擎是按照指定的顺序来执行工作。这种情况很好地说明了为什么在程序中使用了两个流却无法获得加速的窘境。这个问题的直接原因是我们没有意识到硬件的工作方式与CUDA流编程模型的方式是不同的。\n高效地使用多个CUDA流 从上面的说明可以得出，如果同时调度某个流的所有操作，那么很容易在无意中阻塞另一个流的复制操作或者核函数执行。要解决这个问题，在将操作放入流的队列时应采用宽度优先的方式，而非深度优先的方式。也就是说不是首先添加第0个流的所有四个操作（即a的复制、b的复制、核函数以及c的复制），然后不再添加第1个流的所有四个操作，而是将这两个流之间的操作交叉添加。\n首先，将a的复制操作添加到第0个流，然后将a的复制操作添加到第1个流。接着，将b的复制操作添加到第0个流，再将b的复制操作添加到第1个流。接下来，将核函数调用添加到第0个流，再将相同的操作添加到第1个流中。最后，将c的复制操作添加到第0个流中，然后将相同的操作添加到第1个流中。\n下面是实际的代码。我们的修改仅限于for循环中的两个流的处理，采用宽度优先方式将操作分配到两个流的代码如下：\nfor(int i = 0; i \u0026lt; FULL_DATA_SIZE; i += N * 2){ //将复制a的操作放入stream0和stream1的队列 HANDLE_ERROR(cudaMemcpyAsync(dev_a0, host_a + i, N * sizeof(int), cudaMemcpyHostToDevice, stream0)); HANDLE_ERROR(cudaMemcpyAsync(dev_a1, host_a + i + N, N * sizeof(int), cudaMemcpyHostToDevice, stream1)); //将复制b的操作放入stream0和stream1的队列 HANDLE_ERROR(cudaMemcpyAsync(dev_b0, host_b + i, N * sizeof(int), cudaMemcpyHostToDevice, stream0)); HANDLE_ERROR(cudaMemcpyAsync(dev_b1, host_b + i + N, N * sizeof(int), cudaMemcpyHostToDevice, stream1)); //将核函数的执行放入stream0和stream1的队列中 kernel\u0026lt;\u0026lt;\u0026lt;N / 256, 256, 0, stream0\u0026gt;\u0026gt;\u0026gt;(dev_a0, dev_b0, dev_c0); kernel\u0026lt;\u0026lt;\u0026lt;N / 256, 256, 0, stream1\u0026gt;\u0026gt;\u0026gt;(dev_a1, dev_b1, dev_c1); //将复制c的操作放入stream0和stream1的队列 HANDLE_ERROR(cudaMemcpyAsync(host_c + i, dev_c0, N * sizeof(int), cudaMemcpyDeviceToHost, stream0)); HANDLE_ERROR(cudaMemcpyAsync(host_c + i + N, dev_c1, N * sizeof(int), cudaMemcpyDeviceToHost, stream1)); } 此时，新的执行时间线将如下图所示：\n","date":"February 28, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E4%B8%83/","series":[],"smallImg":"","tags":[{"title":"CUDA","url":"/tags/cuda/"}],"timestamp":1709078400,"title":"CUDA学习(七)"},{"authors":[],"categories":[{"title":"Tutorial","url":"/categories/tutorial/"}],"content":"在某些情况中，对于单线程应用程序来说非常简单的任务，在大规模并行架构上实现时会变成一个复杂的问题。在本章中，我们将举例其中一些情况，并在这些情况中安全地完成传统单线程应用程序中的简单任务。\n原子操作简介 在编写传统的单线程应用程序时，程序员通常不需要使用原子操作。下面会介绍一下原子操作是什么，以及为什么在多线程程序中需要使用它们。我们先分析C或者C++的递增运算符：\nx++;\n在这个操作中包含了三个步骤：\n读取x中的值。 将步骤1中读到的值增加1。 将递增后的结果写回到x。 有时候，这个过程也称为读取-修改-写入操作。\n当两个线程都需要对x的值进行递增时，假设x的初始值为7，理想情况下，两个线程按顺序对x进行递增，第一个线程完成三个步骤后第二个线程紧接着完成三个步骤，最后得到的结果是9。但是，实际情况下会出现两个线程的操作彼此交叉进行，这种情况下得到的结果将小于9(比如两个线程同时读取x=7，计算完后写入，那样的话x最后会等于8)。\n因此，我们需要通过某种方式一次性地执行完读取-修改-写入这三个操作，并在执行过程中不会被其他线程中断。由于这些操作的执行过程不能分解为更小的部分，因此我们将满足这种条件限制的操作称为原子操作。\n计算直方图 本章将通过给出计算直方图的例子来介绍如何进行原子性计算。\n在CPU上计算直方图 某个数据的直方图表示每个元素出现的频率。在示例中，这个数据将是随机生成的字节流。我们可以通过工具函数big_random_block()来生成这个随机的字节流。在应用程序中将生成100MB的随机数据。\n#include \u0026#34;../common/book.h\u0026#34; #define SIZE (100 * 1024 * 1024) int main(){ unsigned char *buffer = (unsigned char*)big_random_block(SIZE); } 由于每个随机字节（8比特）都有256个不同的可能取值（从0x00到0xFF），因此在直方图中需要包含256个元素，每个元素记录相应的值在数据流中的出现次数。我们创建了一个包含256个元素的数组，并将所有元素的值初始化为0。\nunsigned int histo[256]; for(int i = 0; i \u0026lt; 256; i++){ histo[i] = 0; } 接下来需要计算每个值在buffer[]数据中的出现频率。算法思想是，每当在数组buffer[]中出现某个值z时，就递增直方图数组中索引为z的元素，这样就能计算出值z的出现次数。如果当前看到的值为buffer[i]，那么将递增数组中索引等于buffer[i]的元素。由于元素buffer[i]位于histo[buffer[i]]，我们只需一行代码就可以递增相应的计数器。在一个for循环中对buffer[]中的每个元素执行这个操作：\nfor(int i = 0; i \u0026lt; SIZE; i++){ histo[buffer[i]]++; } 接下来将验证直方图的所有元素相加起来是否等于正确的值。\nlong histoCount = 0; for(int i = 0; i \u0026lt; 256; i++){ histoCount += histo[i]; } printf(\u0026#34;Histogram Sum: %ld\\n\u0026#34;, histoCount); free(buffer); 在GPU上计算直方图 以下时计算直方图的GPU版本\nint main(){ unsigned char* buffer = (unsigned char*)big_random_block(SIZE); //初始化计时事件 cudaEvent_t start, stop; HANDLE_ERROR(cudaEventCreate(\u0026amp;start)); HANDLE_ERROR(cudaEventCreate(\u0026amp;stop)); HANDLE_ERROR(cudaEventRecord(start, 0)); //在GPU上为文件的数据分配内存 unsigned char *dev_buffer; unsigned int *dev_histo; HANDLE_ERROR(cudaMallc((void**)\u0026amp;dev_buffer, SIZE)); HANDLE_ERROR(cudaMemcpy(dev_buffer, buffer, SIZE, cudaMemcpyHostToDevice)); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_histo, 256 * sizeof(int))); HANDLE_ERROR(cudaMemset(dev_histo, 0, 256 * sizeof(int))); unsigned int histo[256]; HANDLE_ERROR(cudaMemcpy(histo, dev_histo, 256 * sizeof(int), cudaMemcpyDeviceToHost)); //得到停止事件并显示计时结果 HANDLE_ERROR(cudaEventRecord(stop, 0)); HANDLE_ERROR(cudaEventSynchronize(stop)); float elapsedTime; HANDLE_ERROR(cudaEventElapsedTime(\u0026amp;elapsedTime, start, stop)); printf(\u0026#34;Time to generate: %3.1f ms\\n\u0026#34;, elapsedTime); //下面是验证直方图的总和是否等于正确的值，因为是在CPU上运行，并不需要对此进行计时 long histoCount = 0; for(int i = 0; i \u0026lt; 256; i++){ histoCount += histo[i]; } printf(\u0026#34;Histogram Sum: %ld\\n\u0026#34;, histoCount); //验证GPU与CPU的搭配的是相同的计数值 for(int i = 0; i \u0026lt; SIZE; i++){ histo[buffer[i]]--; } for(int i = 0; i \u0026lt; 256; i++){ if(histo[i] != 0){ printf(\u0026#34;Failure at %d!\\n\u0026#34;, i); } } HANDLE_ERROR(cudaEventDestroy(start)); HANDLE_ERROR(cudaEventDestroy(stop)); cudaFree(dev_histo); cudaFree(dev_buffer); free(buffer); } cudaMemset()与C的memset()是相似的，不同之处在于cudaMemset()将返回一个错误码，将高速调用者在设置GPU内存时发生的错误。\n接下来会介绍GPU上计算直方图的代码。计算直方图的核函数需要的参数包括：\n一个指向输入数组的指针 输入数组的长度 一个指向输出直方图的指针 核函数执行的第一个计算就是计算输入数据数组中的偏移。每个线程的起始偏移都是0到线程数量减1之间的某个值，然后，对偏移的增量为已启动线程的总数。\n#include \u0026#34;../common/book.h\u0026#34; #define SIZE (100 * 1024 * 1024) __global__ void histo_kernel(unsigned char *buffer, long size, unsigned int *histo){ int i = threadIdx.x + blockIdx.x * blockDim.x; int stride = blockDim.x * gridDim.x; //当每个线程知道它的起始偏移i以及递增的数量，这段代码将遍历输入数组，并递增直方图中相应元素的值 while(i \u0026lt; size){ atomicAdd(\u0026amp;(histo[buffer[i]]), 1); i += stride; } } 函数调用atomicAdd(addr, y)将生成一个原子的操作序列，这个操作序列包括读取地址addr处的值，将y增加到这个值，以及将结果保存回地址addr。底层硬件将确保当执行这些操作时，其他任何线程都不会读取或写入地址addr上的值。\n然而，原子操作回导致性能降低，但是解决问题的方法有时会需要更多而非更少的原子操作。**这里的主要问题并非在于使用了过多的原子操作，而是有数千个线程在少量的内存地址上发生竞争。**要解决这个问题，我们需要将直方图计算分为两个阶段。\n第一个阶段，每个并行线程块将计算它所处理数据的直方图。每个线程块在执行这个操作时都是相互独立的，因此可以在共享内存中计算这些直方图，这将避免每次将写入操作从芯片发送到DRAM。但是这种方式仍然需要原子操作，因为线程块中的多个线程之间仍然会处理相同值的数据元素。**不过，现在只有256个线程在256个地址上发生竞争，这将极大地减少在使用全局内存时在数千个线程之间发生竞争的情况。**我们将使用共享内存缓冲区temp[]而不是全局内存缓冲区histo[]，而且需要随后调用__syncthreads()来确保提交最后的写入操作。 第二个阶段则是将每个线程块的临时直方图合并到全局缓冲区histo[]中。由于我们使用了256个线程，并且直方图中包含了256个元素，因此每个线程将自动把它计算得到的元素只增加到最终直方图的元素上（如果线程数量不等于元素数量，那么这个阶段将更为复杂）。我们并不保证线程块将按照何种顺序将各自的值相加到最终直方图中，但由于整数加法时可交换的，无论哪种顺序都会得到相同的结果。 __global__ void histo_kernel(unsigned char* buffer, long size, unsigned int *histo){ __shared__ unsigned int temp[256]; temp[threadIdx.x] = 0; __syncthreads(); int i = threadIdx.x + blockIdx.x * blockDim.x; int stride = blockDim.x * gridDim.x; while(i \u0026lt; size){ atomicAdd(\u0026amp;temp[buffer[i]], 1); i += offset; } __syncthreads(); atomicAdd(*(histo[threadIdx.x]), temp[threadIdx.x]); } ","date":"February 27, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E5%85%AD/","series":[],"smallImg":"","tags":[{"title":"CUDA","url":"/tags/cuda/"}],"timestamp":1708992000,"title":"CUDA学习(六)"},{"authors":[],"categories":[{"title":"Tutorial","url":"/categories/tutorial/"}],"content":"本章将学习如何分配和使用纹理内存(texture memory)。和常量内存一样，纹理内存是另一种类型的只读内存，在特定的访问模式中，纹理内存同样能够提升性能并减少内存流量。虽然纹理内存最初是针对传统的图形处理应用程序而设计的，但在某些GPU计算应用程序中同样非常有用。\n与常量内存相似的是，纹理内存同样缓存在芯片上，因此在某些情况中，它能够减少对内存请求并提供更高效的内存带宽。纹理缓存是专门为那些在内存访问模式中存在大量空间局部性(spatial locality)的图形应用程序而设计的。在某个计算应用程序中，这意味着一个线程读取的位置可能与邻近线程读取的位置“非常接近”。\n热传导模拟 本章用一个简单的热传导模拟的模型来介绍如何使用纹理内存。\n简单的传热模型 我们构造一个简单的二维热传导模拟。首先假设有一个矩形房间，将其分成一个格网，每个格网中随机散步一些“热源”，他们有着不同的温度。\n在给定了矩形格网以及热源分布后，我们可以计算格网中每个单元格的温度随时间的变化情况。为了简单，热源单元本身的温度将保持不变。在时间递进的每个步骤中，我们假设热量在某个单元及其邻接单元之间“流动”。如果某个单元的临界单元的温度更高，那么热量将从邻接单元传导到该单元，相反地，如果某个单元的温度比邻接单元的温度高，那么它将变冷。\n在热传导模型中，我们对单元中新温度的计算方法为，将单元与邻接单元的温差相加起来，然后加上原有温度。 $$ T_{NEW} = T_{OLD} + \\sum_{NEIGHBOURS}k(T_{NEIGHBORS} - T_{OLD}) $$ 在上面的计算单元温度的等式中，常量k表示模拟过程中热量的流动速率，k值越大，表示系统会更快地达到稳定温度，而k值越小，则温度梯度将存在更长时间。由于我们只考虑4个邻接单元(上、下、左、右)并且等式中的k和$$T_{OLD}$$都是常数，因此把上述公式展开表示为: $$ T_{NEW} = T_{OLD} + k(T_{TOP} + T_{BOTTOM} + T_{LEFT} + T_{RIGHT} - 4T_{OLD}) $$\n温度更新的计算 以下是更新流程的基本介绍:\n给定一个包含初始输入温度的格网，将其中作为热源的单元温度值复制到格网相应的单元中来覆盖这些单元之前计算出来的温度，确保“加热单元将保持恒温”的条件。这个复制操作在copy_const_kernel()中执行。 给定一个输入温度格网，根据新的公式计算出输出温度格网。这个更新操作在blend_kernel()中执行。 将输入温度格网和输出温度格网交换，为下一个步骤的计算做好准备。当模拟下一个时间步时，在步骤2中计算得到的输出温度格网将成为步骤1中的输入温度格网。 下面是两个函数的具体实现:\n__global__ void copy_const_kernel(float *iptr, const float *cptr){ //将threadIdx/BlockIdx映射到像素位置 int x = threadIdx.x + blockIdx.x * blockDim.x; int y = threadIdx.y + blockIdx.y * blockDim.y; int offset = x + y * blockDim.x * gridDim.x; if(cptr[offset] != 0){ iptr[offset] = cptr[offset]; } } //为了执行更新操作，可以在模拟过程中让每个线程都负责计算一个单元。 //每个线程都将读取对应单元及其邻接单元的温度值，执行更新运算，然后计算得到新值来更新温度。 __global__ void blend_kernel(float *outSrc, const float *inSrc){ //将threadIdx/BlockIdx映射到像素位置 int x = threadIdx.x + blockIdx.x * blockDim.x; int y = threadIdx.y + blockIdx.y * blockDim.y; int offset = x + y * blockDim.x * gridDim.x; int left = offset - 1; int right = offset + 1; if(x == 0) left++; if(x == DIM - 1) right--; int top = offset - DIM; int bottom = offset + DIM; if(y == 0) bottom += DIM; if(y == DIM - 1) top -= DIM; outSrc[offset] = inSrc[offset] + SPEED * (inSrc[top] + inSrc[bottom] + inSrc[left] + inSrc[right] - inSrc[offset] * 4); } 模拟过程动态演示 剩下的代码主要是设置好单元，然后显示热量的动画输出\n#include \u0026#34;cuda.h\u0026#34; #include \u0026#34;../common/book.h\u0026#34; #include \u0026#34;cpu_anim.h\u0026#34; #define DIM 1024 #define PI 3.1415926535897932f #define MAX_TEMP 1.0f #define MIN_TEMP 0.0001f #define SPEED 0.25f //更新函数中需要的全局变量 struct DataBlock{ unsigned char *output_bitmap; float *dev_inSrc; float *dev_outSrc; float *dev_constSrc; CPUAnimBitmap *bitmap; cudaEvent_t start, stop; float totalTime; float frames; } void anim_gpu(DataBlock *d, int ticks){ HANDLE_ERROR(cudaEventRecord(d-\u0026gt;start, 0)); dim3 blocks(DIM / 16, DIM / 16); dim3 threads(16, 16); CPUAnimBitmap *bitmap = d-\u0026gt;bitmap; for(int i = 0; i \u0026lt; 90; 0++){ copy_const_kernel\u0026lt;\u0026lt;\u0026lt;blocks, threads\u0026gt;\u0026gt;\u0026gt;(d-\u0026gt;dev_inSrc, d-\u0026gt;dev_constSrc); blend_kernel\u0026lt;\u0026lt;\u0026lt;blocks, threads\u0026gt;\u0026gt;\u0026gt;(d-\u0026gt;dev_inSrc, d-\u0026gt;dev_constSrc); swap(d-\u0026gt;dev_inSrc, d-\u0026gt;dev_outSrc); } float_to_color\u0026lt;\u0026lt;\u0026lt;blocks, threads\u0026gt;\u0026gt;\u0026gt;(d-\u0026gt;output_bitmap, d-\u0026gt;dev_inSrc); HANDLE_ERROR(cudaMemcpy(bitmap-\u0026gt;get_ptr(), d-\u0026gt;output_bitmap, bitmap-\u0026gt;image_size(), cudaMemcpyDeviceToHost)); HANDLE_ERROR(cudaEventRecord(d-\u0026gt;stop, 0)); HANDLE_ERROR(cudaEventSynchronize(d-\u0026gt;stop)); float elapsedTime; HANDLE_ERROR(cudaEventElapsedTime(\u0026amp;elapsedTime, d-\u0026gt;start, d-\u0026gt;stop)); d-\u0026gt;totalTime += elapsedTime; ++d-\u0026gt;frames; printf(\u0026#34;Averaged Time per frame: $3.1f ms \\n\u0026#34;, d-\u0026gt;totalTime / d-\u0026gt;frames); } void anim_exit(DataBlock *d){ cudaFree(d-\u0026gt;dev_inSrc); cudaFree(d-\u0026gt;dev_outSrc); cudaFree(d-\u0026gt;dev_constSrc); HANDLE_ERROR(cudaEventDestroy(d-\u0026gt;start)); HANDLE_ERROR(cudaEventDestroy(d-\u0026gt;stop)); } int main(){ DataBlock data; CPUAnimBitmap bitmap(DIM, DIM, \u0026amp;data); data.bitmap = \u0026amp;bitmap; data.totalTime = 0; data.frames = 0; HANDLE_ERROR(cudaEventCreate(\u0026amp;data.start)); HANDLE_ERROR(cudaEventCreate(\u0026amp;data.stop)); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;data.output_bitmap, bitmap.image_size())); //假设float类型的大小为4个字符(即rgba) HANDLE_ERROR(cudaMalloc((void**)\u0026amp;data.dev_inSrc, bitmap.image_size())); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;data.dev_outSrc, bitmap.image_size())); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;data.dev_constSrc, bitmap.image_size())); float *temp = (float*)malloc(bitmap.image_size()); for(int i = 0; i \u0026lt; DIM*DIM; i++){ temp[i] = 0; int x = i % DIM; int y =i / DIM; if((x \u0026gt; 300) \u0026amp;\u0026amp; (x \u0026lt; 600) \u0026amp;\u0026amp; (y \u0026gt; 310) \u0026amp;\u0026amp; (y \u0026lt; 601)){ temp[i] = MAX_TEMP; } temp[DIM * 100 + 100] = (MAX_TEMP + MIN_TEMP) / 2; temp[DIM * 700 + 100] = MIN_TEMP; temp[DIM * 300 + 300] = MIN_TEMP; temp[DIM * 200 + 700] = MIN_TEMP; for(int y = 800; y \u0026lt; 900; y++){ for(int x = 400; x \u0026lt; 500; x++){ temp[x * y * DIM] = MIN_TEMP; } } HANDLE_ERROR(cudaMemcpy(data.dev_constSrc, temp, bitmap.image_size(), cudaMemcpyHostToDevice)); free(temp); bitmap.anim_and_exit((void (*) (void*, int)) anim_gpu, (void (*) (void*)) anim_exit); } } 使用纹理内存 如果要使用纹理内存，首先要将输入的数据声明为texture类型的引用。我们使用浮点类型纹理的引用，因为温度数值是浮点类型。\n//这些变量将位于GPU上 texture\u0026lt;float\u0026gt; texConstSrc; texture\u0026lt;float\u0026gt; textIn; texture\u0026lt;float\u0026gt; textOut; 下一个需要注意的问题是，在为这三个缓冲区分配了GPU内存后，需要通过cudaBindTexture()将这些变量绑定到内存缓冲区。这相当于告诉CUDA运行时两件事情：\n我们希望将制定的缓冲区作为纹理来使用。 我们希望将纹理引用作为纹理的“名字”。 在热传导模拟中分配了这三个内存后，需要将这三个内存绑定到之前声明的纹理引用(texConstSrc, textIn, textOut)。\nHANDLE_ERROR(cudaMalloc((void**)\u0026amp;data.dev_inSrc, imageSize)); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;data.dev_outSrc, imageSize)); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;data.dev_constSrc, imageSize)); HANDLE_ERROR(cudaBindTexture(NULL, textConstSrc, data.dev_constSrc, imageSize)); HANDLE_ERROR(cudaBindTexture(NULL, textIn, data.dev_inSrc, imageSize)); HANDLE_ERROR(cudaBindTexture(NULL, textOut, data.dev_outStc, imageSize)); 此时，纹理变量已经设置好，可以启动核函数。然而，当读取核函数中的纹理时，需要通过特殊的函数来告诉GPU将读取请求转发到纹理内存而不是标准的全局内存。因此，当读取内存时不再使用方括号从缓冲区读取，而是将blend_kernel()函数内修改为使用tex1Dfetch()。\ntex1Dfetch()实际上是一个编译器内置函数。由于纹理引用必须声明为文件作用域内的全局变量，因此我们不再将输入缓冲区和输出缓冲区作为参数传递给blend_kernel()，因为编译器需要在编译时知道text1Dfetch()应该对哪些纹理采样。我们需要将一个布尔标志dstOut传递给blend_kernel()，这个标志会告诉我们使用那个缓冲区作为输入，以及哪个缓冲区作为输出。以下是对blend_kernel()的修改。\n__global__ void blend_kernel(float *dst, bool dstOut){ //将threadIdx/BlockIdx映射到像素位置 int x = threadIdx.x + blockIdx.x * blockDim.x; int y = threadIdx.y + blockIdx.y * blockDim.y; int offset = x + y * blockDim.x * gridDim.x; int left = offset - 1; int right = offset + 1; if(x == 0) left++; if(x == DIM - 1) right--; int top = offset - DIM; int bottom = offset + DIM; if(y == 0) bottom += DIM; if(y == DIM - 1) top -= DIM; //替换该行代码 // outSrc[offset] = inSrc[offset] + SPEED * (inSrc[top] + inSrc[bottom] + inSrc[left] + inSrc[right] - inSrc[offset] * 4); float t, l, c, r, b; if(dstOut){ t = text1Dfetch(textIn, top); l = text1Dfetch(textIn, left); c = text1Dfetch(textIn, offset); r = text1Dfetch(textIn, right); b = text1Dfetch(textIn, bottom); } else{ t = text1Dfetch(textOut, top); l = text1Dfetch(textOut, left); c = text1Dfetch(textOut, offset); r = text1Dfetch(textOut, right); b = text1Dfetch(textOut, bottom); } dst[offset] = c + SPEED * (t + b + r + l - 4 * c); } 由于核函数copy_const_kernel()将读取包含热源位置和温度的缓冲区，因此同样需要修改为从纹理内存而不是从全局内存中读取：\n__global__ void copy_const_kernel(float *iptr){ //将threadIdx/BlockIdx映射到像素位置 int x = threadIdx.x + blockIdx.x * blockDim.x; int y = threadIdx.y + blockIdx.y * blockDim.y; int offset = x + y * blockDim.x * gridDim.x; float c = text1Dfetch(textConstSrc, offset); if(c != 0){ iptr[offset] = c; } } 由于blend_kernel()的函数原型被修改为接收一个标志，并且这个标志表示在输入缓冲区与输出缓冲区之间的切换，因此需要对anim_gpu()函数进行相应的修改。现在，不是交换缓冲区，而是在每组调用之后通过设置dstOut = !dstOut来进行切换：\nvoid anim_gpu(DataBlock *d, int ticks){ HANDLE_ERROR(cudaEventRecord(d-\u0026gt;start, 0)); dim3 blocks(DIM / 16, DIM / 16); dim3 threads(16, 16); CPUAnimBitmap *bitmap = d-\u0026gt;bitmap; //for(int i = 0; i \u0026lt; 90; 0++){ // copy_const_kernel\u0026lt;\u0026lt;\u0026lt;blocks, threads\u0026gt;\u0026gt;\u0026gt;(d-\u0026gt;dev_inSrc, d-\u0026gt;dev_constSrc); // blend_kernel\u0026lt;\u0026lt;\u0026lt;blocks, threads\u0026gt;\u0026gt;\u0026gt;(d-\u0026gt;dev_inSrc, d-\u0026gt;dev_constSrc); // swap(d-\u0026gt;dev_inSrc, d-\u0026gt;dev_outSrc); //} //float_to_color\u0026lt;\u0026lt;\u0026lt;blocks, threads\u0026gt;\u0026gt;\u0026gt;(d-\u0026gt;output_bitmap, d-\u0026gt;dev_inSrc); //由于tex是全局并且有界的，因此我们必须通过一个标志来选择每次迭代中哪个是输入/输出 volatile bool dstOut = true; for(int i = 0; i \u0026lt; 90; i++){ float *in, *out; if(dstOut){ in = d-\u0026gt;dev_inSrc; out = d-\u0026gt;dev_outSrc; } else{ out = d-\u0026gt;dev_inSrc; in = d-\u0026gt;dev_outSrc; } copy_const_kernel\u0026lt;\u0026lt;\u0026lt;blocks, threads\u0026gt;\u0026gt;\u0026gt;(in); blend_kernel\u0026lt;\u0026lt;\u0026lt;blocks, threads\u0026gt;\u0026gt;\u0026gt;(out, dstOut); dstOut = !dstOut; } HANDLE_ERROR(cudaMemcpy(bitmap-\u0026gt;get_ptr(), d-\u0026gt;output_bitmap, bitmap-\u0026gt;image_size(), cudaMemcpyDeviceToHost)); HANDLE_ERROR(cudaEventRecord(d-\u0026gt;stop, 0)); HANDLE_ERROR(cudaEventSynchronize(d-\u0026gt;stop)); float elapsedTime; HANDLE_ERROR(cudaEventElapsedTime(\u0026amp;elapsedTime, d-\u0026gt;start, d-\u0026gt;stop)); d-\u0026gt;totalTime += elapsedTime; ++d-\u0026gt;frames; printf(\u0026#34;Averaged Time per frame: $3.1f ms \\n\u0026#34;, d-\u0026gt;totalTime / d-\u0026gt;frames); } 对热传导函数的最后一个修改就是在应用程序运行结束后的清理工作。不仅要释放全局缓冲区，还需要清楚与纹理的绑定：\nvoid anim_exit(DataBlock *d){ cudaUnbindTexture(textIn); cudaUnbindTexture(textOut); cudaUnbindTexture(texConstSrc); cudaFree(d-\u0026gt;dev_inSrc); cudaFree(d-\u0026gt;dev_outSrc); cudaFree(d-\u0026gt;dev_constSrc); HANDLE_ERROR(cudaEventDestroy(d-\u0026gt;start)); HANDLE_ERROR(cudaEventDestroy(d-\u0026gt;stop)); } 使用二维纹理内存 在多数情况下，二维内存空间是非常有用的。首先，要修改纹理引用的声明。默认的纹理引用都是一维的，因此我们需要增加代表维数的参数2，这表示声明的是一个二维纹理引用。\ntexture\u0026lt;float, 2\u0026gt; texConstSrc; textture\u0026lt;float, 2\u0026gt; texIn; textture\u0026lt;float, 2\u0026gt; texOut; 二维纹理将简化blend_kernel()方法的实现。虽然我们需要将tex1Dfeth()调用修改为text2D()调用，但却不再需要通过线性化offset变量以计算top、left、right和bottom等偏移。当使用二维纹理时，可以直接通过x和y来访问纹理。而且当使用tex2D()时，我们不再需要担心发生溢出问题。如果x或y小于0，那么tex2D()将返回0处的值。同理，如果某个值大于宽度，那么tex2D()将返回位于宽度处的值。这些建华带来的好处之一就是核函数的代码将变得更加简单。\n__global__ void blend_kernel(float *dst, bool dstOut){ //将threadIdx/BlockIdx映射到像素位置 int x = threadIdx.x + blockIdx.x * blockDim.x; int y = threadIdx.y + blockIdx.y * blockDim.y; int offset = x + y * blockDim.x * gridDim.x; float t, l, c, r, b; if(dstOut){ t = text2D(textIn, x, y - 1); l = text2D(textIn, x - 1, y); c = text2D(textIn, x, y); r = text2D(textIn, x + 1, y); b = text2D(textIn, x, y + 1); } else{ t = text2D(textOut, x, y - 1); l = text2D(textOut, x - 1, y); c = text2D(textOut, x, y); r = text2D(textOut, x + 1, y); b = text2D(textOut, x, y + 1); } dst[offset] = c + SPEED * (t + b + r + l - 4 * c); } 我们也需要对copy_const_kernel()中进行相应的修改。与核函数blend_kernel()类似的是，我们不再需要通过offset来访问纹理，而是只需使用x和y来访问热源的常量。\n__global__ void copy_const_kernel(float *iptr){ //将threadIdx/BlockIdx映射到像素位置 int x = threadIdx.x + blockIdx.x * blockDim.x; int y = threadIdx.y + blockIdx.y * blockDim.y; int offset = x + y * blockDim.x * gridDim.x; float c = text2D(textConstSrc, x, y); if(c != 0){ iptr[offset] = c; } } 在main()需要对纹理绑定调用进行修改，并告诉运行时：缓冲区将被视为二维纹理而不是一维纹理：\nint main(){ DataBlock data; CPUAnimBitmap bitmap(DIM, DIM, \u0026amp;data); data.bitmap = \u0026amp;bitmap; data.totalTime = 0; data.frames = 0; HANDLE_ERROR(cudaEventCreate(\u0026amp;data.start)); HANDLE_ERROR(cudaEventCreate(\u0026amp;data.stop)); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;data.output_bitmap, bitmap.image_size())); //假设float类型的大小为4个字符(即rgba) HANDLE_ERROR(cudaMalloc((void**)\u0026amp;data.dev_inSrc, bitmap.image_size())); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;data.dev_outSrc, bitmap.image_size())); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;data.dev_constSrc, bitmap.image_size())); cudaChannelFormatDesc desc = cudaCreateChannelDesc\u0026lt;float\u0026gt;(); HANDLE_ERROR(cudaBindTexture2D(NULL, texConstSrc, data.dev_constSrc, desc, DIM, DIM, sizeof(float * DIM))); HANDLE_ERROR(cudaBindTexture2D(NULL, texin, data.dev_inSrc, desc, DIM, DIM, sizeof(float * DIM))); HANDLE_ERROR(cudaBindTexture2D(NULL, texOut, data.dev_outSrc, desc, DIM, DIM, sizeof(float * DIM))); float *temp = (float*)malloc(bitmap.image_size()); for(int i = 0; i \u0026lt; DIM*DIM; i++){ temp[i] = 0; int x = i % DIM; int y =i / DIM; if((x \u0026gt; 300) \u0026amp;\u0026amp; (x \u0026lt; 600) \u0026amp;\u0026amp; (y \u0026gt; 310) \u0026amp;\u0026amp; (y \u0026lt; 601)){ temp[i] = MAX_TEMP; } temp[DIM * 100 + 100] = (MAX_TEMP + MIN_TEMP) / 2; temp[DIM * 700 + 100] = MIN_TEMP; temp[DIM * 300 + 300] = MIN_TEMP; temp[DIM * 200 + 700] = MIN_TEMP; for(int y = 800; y \u0026lt; 900; y++){ for(int x = 400; x \u0026lt; 500; x++){ temp[x * y * DIM] = MIN_TEMP; } } HANDLE_ERROR(cudaMemcpy(data.dev_constSrc, temp, bitmap.image_size(), cudaMemcpyHostToDevice)); free(temp); bitmap.anim_and_exit((void (*) (void*, int)) anim_gpu, (void (*) (void*)) anim_exit); } } 虽然我们需要通过不同的函数来告诉运行时绑定一维纹理还是二维纹理，但是可以通过同一个函数cudaUnbindTexture()来取消纹理绑定。所以执行释放操作的函数anim_exit()可以保持不变。\n","date":"February 23, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E4%BA%94/","series":[],"smallImg":"","tags":[{"title":"CUDA","url":"/tags/cuda/"}],"timestamp":1708646400,"title":"CUDA学习(五)"},{"authors":[],"categories":[{"title":"Tutorial","url":"/categories/tutorial/"}],"content":"这一章会介绍如何在CUDA C中使用常量内存、了解常量内存的性能特性以及学习如何使用CUDA事件来测量应用程序的性能。\n常量内存 到目前为止，我们知道CUDA C程序中可以使用全局内存和共享内存。但是，CUDA C还支持另一种类型的内存，即常量内存。常量内存用于保存在核函数执行期间不会发生变化的数据。在某些情况下，用常量内存来替换全局内存能有效地减少内存带宽。\n在GPU上实现光线跟踪 我们给出一个简单的光线跟踪应用程序示例来学习。由于OpenGL和DirectX等API都不是专门为了实现光线跟踪而设计的，因此我们必须使用CUDA C来实现基本的光线跟踪器。本示例构造的光线跟踪器非常简单，旨在学习常量内存的使用上(并不能通过示例代码来构建一个功能完备的渲染器)。这个光线跟踪器只支持一组包含球状物体的场景，并且相机被固定在了Z轴，面向原点。此外，示例代码也不支持场景中的任何照明，从而避免二次光线带来的复杂性。代码也不计算照明效果，而只是为每个球面分配一个颜色值，如果它们是可见的，则通过某个预先计算的值对其着色。\n光线跟踪器将从每个像素发射一道光线，并且跟踪到这些光线会命中哪些球面。此外，它还将跟踪每道命中光线的深度。当一道光线穿过多个球面时，只有最接近相机的球面才会被看到。这个代码的光线跟踪器会把相机看不到的球面隐藏起来。\n通过一个数据结构对球面建模，在数据结构中包含了球面的中心坐标(x, y, z)，半径radius，以及颜色值(r, g, b)。\n#define INF 2e10f struct sphere{ float r, g, b; float radius; float x, y, z; __device__ float hit(float ox, float oy, float *n){ float dx = ox - x; float dy = oy - y; if(dx * dx + dy * dy \u0026lt; radius * radius){ float dz = sqrtf(radius * radius - dx * dx - dy * dy); *n = dz / sqrtf(radius * radius); return dz + z; } return -INF; } } 这个结构中定义了一个方法hit(float ox, float oy, float *n)。对于来自(ox, oy)处像素的光线，这个方法将计算光线是否与这个球面相交。如果光线与球面相交，那么这个方法将计算从相机到光线命中球面处的距离。我们需要这个信息，因为当光线命中多个球面时，只有最接近相机的球面才会被看见。\nmain()函数遵循了与前面示例大致相同的代码结构。\n#include \u0026#34;cuda.h\u0026#34; #include \u0026#34;../common/book.h\u0026#34; #include \u0026#34;cpu_bitmap.h\u0026#34; #define rnd(x) (x * rand() / RAND_MAX) #define SPHERES 20 Sphere *s; int main(){ //记录起始时间 cudaEvent_ start, stop; HANDLE_ERROR(cudaEventCreate(\u0026amp;start)); HANDLE_ERROR(cudaEventCreate(\u0026amp;stop)); HANDLE_ERROR(cudaEventRecord(start, 0)); CPUBitmap bitmap(DIM, DIM); unsigned char *dev_bitmap; //在GPU上分配内存以计算输出位图 HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_bitmap, bitmap.image_size())); //为Sphere数据集分配内存 HANDLE_ERROR(cudaMalloc((void**)\u0026amp;s, sizeof(Sphere) * SPHERES)); } 在分配输入数据和输出数据的内存后，我们将随机地生成球面的中心坐标，颜色以及半径。\n//分配临时内存，对其初始化，并复制到GPU上的内存，然后释放临时内存 Sphere *temp_s = (Sphere*) malloc(sizeof(Sphere) * SPHERES); for(int i = 0; i \u0026lt; SPHERES; i++){ temp_s[i].r = rnd(1.0f); temp_s[i].g = rnd(1.0f); temp_s[i].b = rnd(1.0f); temp_s[i].x = rnd(1000.0f) - 500; temp_s[i].y = rnd(1000.0f) - 500; temp_s[i].z = rnd(1000.0f) - 500; temp_s[i].radius = rnd(100.0f) + 20; } 当前，程序将生成一个包含20个球面的随机数组，但这个数量值是通过一个#define宏指定的，因此可以相应的做出调整。\n通过cudaMemcpy()将这个球面数组复制到GPu，然后释放临时缓冲区。\nHANDLE_ERROR(cudaMemcpy(s, temps, sizeof(Sphere) * SPHERES, cudaMemcpyHostToDevice)); free(temp_s); 现在，输入数据位于GPU上，并且我们已经为输出数据分配好了空间，因此可以启动核函数。\n//从球面数据汇总生成一张位图 dim3 grids(DIM / 16, DIM / 16); dim3 threads(16, 16); kernel\u0026lt;\u0026lt;\u0026lt;grids, threads\u0026gt;\u0026gt;\u0026gt;(dev_bitmap); 这个核函数将执行光线跟踪计算并从输入的一组球面中为每个像素计算颜色数据。最后，我们把输出图像从GPU中复制回来，并显示它。我们还要释放所有已经分配但还未释放的内存。\n//将位图从GPU复制回到CPU以显示 HANDLE_ERROR(cudaMemcpy(bitmap.get_ptr(), dev_bitmap, bitmap.image_size(), cudaMemcpyDeviceToHost)); bitmap.display_and_exit(); //释放内存 cudaFree(dev_bitmap); cudaFree(s); 下面的代码将介绍如何实现核函数的光线跟踪算法。每个线程都会为输出影像中的一个像素计算颜色值，因此我们遵循一种惯用的方式，计算每个线程对应的x坐标和y坐标，并且根据这两个坐标来计算输出缓冲区的偏移。此外，我们还将把图像坐标(x, y, z)偏移DIM/2，这样z轴将穿过图像的中心。\n__global__ void kernel(unsigned char *ptr){ //将threadIdx/BlockIdx映射到像素位置 int x = threadIdx.x + blockIdx.x * blockDim.x; int y = threadIdx.y + blockIdx.y * blockDim.y; int offset = x + y * blockDim.x * gridDim.x; float ox = x - DIM / 2; float oy = y - DIM / 2; //由于每条光线都需要判断与球面相交的情况，因此我们现在对球面数组进行迭代，并判断每个球面的命中情况。 float r = 0, g = 0, b = 0; float maxz = -INF; for(int i = 0; i \u0026lt; SPHERES; i++){ float n; float t = s[i].hit(ox, oy, \u0026amp;n); if(t \u0026gt; maxz){ float fscale = n; r = s[i].r * fscale; g = s[i].g * fscale; b = s[i].b * fscale; } } //在判断了每个球面的相交情况后，可以将当前颜色值保存到输出图像中 ptr[offset * 4 + 0] = (int)(r * 255); ptr[offset * 4 + 1] = (int)(g * 255); ptr[offset * 4 + 1] = (int)(b * 255); ptr[offset * 4 + 3] = 255; } 通过常量内存来实现光线跟踪 上述代码中并没有提到常量内存。下面的代码将使用常量内存来修改这个例子。由于常量内存无法修改，因此显然无法用常量内存来保存输出图像的数据。在这个示例中只有一个输入数据，即球面数组，因此应该将这个数据保存到常量内存中。\n声明数组时，要在前面加上__constant__修饰符。\n__constant__ Sphere s[SPHERES]; 最初的示例中，我们声明了一个指针，然后通过cudaMalloc()来为指针分配GPU内存。当我们将其修改为常量内存时，同样要将这个声明修改为在常量内存中静态地分配空间。我们不再需要对球面数组调用cudaMalloc()或cudaFree()。而是在编译时为这个数组提交一个固定的大小。将main()函数修改为常量内存的代码如下:\nint main(){ CPUBitmap bitmap(DIM, DIM); unsigned char *dev_bitmap; //在GPU上分配内存以计算输出位图 HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_bitmap, bitmap.image_size())); //分配临时内存，对其初始化，并复制到GPU上的内存，然后释放临时内存 Sphere *temp_s = (Sphere*) malloc(sizeof(Sphere) * SPHERES); for(int i = 0; i \u0026lt; SPHERES; i++){ temp_s[i].r = rnd(1.0f); temp_s[i].g = rnd(1.0f); temp_s[i].b = rnd(1.0f); temp_s[i].x = rnd(1000.0f) - 500; temp_s[i].y = rnd(1000.0f) - 500; temp_s[i].z = rnd(1000.0f) - 500; temp_s[i].radius = rnd(100.0f) + 20; } HANDLE_ERROR(cudaMemcpyToSymbol(s, temp_s, sizeof(Sphere) * SPHERES)); free(temp_s); //从球面数据中生成一张位图 dim3 grids(DIM / 16, DIM / 16); dim3 threads(16, 16); kernel\u0026lt;\u0026lt;\u0026lt;grids, threads\u0026gt;\u0026gt;\u0026gt;(dev_bitmap); //将位图从GPU复制回到CPU以显示 HANDLE_ERROR(cudaMemcpy(bitmap.get_ptr(), dev_bitmap, bitmap.image_size(), cudaMemcpyDeviceToHost)); bitmap.display_and_exit(); //释放内存 cudaFree(dev_bitmap); } cudaMemcpyToSymbol()与参数为cudaMemcpyHostToDevice()的cudaMemcpy()之间唯一差异时cudaMemcpyToSymbol()会复制到常量内存，而cudaMemcpy()会复制到全局内存。\n使用事件来测量性能 如何判断常量内存对程序性能有着多大影响？最简单的方式就是判断哪个版本的执行事件更短。使用CPU计时器或者操作系统中的某个计时器会带来各种延迟。为了测量GPU在某个任务上话费的时间，我们将使用CUDA的事件API。\nCUDA中的事件本质上是一个GPU时间戳，这个时间戳是在用户指定的时间点上记录的。由于GPU本身支持记录时间戳，因此就避免了当使用CPU定时器来统计GPU执行的事件时可能遇到的诸多问题。比如，下面的代码开头告诉CUDA运行时记录当前的时间，首先创建一个事件，然后记录这个事件。\ncudaEvent_t start; cudaEventCreate(\u0026amp;start); cudaEventRecord(start, 0); 要统计一段代码的执行时间，不仅要创建一个起始事件，还要创建一个结束事件。当在GPU上执行某个工作时，我们不仅要告诉CUDA运行时记录起始时间，还要记录结束时间:\ncudaEvent_t start, stop; cudaEventCreate(\u0026amp;start); cudaEventCreate(\u0026amp;stop); cudaEventRecord(start, 0); //在GPU执行一些工作 cudaEventRecord(stop, 0); cudaEventSynchronize(stop); //表示stop事件之前的所有GPU工作已经完成，可以安全读取在stop中保存的时间戳 下面是对光线跟踪器进行性能测试的代码:\nint main(){ //记录起始时间 cudaEvent_t start, stop; cudaEventCreate(\u0026amp;start); cudaEventCreate(\u0026amp;stop); cudaEventRecord(start, 0); CPUBitmap bitmap(DIM, DIM); unsigned char *dev_bitmap; //在GPU上分配内存以计算输出位图 HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_bitmap, bitmap.image_size())); //分配临时内存，对其初始化，并复制到GPU上的内存，然后释放临时内存 Sphere *temp_s = (Sphere*) malloc(sizeof(Sphere) * SPHERES); for(int i = 0; i \u0026lt; SPHERES; i++){ temp_s[i].r = rnd(1.0f); temp_s[i].g = rnd(1.0f); temp_s[i].b = rnd(1.0f); temp_s[i].x = rnd(1000.0f) - 500; temp_s[i].y = rnd(1000.0f) - 500; temp_s[i].z = rnd(1000.0f) - 500; temp_s[i].radius = rnd(100.0f) + 20; } HANDLE_ERROR(cudaMemcpyToSymbol(s, temp_s, sizeof(Sphere) * SPHERES)); free(temp_s); //从球面数据中生成一张位图 dim3 grids(DIM / 16, DIM / 16); dim3 threads(16, 16); kernel\u0026lt;\u0026lt;\u0026lt;grids, threads\u0026gt;\u0026gt;\u0026gt;(dev_bitmap); //将位图从GPU复制回到CPU以显示 HANDLE_ERROR(cudaMemcpy(bitmap.get_ptr(), dev_bitmap, bitmap.image_size(), cudaMemcpyDeviceToHost)); //获得结束时间，并显示计时结果 HANDLE_ERROR(cudaEventRecord(stop, 0)); HANDLE_ERROR(cudaEventSynchronize(stop)); float elapsedTime; HANDLE_ERROR(cudaEventElapsedTime(\u0026amp;elapsedTime, start, stop)); printf(\u0026#34;Time to generate: %3.1f ms\\n\u0026#34;, elapsedTime); HANDLE_ERROR(cudaEventDestroy(start)); HANDLE_ERROR(cudaEventDestroy(stop)); //显示位图 bitmap.display_and_exit(); //释放内存 cudaFree(dev_bitmap); } ","date":"February 18, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E5%9B%9B/","series":[],"smallImg":"","tags":[{"title":"CUDA","url":"/tags/cuda/"}],"timestamp":1708214400,"title":"CUDA学习(四)"},{"authors":[],"categories":[{"title":"Tutorial","url":"/categories/tutorial/"}],"content":"这一章将介绍线程块以及线程之间的通信机制和同步机制。\n在GPU启动并行代码的实现方法是告诉CUDA运行时启动核函数的多个并行副本。我们将这些并行副本称为线程块(Block)。\nCUDA运行时将把这些线程块分解为多个线程。当需要启动多个并行线程块时，只需将尖括号中的第一个参数由1改为想要启动的线程块数量。\n在尖括号中，第二个参数表示CUDA运行时在每个线程块中创建的线程数量。假设尖括号中的变量为\u0026laquo;\u0026lt;N, M\u0026raquo;\u0026gt;总共启动的线程数量可以按照以下公式计算: $$ N个线程块 * M个线程/线程块 = N*M个并行线程 $$\n使用线程实现GPU上的矢量求和 在之前的代码中，我们才去的时调用N个线程块，每个线程块对应一个线程add\u0026lt;\u0026lt;\u0026lt;N, 1\u0026gt;\u0026gt;\u0026gt;(dev_a, dev_b, dev_c);。\n如果我们启动N个线程，并且所有线程都在一个线程块中，则可以表示为add\u0026lt;\u0026lt;\u0026lt;1, N\u0026gt;\u0026gt;\u0026gt;(dev_a, dev_b, dev_c);。此外，因为只有一个线程块，我们需要通过线程索引来对数据进行索引(而不是线程块索引)，需要将int tid = blockIdx.x;修改为int tid = threadIdx.x;\n在GPU上对更长的矢量求和 对于启动核函数时每个线程块中的线程数量，硬件也进行了限制。具体来说，最大的线程数量不能超过设备树形结构中maxThreadsPerBlock域的值。对目前的GPU来说一个线程块最多有1024个线程。如果要通过并行线程对长度大于1024的矢量进行相加的话，就需要将线程与线程块结合起来才能实现。\n此时，计算索引可以表示为:\nint tid = threadIdx.x + blockIdx.x * blockDim.x; blockDim保存的事线程块中每一维的线程数量，由于使用的事一维线程块，因此只用到blockDim.x。\n此外，gridDim是二维的，而blockDim是三维的。\n假如我们使用多个线程块处理N个并行线程，每个线程块处理的线程数量为128，那样可以启动N/128个线程块。然而问题在于，当N小于128时，比如127，那么N/128等于0，此时将会启动0个线程块。所以我们希望这个除法能够向上取整。我们可以不用调用 ceil()函数，而是将计算改为(N+127)/N。因此，这个例子调用核函数可以写为:\nadd\u0026lt;\u0026lt;\u0026lt;(N + 127) / 128, 128\u0026gt;\u0026gt;\u0026gt;(dev_a, dev_b, dev_c); 当N不是128的整数倍时，将启动过多的线程。然而，在核函数中已经解决了这个问题。在访问输入数组和输出数组之前，必须检查线程的便宜是否位于0到N之间。\nif(tid \u0026lt; N){ c[tid] = a[tid] + b[tid]; } 因此，当索引越过数组的边界时，核函数将自动停止执行计算。核函数不会对越过数组边界的内存进行读取或者写入。\n在GPU上对任意长度的矢量求和 当矢量的长度很长时，我们可以让每一个线程执行多个矢量相加。例如\n__global__ void add(int *a, int *b, int *c){ int tid = threadIdx.x + blockIdx.x * blockDim.x; while(tid \u0026lt; N){ c[tid] = a[tid] + b[tid]; tid += blockDim.x * gridDim.x; } } 当每个线程计算完当前索引上的任务后，接着就需要对索引进行递增，其中递增的步长为线程格中正在运行的线程数量。这个数值等于每个线程块中的线程数量乘上线程格中线程块的数量，即blockDim.x * gridDim.x。\n共享内存和同步 CUDA C编译器对共享内存中的变量与普通变量分别采取不同的处理方式。对于在GPU上启动的每个线程块，CUDA C编译器都将创建该变量的一个副本，线程块中的每个线程都共享这块内存，但线程却无法看到也不能修改其他线程块的变量副本。这就实现了一个非常好的方式，使得一个线程块中的多个线程能够在计算上进行通信和协作。\n而且，共享内存缓冲区驻留在物理GPU上，而不是驻留在GPU之外的系统内存中。因此，在访问共享内存时的延迟要远远低于访问普通缓冲区的延迟，使得共享内存像每个线程块的高速缓存或者中间结果暂存器那样高效。\n如果想要实现线程之间通信，那么还需要一种机制来实现线程之间的同步。例如，如果线程A将一个值写入到共享内存，并且我们希望线程B对这个值进行一些操作，那么只有当线程A的写入操作完成之后，线程B才能开始执行它的操作。如果没有同步，那么将发生竞态条件(race condition)。\n下面将通过一个矢量的点积运算来详细介绍共享内存和同步。矢量点积运算为矢量相乘结束后将值相加起来以得到一个标量输出值。例如对两个包含4个元素的矢量进行点积运算: $$ (x_1, x_2, x_3, x_4) * (y_1, y_2, y_3, y_4) = x_1y_1 + x_2y_2 + x_3y_3 + x_4y_4 $$ 由于最终结果是所有乘积的总和，因此每个线程要保存它所计算的乘积的加和。下面代码实现了点积函数的第一个步骤:\n#include \u0026#34;../common/book.h\u0026#34; #define imin(a, b) (a \u0026lt; b ? a : b) const int N = 33 * 1024; const int threadsPerBlock = 256; __global__ void dot(float *a, float *b, float *c){ __shared__ float cache[threadsPerBlock]; int tid = threadIdx.x + blockIdx.x * blockDim.x; int cacheIndex = threadIdx.x; float temp = 0; while(tid \u0026lt; N){ temp += a[tid] * b[tid]; tid += blockDim.x * gridDim.x; } //设置cache中相应位置上的值 cache[cacheIndex] = temp; } 代码中声明了一个共享内存缓冲区，名字为cache。这个缓冲区将保存每个线程计算的加和值。我们将cache数组的大小声明为threadsPerBlock，这样线程块中每个线程都能将它计算的临时结果保存到某个位置上。之前在分配全局内存时，我们为每个执行核函数的线程都分配了足够的内存，即线程块的数量乘以threadsPerBlock。但对于共享变量来说，由于编译器将为每个线程块生成共享变量的一个副本，因此只需根据线程块中线程的数量来分配内存。\n我们需要将cache中所有的值相加起来。在执行这个运算时，需要通过一个线程来读取保存在cache中的值。由于race condition，我们需要使用下面的代码来确保对所有共享数组cache[]的写入操作在读组cache之前完成:\n//对线程块中的线程进行同步 __syncthreads(); 这个函数调用将确保线程块中的每个线程都执行完__syncthreads()前面的语句后，才会执行下一条语句。\n这时，我们可以将其中的值相加起来(称为归约Reduction)。代码的基本思想是每个线程将cache[]中的两个值相加起来，然后将结果保存回cache[]。由于每个线程都将两个值合并为一个值，那么在完成这个步骤后，得到的结果数量就是计算开始时数值数量的一半。下一个步骤中我们对这一半数值执行相同的操作，在将这种操作执行log2(threadsPerBlock)步骤后，就能得到cache[]中所有值的总和。对于这个示例来说，我们在每个线程块中使用了256个线程，因此需要8次迭代将cache[]中的256个值归约为1个值。这个归约过程的实现可以表示为以下代码:\n//对于归约运算来说，以下代码要求threadsPerBlock必须时2的指数 int i = blockDim.x / 2; while(i != 0){ if(cacheIndex \u0026lt; i){ cache[cacheIndex] += cache[cacheIndex + i]; } __syncthreads(); i /= 2; } 再结束了while()循环后，每个线程块都得到了一个值，这个值位于cache[]的第一个元素中，并且就等于该线程块中两两元素乘积的加和。然后，我们将这个值保存到全局内存并结束核函数:\nif(cacheIndex == 0){ c[blockIdx.x] = cache[0]; } 只让cacheIndex为0的线程执行保存操作时因为每个线程块只有一个值写入到全局内存，因此每个线程块只需要一个线程来执行这个操作。最后，由于每个线程块都只写入一个值到全局数据c[]中，因此可以通过blockIdx来索引这个值。\n点积运算的最后一个步骤就是计算c[]中所有元素的总和。像GPU这种大规模的并行机器在执行最后的归约步骤时，通常会浪费计算资源，因为此时的数据集往往会非常小。因此，我们可以将执行控制返回给主机，并且由CPU来完成最后一个加法步骤。\n下面给出了完整的代码实现:\n#include \u0026#34;../common/book.h\u0026#34; #define imin(a, b) (a \u0026lt; b? a : b) const int N = 33 * 1024; const int threadsPerBlock = 256; const int blocksPerGrid = imin(32, (N + threadsPerBlock - 1) / threadsPerBlock); __global__ void dot(float *a, float *b, float *c){ __shared__ float cache[threadsPerBlock]; int tid = threadIdx.x + blockIdx.x * blockDim.x; int cacheIndex = threadIdx.x; float temp = 0; while(tid \u0026lt; N){ temp += a[tid] * b[tid]; tid += blockDim.x * gridDim.x; } //设置cache中相应位置上的值 cache[cacheIndex] = temp; //对线程块中的线程进行同步 __syncthreads(); //对于归约来说，以下代码要求threadsPerBlock必须是2的指数 int i = blockDim.x / 2; while(i != 0){ if(cacheIndex \u0026lt; i){ cache[cacheIndex] += cache[cacheIndex + i]; } __syncthreads(); //循环中更新了变量cache，所以需要在下一次循环前进行同步。该同步语句需要所有的线程都必须运行才行。如果有线程不能运行这一处代码，会导致其他线程永远等待。 i /= 2; } if(cacheIndex == 0){ c[blockIndex.x] = cache[0]; } } int main(){ float *a, *b, c, *partial_c; float *dev_c, *dev_b, *dev_partial_c; //在CPU上分配内存 a = (float*) malloc(N*sizeof(float)); b = (float*) malloc(N*sizeof(float)); partial_c = (float*) malloc(blocksPerGrid * sizeof(float)); //在GPU上分配内存 HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_a, N * sizeof(float))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_b, N * sizeof(float))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_partial_c, blocksPerGrid * sizeof(float))); //填充主机内存 for(int i = 0; i \u0026lt; N; i++){ a[i] = i; b[i] = i * 2; } //将数组\u0026#34;a\u0026#34;和\u0026#34;b\u0026#34;复制到GPU HANDLE_ERROR(cudaMemcpy(dev_a, a, N * sizeof(float), cudaMemcpyHostToDevice)); HANDLE_ERROR(cudaMemcpy(dev_b, b, N * sizeof(float), cudaMemcpyHostToDevice)); dot\u0026lt;\u0026lt;\u0026lt;blocksPerGrid, threadsPerBlock\u0026gt;\u0026gt;\u0026gt;(dev_a, dev_b, dev_partial_c); //将数组\u0026#34;c\u0026#34;从GPU复制到CPU HANDLE_ERROR(cudaMemcpy(partial_c, dev_partial_c, blocksPerGrid * sizeof(float), cudaMemcpyDeviceToHost)); //在CPU上完成最终的求和运算 c = 0; for(int i = 0; i \u0026lt; blocksPerGrid; i++){ c += partial_c[i]; } #define sum_squares(x) (x * (x + 1) * (2 * x + 1) / 6) printf(\u0026#34;Does GPU value %.6g = %.6g? \\n\u0026#34;, c, 2 * sum_square((float) (N - 1))); //释放GPU上的内存 cudaFree(dev_a); cudaFree(dev_b); cudaFree(dev_partial_c); //释放CPU上的内存 free(a); free(b); free(partial_c); } ","date":"February 12, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E4%B8%89/","series":[],"smallImg":"","tags":[{"title":"CUDA","url":"/tags/cuda/"}],"timestamp":1707696000,"title":"CUDA学习(三)"},{"authors":[],"categories":[{"title":"Tutorial","url":"/categories/tutorial/"}],"content":"这一部分将介绍CUDA的并行编程方式\n矢量求和运算 假设有两组数据，我们需要将这两组数据中对应的元素两两相加，并将结果保存在第三个数组中。\n基于CPU的矢量求和 首先，下面的代码是通过传统的C代码来实现这个求和运算\n#include \u0026#34;../common/book.h\u0026#34; #define N 10 void add(int *a, int *b, int *c){ int tid = 0;\t//这是第0个CPU，因此索引从0开始 while(tid \u0026lt; N){ c[tid] = a[tid] + b[tid]; tid += 1;\t//由于只有一个CPU，因此每次递增1 } } int main(){ int a[N], b[N], c[N]; //在CPU上为数组\u0026#34;a\u0026#34;和\u0026#34;b\u0026#34;赋值 for(int i = 0; i \u0026lt; N; i++){ a[i] = -i; b[i] = i * i; } add(a, b, c); //显示结果 for(int i = 0; i \u0026lt; N; i++){ printf(\u0026#34;%d + %d = %d\\n\u0026#34;, a[i], b[i], c[i]); } return 0; } add()中使用while循环而不是for循环是为了代码能够在拥有多个CPU或者多个CPU核的系统上并行运行，比如双核处理器上可以将每次递增的大小改为2。\n//第一个CPU核 void add(int *a, int *b, int *c){ int tid = 0; while(tid \u0026lt; N){ c[tid] = a[tid] + b[tid]; tid += 2; } } //第二个CPU核 void add(int *a, int *b, int *c){ int tid = 1; while(tid \u0026lt; N){ c[tid] = a[tid] + b[tid]; tid += 2; } } 基于GPU的矢量求和 下面是基于GPU的矢量求和代码\n#include \u0026#34;../common/book.h\u0026#34; #define N 10 __global__ add(int *dev_a, int *dev_c, int *dev_c){ int tid = blockIdx.x; //计算该索引处的数据 if(tid \u0026lt; N){ c[tid] = a[tid] + b[tid]; } } int main(){ int a[N], b[N], c[N]; int *dev_a, *dev_b, *dev_c; //在GPU上分配内存 HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_a, N * sizeof(int))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_b, N * sizeof(int))); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_c, N * sizeof(int))); //在CPU上为数组\u0026#34;a\u0026#34;和\u0026#34;b\u0026#34;赋值 for(int i = 0; i \u0026lt; N; i++){ a[i] = -i; b[i] = i * i; } //将数组\u0026#34;a\u0026#34;和\u0026#34;b\u0026#34;复制到GPU HANDLE_ERROR(cudaMemcpy(dev_a, a, N * sizeof(int), cudaMemcpyHostToDevice)); HANDLE_ERROR(cudaMemcpy(dev_b, b, N * sizeof(int), cudaMemcpyHostToDevice)); add\u0026lt;\u0026lt;\u0026lt;N, 1\u0026gt;\u0026gt;\u0026gt;(dev_a, dev_b, dev_c); //将数组\u0026#34;c\u0026#34;从GPU复制到CPU HANDLE_ERROR(cudaMemcpy(c, dev_c, N * sizeof(int), cudaMemcpyDeviceToHost)); //显示结果 for(int i = 0; i \u0026lt; N; i++){ printf(\u0026#34;%d + %d = %d\\n\u0026#34;, a[i], b[i], c[i]); } //释放在GPU上分配的内存 cudaFree(dev_a); cudaFree(dev_b); cudaFree(dev_c); return 0; } 在示例代码中，调用add函数的尖括号内的数值是\u0026laquo;\u0026lt;N, 1\u0026raquo;\u0026gt;，其中第一个参数表示设备在执行核函数时使用的并行线程块的数量。比如如果制定的事kernel\u0026laquo;\u0026lt;256, 1\u0026raquo;\u0026gt;()，那么将有256个线程块在GPU上运行。\n在add函数里面，我们可以使用blockIdx.x获取具体的线程块(blockIdx是一个内置变量，不需要定义它)，通过这种方式可以让不同的线程块并行执行数组的矢量相加。\n下一章将会详细解释线程块以及线程之间的通信机制和同步机制。\n","date":"February 8, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E4%BA%8C/","series":[],"smallImg":"","tags":[{"title":"CUDA","url":"/tags/cuda/"}],"timestamp":1707350400,"title":"CUDA学习(二)"},{"authors":[],"categories":[{"title":"Tutorial","url":"/categories/tutorial/"}],"content":"参考书目: GPU高性能编程CUDA实战\n书目网页链接: https://hpc.pku.edu.cn/docs/20170829223652566150.pdf\n该博客参考于上述书籍，虽然书有一点老，但是作为初学者而言仍然能学到很多东西。\n本书所包含的代码都在下面的连接中，可以下载来学习: https://developer.nvidia.com/cuda-example\nCUDA C简介 首先来看一个CUDA C的示例:\n#include \u0026#34;../common/book.h\u0026#34; int main(){ prinf(\u0026#34;Hello World!\\n\u0026#34;); return 0; } 这个示例只是为了说明，CUDA C与熟悉的标准C在很大程度上是没有区别的。\n核函数调用 在GPU设备上执行的函数通常称为核函数(Kernel)\n#include \u0026lt;iostream\u0026gt; __global__ void kernel(){ } int main(){ kernel\u0026lt;\u0026lt;\u0026lt;1, 1\u0026gt;\u0026gt;\u0026gt;(); printf(\u0026#34;Hello World!\\n\u0026#34;); return 0; } 跟之前的代码相比多了两处\n一个空的函数kernel()，并且带有修饰符__global__。 对这个空函数的调用，并且带有修饰字符\u0026laquo;\u0026lt;1, 1\u0026raquo;\u0026gt;。 这个__global__可以认为是告诉编译器，函数应该编译为在设备而不是在主机上运行。函数kernel()将被交给编译器设备代码的编译器，而main()函数将被交给主机编译器。\n传递参数 以下是对上述代码的进一步修改，可以实现将参数传递给核函数\n#include \u0026lt;iostream\u0026gt; #include \u0026#34;book.h\u0026#34; __global__ void add(int a, int b, int* c){ *c = a + b; printf(\u0026#34;c is %d\\n\u0026#34;, *c); } int main(void){ int c = 0; int* dev_c; printf(\u0026#34;original c is %d\\n\u0026#34;, c); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_c, sizeof(int))); add\u0026lt;\u0026lt;\u0026lt;1, 1\u0026gt;\u0026gt;\u0026gt;(2, 7, dev_c); HANDLE_ERROR(cudaMemcpy(\u0026amp;c, dev_c, sizeof(int), cudaMemcpyDeviceToHost)); cudaFree(dev_c); printf(\u0026#34;2 + 7 = %d\\n\u0026#34;, c); return 0; } 其中\u0026quot;book.h\u0026quot;包含了HANDLE_ERROR，也可以不使用\u0026quot;book.h\u0026quot;而是在代码中添加HANDLE_ERROR函数。\n#include \u0026lt;iostream\u0026gt; static void HandleError( cudaError_t err, const char *file, int line ) { if (err != cudaSuccess) { printf( \u0026#34;%s in %s at line %d\\n\u0026#34;, cudaGetErrorString( err ), file, line ); exit( EXIT_FAILURE ); } } #define HANDLE_ERROR( err ) (HandleError( err, __FILE__, __LINE__ )) __global__ void add(int a, int b, int* c){ *c = a + b; printf(\u0026#34;c is %d\\n\u0026#34;, *c); } int main(void){ int c = 0; int* dev_c; printf(\u0026#34;original c is %d\\n\u0026#34;, c); HANDLE_ERROR(cudaMalloc((void**)\u0026amp;dev_c, sizeof(int))); add\u0026lt;\u0026lt;\u0026lt;1, 1\u0026gt;\u0026gt;\u0026gt;(2, 7, dev_c); HANDLE_ERROR(cudaMemcpy(\u0026amp;c, dev_c, sizeof(int), cudaMemcpyDeviceToHost)); printf(\u0026#34;2 + 7 = %d\\n\u0026#34;, c); cudaFree(dev_c); return 0; } cudaMalloc()用来分配内存，这个函数的调用行为非常类似于标准C函数的malloc()，但该函数作用是告诉CUDA运行时在设备上分配内存。 第一个参数是一个指针，指向用于保存新分配内存地址的变量。第二个参数是分配内存的大小。 该函数返回的类型是void*。 不能使用标准C的free()函数来释放cudaMallocc()分配的内存。要释放cudaMalloc()分配的内存，需要调用cudaFree()。 HANDLE_ERROR()是定义的一个宏，作为辅助代码的一部分，用来判断函数调用是否返回了一个错误值，如果是的话，将输出相应的错误消息。 在主机代码中可以通过调用cudaMemcpy()来访问设备上的内存。 第一个参数是目标(target)指针，第二个参数是源(source)指针，第三个参数分配内存大小。第四个参数则是指定设备内存指针。 第四个参数一般有cudaMemcpyDeviceToHost，cudaMemcpyHostToDevice, cudaMemcpyDeviceToDevice三种。cudaMemcpyDeviceToHost说明我们将设备内存指针的数据传递给主机内存指针，此时第一个参数指针是在主机上，第二个参数指针是在设备上。cudaMemcpyHostToDevice说明我们将主机内存指针的数据传递给设备内存指针，此时第一个参数指针是在设备上，第二个参数指针是在主机上。此外还可以通过传递参数cudaMemcpyDeviceToDevice莱高速运行时这两个指针都在设备上。如果源指针和目标指针都在主机上，则可以直接调用memcpy()函数。 查询设备 我们可以使用cudaGetDeviceCount()来查询设备数量(比如GPU数量)。\nint count; HANDLE_ERROR(cudaGetDeviceCount(\u0026amp;count)); CUDA设备属性包含很多信息，可以在书上或者NVIDIA官方网站上查到。\n","date":"February 4, 2024","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E4%B8%80/","series":[],"smallImg":"","tags":[{"title":"CUDA","url":"/tags/cuda/"}],"timestamp":1707004800,"title":"CUDA学习(一)"},{"authors":[],"categories":[],"content":"Hi there, I\u0026rsquo;m XXX.\n","date":"January 1, 1","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/about/","series":[],"smallImg":"","tags":[],"timestamp":-62135596800,"title":"About"},{"authors":[],"categories":[],"content":"","date":"January 1, 1","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/contact/","series":[],"smallImg":"","tags":[],"timestamp":-62135596800,"title":"Contact Us"},{"authors":[],"categories":[],"content":"","date":"January 1, 1","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/offline/","series":[],"smallImg":"","tags":[],"timestamp":-62135596800,"title":"Offline"}]
