<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Posts on Kaster Mist's Blog</title><link>https://KasterMist.com/posts/</link><description>Recent content in Posts on Kaster Mist's Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Copyright © 2024-2024 Kaster. All Rights Reserved.</copyright><lastBuildDate>Wed, 15 May 2024 13:48:56 +0800</lastBuildDate><atom:link href="https://KasterMist.com/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Git 学习 (三)</title><link>https://KasterMist.com/posts/git/git_tutorial_3/</link><pubDate>Wed, 15 May 2024 13:48:56 +0800</pubDate><guid>https://KasterMist.com/posts/git/git_tutorial_3/</guid><description>&lt;p>本章将着重介绍如何更清晰的整理提交记录，git tag和git describe的用法。&lt;/p></description><enclosure url="https://KasterMist.com/images/logo/git.jpg" length="30657" type="image/.jpg"/></item><item><title>Git 学习 (二)</title><link>https://KasterMist.com/posts/git/git_tutorial_2/</link><pubDate>Sun, 12 May 2024 16:43:56 +0800</pubDate><guid>https://KasterMist.com/posts/git/git_tutorial_2/</guid><description>&lt;p>本章节将介绍git的一些高级操作，这些操作主要用来如何在提交树上进行移动，方便更灵活的更改分支以及提交节点。&lt;/p></description><enclosure url="https://KasterMist.com/images/logo/git.jpg" length="30657" type="image/.jpg"/></item><item><title>Git 学习 (一)</title><link>https://KasterMist.com/posts/git/git_tutorial_1/</link><pubDate>Sat, 11 May 2024 17:02:07 +0800</pubDate><guid>https://KasterMist.com/posts/git/git_tutorial_1/</guid><description><![CDATA[<p>本文章根据&quot;Learn Git Branching&quot;网站进行记录知识点。该网站将git流程可视化，可以很好的学习git相关知识。如果有兴趣，可以访问该网站: <a href="https://learngitbranching.js.org/?locale=zh_CN" target="_blank" rel="noopener noreferrer">https://learngitbranching.js.org/?locale=zh_CN<i class="fas fa-external-link-square-alt ms-1"></i></a></p>]]></description><enclosure url="https://KasterMist.com/images/logo/git.jpg" length="30657" type="image/.jpg"/></item><item><title>Vscode 插件: C/C++</title><link>https://KasterMist.com/posts/vscode/1/</link><pubDate>Wed, 08 May 2024 15:17:44 +0800</pubDate><guid>https://KasterMist.com/posts/vscode/1/</guid><description>&lt;p>本章将介绍vscode的C/C++插件的具体使用方法。&lt;/p></description><enclosure url="https://KasterMist.com/images/logo/vscode.jpeg" length="7706" type="image/.jpeg"/></item><item><title>Hugo Bootstrap Skeleton Notes</title><link>https://KasterMist.com/posts/hugo_theme_usage_notes/hugo_bootstrap_skeleton/</link><pubDate>Thu, 02 May 2024 16:00:44 +0800</pubDate><guid>https://KasterMist.com/posts/hugo_theme_usage_notes/hugo_bootstrap_skeleton/</guid><description>&lt;p>本文章主要记录使用博客期间遇到的有关hugo bootstrap skeleton主题的一些问题以及解决方法。&lt;/p></description></item><item><title>Vim</title><link>https://KasterMist.com/posts/linux/vim/</link><pubDate>Sat, 13 Apr 2024 00:00:00 +0000</pubDate><guid>https://KasterMist.com/posts/linux/vim/</guid><description>&lt;p>本章节将介绍vim的常用语法。&lt;/p></description><enclosure url="https://KasterMist.com/images/logo/vim.png" length="11793" type="image/.png"/></item><item><title>Linux 常用命令</title><link>https://KasterMist.com/posts/linux/linux-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</link><pubDate>Thu, 04 Apr 2024 00:00:00 +0000</pubDate><guid>https://KasterMist.com/posts/linux/linux-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</guid><description><![CDATA[<h2 id="基础命令" data-numberify>基础命令<a class="anchor ms-1" href="#基础命令"></a></h2>

<h4 id="查询文件和子目录-ls" data-numberify>查询文件和子目录 ls<a class="anchor ms-1" href="#查询文件和子目录-ls"></a></h4>
<p>查询文件和子目录的最简单的命令是<code>ls</code>。它可以列出当前目录的文件和子目录。常用的指令有:</p>
<ul>
<li><code>ls /path/to/directory</code> 列出指定目录中的文件和子目录</li>
<li><code>ls -a</code>列出隐藏文件和子目录</li>
<li><code>ls -l</code>以详细格式列出文件和子目录，包含读写权限、创建时间等信息</li>
</ul>]]></description></item><item><title>CUDA学习(十)--书籍中的函数整合</title><link>https://KasterMist.com/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E5%8D%81-%E4%B9%A6%E7%B1%8D%E4%B8%AD%E7%9A%84%E5%87%BD%E6%95%B0%E6%95%B4%E5%90%88/</link><pubDate>Mon, 11 Mar 2024 00:00:00 +0000</pubDate><guid>https://KasterMist.com/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E5%8D%81-%E4%B9%A6%E7%B1%8D%E4%B8%AD%E7%9A%84%E5%87%BD%E6%95%B0%E6%95%B4%E5%90%88/</guid><description>&lt;p>前面几章主要是对参考书目的内容进行一个概括。本章将根据参考书目的内容对所学到的所有函数进行一个整理和总结，以便复习和参考。英伟达的官方网站包含了所有的CUDA函数，可参考https://developer.download.nvidia.cn/compute/DevZone/docs/html/C/doc/html/index.html&lt;/p></description></item><item><title>CUDA学习(九)</title><link>https://KasterMist.com/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E4%B9%9D/</link><pubDate>Fri, 08 Mar 2024 00:00:00 +0000</pubDate><guid>https://KasterMist.com/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E4%B9%9D/</guid><description><![CDATA[<p>这部分是基于原子操作章节进行的高级操作介绍，即实现锁定数据结构。</p>
<p>原子操作只能确保每个线程在完成读取-修改-写入的序列之前，将不会有其他的线程读取或者写入目标内存。然而，原子操作并不能确保这些线程将按照何种顺序执行。例如当有三个线程都执行加法运算时，加法运行的执行顺序可以为(A + B) + C，也可以为A + (B + C)。这对于整数来说是可以接受的，因为中间结果可能被截断，因此(A + B) + C通常并不等于A + (B + c)。因此，浮点数值上的原子数学运算是否有用就值得怀疑🤔。因此在早期的硬件中，浮点数学运算并不是优先支持的功能。</p>
<p>然而，如果可以容忍在计算结果中存在某种程度的不确定性，那么仍然可以完全在GPU上实现归约运算。我们首先需要通过某种方式来绕开原子浮点数学运算。在这个解决方案中仍将使用原子操作，但不是用于算数本身。</p>]]></description></item><item><title>CUDA学习(八)</title><link>https://KasterMist.com/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E5%85%AB/</link><pubDate>Thu, 29 Feb 2024 00:00:00 +0000</pubDate><guid>https://KasterMist.com/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E5%85%AB/</guid><description><![CDATA[本章将介绍如何分配和使用零拷贝内存(Zero-Copy Memory)，如何在同一个应用程序中使用多个GPU，以及如何分配和使用可移动的固定内存(Portable Pinned Memory)。
零拷贝主机内存 上一章介绍了固定内存（页锁定内存），这种新型的主机内存能够确保不会交换出物理内存。我们通过cudaHostAlloc()来分配这种内存，并且传递参数cudaHostAllocDefault来获得默认的固定内存。本章会介绍在分配固定内存时可以使用其他参数值。除了cudaHostAllocDefault外，还可以传递的标志之一是cudaHostAllocMapped。通过cudaHostAllocMapped分配的主机内存也是固定的，它与通过cudaHostAllocDefault分配的固定内存有着相同的属性，特别是当它不能从物理内存中交换出去或者重新定位时。但这种内存除了可以用于主机与GPU之间的内存复制外，还可以打破主机内存规则之一：可以在CUDA C核函数中直接访问这种类型的主机内存。由于这种内存不需要复制到GPU，因此也被称为零拷贝内存。
通过零拷贝内存实现点积运算 通常，GPU只能访问GPU内存，而CPU也只能访问主机内存。但在某些环境中，打破这种规则或许能带来更好的效果。下面仍然给出一个矢量点积运算来进行介绍。这个版本不将输入矢量显式复制到GPU，而是使用零拷贝内存从GPU中直接访问数据。我们将编写两个函数，其中一个函数是对标准主机内存的测试，另一个函数将在GPU上执行归约运算，并使用零拷贝内存作为输入缓冲区和输出缓冲区。首先是点积运算的主机内存版本:
float malloc_test(int size){ //首先创建计时事件，然后分配输入缓冲区和输出缓冲区，并用数据填充输入缓冲区。 cudaEvent_t start, stop; float *a, *b, c, *partial_c; float *dev_a, *dev_b, *dev_partial_c; float elapsedTime; HANDLE_ERROR(cudaEventCreate(&amp;start)); HANDLE_ERROR(cudaEventCreate(&amp;stop)); //在CPU上分配内存 a = (float*)malloc(size * sizeof(float)); b = (float*)malloc(size * sizeof(float)); partial_c = (float*)malloc(blocksPerGrid * sizeof(float)); //在GPU上分配内存 HANDLE_ERROR(cudaMalloc((void**)&amp;dev_a, size * sizeof(float))); HANDLE_ERROR(cudaMalloc((void**)&amp;dev_b, size * sizeof(float))); HANDLE_ERROR(cudaMalloc((void**)&amp;dev_partial_c, blocksPerGrid * sizeof(float))); //用数据填充主机内存 for(int i = 0; i &lt; size; i++){ a[i] = i; b[i] = i * 2; } //启动计时器，将输入数据复制到GPU，执行点积核函数，并将中间计算结果复制回主机。 HANDLE_ERROR(cudaEventRecord(start, 0)); //将数组“a“和”b”复制到GPU HANDLE_ERROR(cudaMemcpy(dev_a, a, size * sizeof(float), cudaMemcpyHostToDevice)); HANDLE_ERROR(cudaMemcpy(dev_b, b, size * sizeof(float), cudaMemcpyHostToDevice)); dot&lt;&lt;&lt;blocksPerGrid, threadsPerBlock&gt;&gt;&gt;(size, dev_a, dev_b, dev_partial_c); //将数组“c”从GPU复制到CPU HANDLE_ERROR(cudaMemcpy(partial_c, dev_partial_c, blocksPerGrid * sizeof(float), cudaMemcpyDeviceToHost)); //停止计时器 HANDLE_ERROR(cudaEventRecord(stop, 0)); HANDLE_ERROR(cudaEventSynchronize(stop)); HANDLE_ERROR(cudaEventElapsedTime(&amp;elapsedTime, start, stop)); //将中间计算结果相加起来，并释放输入缓冲区和输出缓冲区 //结束CPU上的计算 c = 0; for(int i = 0; i &lt; blocksPerGrid; i++){ c += partial_c[i]; } HANDLE_ERROR(cudaFree(dev_a)); HANDLE_ERROR(cudaFree(dev_b)); HANDLE_ERROR(cudaFree(dev_partial_c)); //释放CPU上的内存 free(a); free(b); free(partial_c); //释放事件 HANDLE_ERROR(cudaEventDestroy(start)); HANDLE_ERROR(cudaEventDestroy(stop)); printf(&#34;Value calculated: %f\n&#34;, c); return elapsedTime; } 使用零拷贝内存的版本是非常类似的多，只是在内存分配上有所不同：]]></description></item><item><title>CUDA学习(七)</title><link>https://KasterMist.com/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E4%B8%83/</link><pubDate>Wed, 28 Feb 2024 00:00:00 +0000</pubDate><guid>https://KasterMist.com/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E4%B8%83/</guid><description><![CDATA[在并行环境中，任务可以是任意操作。例如，应用程序可以执行两个任务：其中一个线程重绘程序的GUI，而另一个线程通过网络下载更新包。这些任务并行执行，彼此之间没有任何共同的地方。虽然GPU上的任务并行性并不像CPU上的任务并行性那样灵活，但仍然可以进一步提高程序在GPU上的运行速度。本章将介绍CUDA流，以及如何通过流在GPU上同时执行多个任务。
页锁定(Page-Locked)主机内存 CUDA提供了自己独有的机制来分配主机内存：cudaHostAlloc()。malloc()分配的内存与cudaHostAlloc()分配的内存之间存在一个重要差异。C库函数malloc()将分配标准的、可分页的(Pagable)主机内存，而cudaHostAlloc()将分配页锁定的主机内存。页锁定内存也称为固定内存(Pinned Memory)或者不可分页内存，它有一个重要属性：操作系统将不会对这块内存分页并交还到磁盘上，从而确保了该内存始终驻留在物理内存中。因此，操作系统能够安全地使某个应用程序访问该内存的物理地址，因为这块内存将不会被破坏或者重新定位。
由于GPU知道内存的物理地址，因此可以通过“直接内存访问（Direct Memory Access，DMA）”技术来在GPU和主机之间复制数据。由于DMA在执行复制时无需CPU的介入，这也就意味着，CPU很可能在DMA的执行过程中将目标内存交换到磁盘上，或者通过更新操作系统的分页来重新定位目标内存的物理地址。CPU可能会移动可分页的数据，这就可能对DMA操作造成延迟。因此，在DMA复制过程中使用固定内存时非常重要的。
事实上，当使用可分页内存进行复制时，CUDA驱动程序仍然会通过DAM把数据传输给GPU。因此，复制操作将执行两遍，第一遍从可分页内存复制到一块“临时的”页锁定内存，然后再从这个页锁定内存复制到GPU上。因此，**每当从可分页内存中执行复制操作时，复制速度将受限于PCIE（高速串行计算机扩展总线标准）传输速度和系统前端总线速度相对较低的一方。当在GPU和主机间复制数据时，这种差异会使页锁定主机内存的性能比标准可分页内存的性能要高达约2倍。**计时PCIE的速度与前端总线的速度相等，由于可分页内存需要更多一次由CPU参与的复制操作，因此会带来额外的开销。
然而，使用cudaHostAlloc()分配固定内存时，将失去虚拟内存的所有功能。特别是在应用程序中使用每个页锁定内存时都需要分配物理内存，因为这些内存不能交换到磁盘上。这意味着与使用标准的malloc()调用相比，系统将更快地耗尽内存。因此，应用程序在物理内存较少的机器上会运行失败，而且意味着应用程序将影响在系统上运行的其他应用程序的性能。建议仅对cudaMemcpy()调用中的源内存或者目标内存才使用页锁定内存，并且在不再需要使用它们时立即释放，而不是等到应用程序关闭时才释放。
下面给的例子来说明如何分配固定内存，以及它相对于标准可分页内存的性能优势。这个例子主要是测试cudaMemcpy()在可分配内存和页锁定内存上的性能。我们要做的就是分配一个GPU缓冲区以及一个大小相等的主机缓冲区，然后两个缓冲区之间执行一些复制操作（从主机到设备、从设备到主机）。为了获得精确的时间统计，我们为复制操作的起始时刻和结束时刻分别设置了CUDA事件。
float cuda_malloc_test(int size, bool up){ cudaEvent_t start, stop; int *a, *dev_a; float elapsedTime; HANDLE_ERROR(cudaEventCreate(&amp;start)); HANDLE_ERROR(cudaEventCreate(&amp;stop)); a = (int*)malloc(size * sizeof(*a)); HANDLE_NULL(a); HANDLE_ERROR(cudaMalloc((void**)&amp;dev_a, size * sizeof(&amp;dev_a))); HANDLE_ERROR(cuddaEventRecord(start, 0)); //为size个整数分别分配主机缓冲区和GPU缓冲区，然后执行100次复制操作，并由参数up来指定复制方向，在完成复制操作后停止计时器 for(int i = 0; i &lt; 100; i++){ if(up){ HANDLE_ERROR(cudaMemcpy(dev_a, a, size * sizeof(*dev_a), cudaMemcpyHostToDevice)); } else{ HANDLE_ERROR(cudaMemcpy(a, dev_a, size * sizeof(*dev_a), cudaMemcpyDeviceToHost)); } } HANDLE_ERROR(cudaEventRecord(stop, 0)); HANDLE_ERROR(cudaEventSynchronize(stop)); HANDLE_ERROR(cudaEventElapsedTime(&amp;elapsedTime, start, stop)); free(a); HANDLE_ERROR(cudaFree(dev_a)); HANDLE_ERROR(cudaEventDestroy(start)); HANDLE_ERROR(cudaEventDestroy(stop)); return elapsedTime; } 函数cuda_malloc_test()通过标准的C函数malloc()来分配可分页主机内存，在分配固定内存时则使用了cudaHostAlloc()。]]></description></item><item><title>CUDA学习(六)</title><link>https://KasterMist.com/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E5%85%AD/</link><pubDate>Tue, 27 Feb 2024 00:00:00 +0000</pubDate><guid>https://KasterMist.com/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E5%85%AD/</guid><description><![CDATA[在某些情况中，对于单线程应用程序来说非常简单的任务，在大规模并行架构上实现时会变成一个复杂的问题。在本章中，我们将举例其中一些情况，并在这些情况中安全地完成传统单线程应用程序中的简单任务。
原子操作简介 在编写传统的单线程应用程序时，程序员通常不需要使用原子操作。下面会介绍一下原子操作是什么，以及为什么在多线程程序中需要使用它们。我们先分析C或者C++的递增运算符：
x++;
在这个操作中包含了三个步骤：
读取x中的值。 将步骤1中读到的值增加1。 将递增后的结果写回到x。 有时候，这个过程也称为读取-修改-写入操作。
当两个线程都需要对x的值进行递增时，假设x的初始值为7，理想情况下，两个线程按顺序对x进行递增，第一个线程完成三个步骤后第二个线程紧接着完成三个步骤，最后得到的结果是9。但是，实际情况下会出现两个线程的操作彼此交叉进行，这种情况下得到的结果将小于9(比如两个线程同时读取x=7，计算完后写入，那样的话x最后会等于8)。
因此，我们需要通过某种方式一次性地执行完读取-修改-写入这三个操作，并在执行过程中不会被其他线程中断。由于这些操作的执行过程不能分解为更小的部分，因此我们将满足这种条件限制的操作称为原子操作。
计算直方图 本章将通过给出计算直方图的例子来介绍如何进行原子性计算。
在CPU上计算直方图 某个数据的直方图表示每个元素出现的频率。在示例中，这个数据将是随机生成的字节流。我们可以通过工具函数big_random_block()来生成这个随机的字节流。在应用程序中将生成100MB的随机数据。
#include &#34;../common/book.h&#34; #define SIZE (100 * 1024 * 1024) int main(){ unsigned char *buffer = (unsigned char*)big_random_block(SIZE); } 由于每个随机字节（8比特）都有256个不同的可能取值（从0x00到0xFF），因此在直方图中需要包含256个元素，每个元素记录相应的值在数据流中的出现次数。我们创建了一个包含256个元素的数组，并将所有元素的值初始化为0。
unsigned int histo[256]; for(int i = 0; i &lt; 256; i++){ histo[i] = 0; } 接下来需要计算每个值在buffer[]数据中的出现频率。算法思想是，每当在数组buffer[]中出现某个值z时，就递增直方图数组中索引为z的元素，这样就能计算出值z的出现次数。如果当前看到的值为buffer[i]，那么将递增数组中索引等于buffer[i]的元素。由于元素buffer[i]位于histo[buffer[i]]，我们只需一行代码就可以递增相应的计数器。在一个for循环中对buffer[]中的每个元素执行这个操作：
for(int i = 0; i &lt; SIZE; i++){ histo[buffer[i]]++; } 接下来将验证直方图的所有元素相加起来是否等于正确的值。
long histoCount = 0; for(int i = 0; i &lt; 256; i++){ histoCount += histo[i]; } printf(&#34;Histogram Sum: %ld\n&#34;, histoCount); free(buffer); 在GPU上计算直方图 以下时计算直方图的GPU版本]]></description></item><item><title>CUDA学习(五)</title><link>https://KasterMist.com/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E4%BA%94/</link><pubDate>Fri, 23 Feb 2024 00:00:00 +0000</pubDate><guid>https://KasterMist.com/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E4%BA%94/</guid><description>本章将学习如何分配和使用纹理内存(texture memory)。和常量内存一样，纹理内存是另一种类型的只读内存，在特定的访问模式中，纹理内存同样能够提升性能并减少内存流量。虽然纹理内存最初是针对传统的图形处理应用程序而设计的，但在某些GPU计算应用程序中同样非常有用。
与常量内存相似的是，纹理内存同样缓存在芯片上，因此在某些情况中，它能够减少对内存请求并提供更高效的内存带宽。纹理缓存是专门为那些在内存访问模式中存在大量空间局部性(spatial locality)的图形应用程序而设计的。在某个计算应用程序中，这意味着一个线程读取的位置可能与邻近线程读取的位置“非常接近”。
热传导模拟 本章用一个简单的热传导模拟的模型来介绍如何使用纹理内存。
简单的传热模型 我们构造一个简单的二维热传导模拟。首先假设有一个矩形房间，将其分成一个格网，每个格网中随机散步一些“热源”，他们有着不同的温度。
在给定了矩形格网以及热源分布后，我们可以计算格网中每个单元格的温度随时间的变化情况。为了简单，热源单元本身的温度将保持不变。在时间递进的每个步骤中，我们假设热量在某个单元及其邻接单元之间“流动”。如果某个单元的临界单元的温度更高，那么热量将从邻接单元传导到该单元，相反地，如果某个单元的温度比邻接单元的温度高，那么它将变冷。
在热传导模型中，我们对单元中新温度的计算方法为，将单元与邻接单元的温差相加起来，然后加上原有温度。 $$ T_{NEW} = T_{OLD} + \sum_{NEIGHBOURS}k(T_{NEIGHBORS} - T_{OLD}) $$ 在上面的计算单元温度的等式中，常量k表示模拟过程中热量的流动速率，k值越大，表示系统会更快地达到稳定温度，而k值越小，则温度梯度将存在更长时间。由于我们只考虑4个邻接单元(上、下、左、右)并且等式中的k和$$T_{OLD}$$都是常数，因此把上述公式展开表示为: $$ T_{NEW} = T_{OLD} + k(T_{TOP} + T_{BOTTOM} + T_{LEFT} + T_{RIGHT} - 4T_{OLD}) $$
温度更新的计算 以下是更新流程的基本介绍:
给定一个包含初始输入温度的格网，将其中作为热源的单元温度值复制到格网相应的单元中来覆盖这些单元之前计算出来的温度，确保“加热单元将保持恒温”的条件。这个复制操作在copy_const_kernel()中执行。 给定一个输入温度格网，根据新的公式计算出输出温度格网。这个更新操作在blend_kernel()中执行。 将输入温度格网和输出温度格网交换，为下一个步骤的计算做好准备。当模拟下一个时间步时，在步骤2中计算得到的输出温度格网将成为步骤1中的输入温度格网。 下面是两个函数的具体实现:
__global__ void copy_const_kernel(float *iptr, const float *cptr){ //将threadIdx/BlockIdx映射到像素位置 int x = threadIdx.x + blockIdx.x * blockDim.x; int y = threadIdx.y + blockIdx.y * blockDim.y; int offset = x + y * blockDim.</description></item><item><title>CUDA学习(四)</title><link>https://KasterMist.com/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E5%9B%9B/</link><pubDate>Sun, 18 Feb 2024 00:00:00 +0000</pubDate><guid>https://KasterMist.com/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E5%9B%9B/</guid><description>这一章会介绍如何在CUDA C中使用常量内存、了解常量内存的性能特性以及学习如何使用CUDA事件来测量应用程序的性能。
常量内存 到目前为止，我们知道CUDA C程序中可以使用全局内存和共享内存。但是，CUDA C还支持另一种类型的内存，即常量内存。常量内存用于保存在核函数执行期间不会发生变化的数据。在某些情况下，用常量内存来替换全局内存能有效地减少内存带宽。
在GPU上实现光线跟踪 我们给出一个简单的光线跟踪应用程序示例来学习。由于OpenGL和DirectX等API都不是专门为了实现光线跟踪而设计的，因此我们必须使用CUDA C来实现基本的光线跟踪器。本示例构造的光线跟踪器非常简单，旨在学习常量内存的使用上(并不能通过示例代码来构建一个功能完备的渲染器)。这个光线跟踪器只支持一组包含球状物体的场景，并且相机被固定在了Z轴，面向原点。此外，示例代码也不支持场景中的任何照明，从而避免二次光线带来的复杂性。代码也不计算照明效果，而只是为每个球面分配一个颜色值，如果它们是可见的，则通过某个预先计算的值对其着色。
光线跟踪器将从每个像素发射一道光线，并且跟踪到这些光线会命中哪些球面。此外，它还将跟踪每道命中光线的深度。当一道光线穿过多个球面时，只有最接近相机的球面才会被看到。这个代码的光线跟踪器会把相机看不到的球面隐藏起来。
通过一个数据结构对球面建模，在数据结构中包含了球面的中心坐标(x, y, z)，半径radius，以及颜色值(r, g, b)。
#define INF 2e10f struct sphere{ float r, g, b; float radius; float x, y, z; __device__ float hit(float ox, float oy, float *n){ float dx = ox - x; float dy = oy - y; if(dx * dx + dy * dy &amp;lt; radius * radius){ float dz = sqrtf(radius * radius - dx * dx - dy * dy); *n = dz / sqrtf(radius * radius); return dz + z; } return -INF; } } 这个结构中定义了一个方法hit(float ox, float oy, float *n)。对于来自(ox, oy)处像素的光线，这个方法将计算光线是否与这个球面相交。如果光线与球面相交，那么这个方法将计算从相机到光线命中球面处的距离。我们需要这个信息，因为当光线命中多个球面时，只有最接近相机的球面才会被看见。</description></item><item><title>CUDA学习(三)</title><link>https://KasterMist.com/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E4%B8%89/</link><pubDate>Mon, 12 Feb 2024 00:00:00 +0000</pubDate><guid>https://KasterMist.com/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E4%B8%89/</guid><description><![CDATA[这一章将介绍线程块以及线程之间的通信机制和同步机制。
在GPU启动并行代码的实现方法是告诉CUDA运行时启动核函数的多个并行副本。我们将这些并行副本称为线程块(Block)。
CUDA运行时将把这些线程块分解为多个线程。当需要启动多个并行线程块时，只需将尖括号中的第一个参数由1改为想要启动的线程块数量。
在尖括号中，第二个参数表示CUDA运行时在每个线程块中创建的线程数量。假设尖括号中的变量为&laquo;&lt;N, M&raquo;&gt;总共启动的线程数量可以按照以下公式计算: $$ N个线程块 * M个线程/线程块 = N*M个并行线程 $$
使用线程实现GPU上的矢量求和 在之前的代码中，我们才去的时调用N个线程块，每个线程块对应一个线程add&lt;&lt;&lt;N, 1&gt;&gt;&gt;(dev_a, dev_b, dev_c);。
如果我们启动N个线程，并且所有线程都在一个线程块中，则可以表示为add&lt;&lt;&lt;1, N&gt;&gt;&gt;(dev_a, dev_b, dev_c);。此外，因为只有一个线程块，我们需要通过线程索引来对数据进行索引(而不是线程块索引)，需要将int tid = blockIdx.x;修改为int tid = threadIdx.x;
在GPU上对更长的矢量求和 对于启动核函数时每个线程块中的线程数量，硬件也进行了限制。具体来说，最大的线程数量不能超过设备树形结构中maxThreadsPerBlock域的值。对目前的GPU来说一个线程块最多有1024个线程。如果要通过并行线程对长度大于1024的矢量进行相加的话，就需要将线程与线程块结合起来才能实现。
此时，计算索引可以表示为:
int tid = threadIdx.x + blockIdx.x * blockDim.x; blockDim保存的事线程块中每一维的线程数量，由于使用的事一维线程块，因此只用到blockDim.x。
此外，gridDim是二维的，而blockDim是三维的。
假如我们使用多个线程块处理N个并行线程，每个线程块处理的线程数量为128，那样可以启动N/128个线程块。然而问题在于，当N小于128时，比如127，那么N/128等于0，此时将会启动0个线程块。所以我们希望这个除法能够向上取整。我们可以不用调用 ceil()函数，而是将计算改为(N+127)/N。因此，这个例子调用核函数可以写为:
add&lt;&lt;&lt;(N + 127) / 128, 128&gt;&gt;&gt;(dev_a, dev_b, dev_c); 当N不是128的整数倍时，将启动过多的线程。然而，在核函数中已经解决了这个问题。在访问输入数组和输出数组之前，必须检查线程的便宜是否位于0到N之间。
if(tid &lt; N){ c[tid] = a[tid] + b[tid]; } 因此，当索引越过数组的边界时，核函数将自动停止执行计算。核函数不会对越过数组边界的内存进行读取或者写入。
在GPU上对任意长度的矢量求和 当矢量的长度很长时，我们可以让每一个线程执行多个矢量相加。例如
__global__ void add(int *a, int *b, int *c){ int tid = threadIdx.]]></description></item><item><title>CUDA学习(二)</title><link>https://KasterMist.com/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E4%BA%8C/</link><pubDate>Thu, 08 Feb 2024 00:00:00 +0000</pubDate><guid>https://KasterMist.com/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E4%BA%8C/</guid><description><![CDATA[这一部分将介绍CUDA的并行编程方式
矢量求和运算 假设有两组数据，我们需要将这两组数据中对应的元素两两相加，并将结果保存在第三个数组中。
基于CPU的矢量求和 首先，下面的代码是通过传统的C代码来实现这个求和运算
#include &#34;../common/book.h&#34; #define N 10 void add(int *a, int *b, int *c){ int tid = 0;	//这是第0个CPU，因此索引从0开始 while(tid &lt; N){ c[tid] = a[tid] + b[tid]; tid += 1;	//由于只有一个CPU，因此每次递增1 } } int main(){ int a[N], b[N], c[N]; //在CPU上为数组&#34;a&#34;和&#34;b&#34;赋值 for(int i = 0; i &lt; N; i++){ a[i] = -i; b[i] = i * i; } add(a, b, c); //显示结果 for(int i = 0; i &lt; N; i++){ printf(&#34;%d + %d = %d\n&#34;, a[i], b[i], c[i]); } return 0; } add()中使用while循环而不是for循环是为了代码能够在拥有多个CPU或者多个CPU核的系统上并行运行，比如双核处理器上可以将每次递增的大小改为2。]]></description></item><item><title>CUDA学习(一)</title><link>https://KasterMist.com/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E4%B8%80/</link><pubDate>Sun, 04 Feb 2024 00:00:00 +0000</pubDate><guid>https://KasterMist.com/posts/cuda/cuda%E5%AD%A6%E4%B9%A0-%E4%B8%80/</guid><description><![CDATA[参考书目: GPU高性能编程CUDA实战
书目网页链接: https://hpc.pku.edu.cn/docs/20170829223652566150.pdf
该博客参考于上述书籍，虽然书有一点老，但是作为初学者而言仍然能学到很多东西。
本书所包含的代码都在下面的连接中，可以下载来学习: https://developer.nvidia.com/cuda-example
CUDA C简介 首先来看一个CUDA C的示例:
#include &#34;../common/book.h&#34; int main(){ prinf(&#34;Hello World!\n&#34;); return 0; } 这个示例只是为了说明，CUDA C与熟悉的标准C在很大程度上是没有区别的。
核函数调用 在GPU设备上执行的函数通常称为核函数(Kernel)
#include &lt;iostream&gt; __global__ void kernel(){ } int main(){ kernel&lt;&lt;&lt;1, 1&gt;&gt;&gt;(); printf(&#34;Hello World!\n&#34;); return 0; } 跟之前的代码相比多了两处
一个空的函数kernel()，并且带有修饰符__global__。 对这个空函数的调用，并且带有修饰字符&laquo;&lt;1, 1&raquo;&gt;。 这个__global__可以认为是告诉编译器，函数应该编译为在设备而不是在主机上运行。函数kernel()将被交给编译器设备代码的编译器，而main()函数将被交给主机编译器。
传递参数 以下是对上述代码的进一步修改，可以实现将参数传递给核函数
#include &lt;iostream&gt; #include &#34;book.h&#34; __global__ void add(int a, int b, int* c){ *c = a + b; printf(&#34;c is %d\n&#34;, *c); } int main(void){ int c = 0; int* dev_c; printf(&#34;original c is %d\n&#34;, c); HANDLE_ERROR(cudaMalloc((void**)&amp;dev_c, sizeof(int))); add&lt;&lt;&lt;1, 1&gt;&gt;&gt;(2, 7, dev_c); HANDLE_ERROR(cudaMemcpy(&amp;c, dev_c, sizeof(int), cudaMemcpyDeviceToHost)); cudaFree(dev_c); printf(&#34;2 + 7 = %d\n&#34;, c); return 0; } 其中&quot;book.]]></description></item></channel></rss>